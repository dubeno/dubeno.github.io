<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="AI架构师, ai infra, AI系统设计, AI系统优化, 机器学习, 深度学习, 大数据处理, 高性能AI架构, 可扩展AI系统, ChatGPT, Stable Diffusion, AI 绘画, 大模型">
  
  
    <meta name="description" content="这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <title>Stable Diffusion的工作原理 |  AI架构 | AI系统基础架构设计与优化</title>
  
    <link rel="apple-touch-icon" sizes="57x57" href="/images/webclip/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/images/webclip/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/images/webclip/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/images/webclip/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/images/webclip/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/images/webclip/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/images/webclip/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/webclip/apple-touch-icon-180x180.png">
    <link rel="apple-touch-icon" sizes="167x167" href="/images/webclip/apple-touch-icon-167x167.png">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <main class="main">
    
	<header class="header">

	<div class="container">
		<nav class="navbar d-flex align-items-center">
			<a class="brand" href="/">
				<img class="logo lazyload" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
			</a>
			<ul class="main-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		</nav>
		<a id="mobile-nav-toggle">
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
		</a>
	</div>
</header>

    <section>
      <div class="container">
  <article id="post-Stable-Diffusion的工作原理" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="artcle-cover mb-5">
    
  </div>
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h1 class="article-title" itemprop="name">
      Stable Diffusion的工作原理
    </h1>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T08:07:29.000Z" itemprop="datePublished">
  2023-06-17
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 是一种文本到图像的潜在扩散模型，由<a target="_blank" rel="noopener" href="https://github.com/CompVis">CompVis</a>、<a target="_blank" rel="noopener" href="https://stability.ai/">Stability AI</a>和<a target="_blank" rel="noopener" href="https://laion.ai/">LAION</a>的研究人员和工程师创建。它使用来自<a target="_blank" rel="noopener" href="https://laion.ai/blog/laion-5b/">LAION-5B</a>数据库子集的 512x512 图像进行训练。 <em>LAION-5B</em>是目前最大的、可免费访问的多模态数据集。</p>
<p>看过Stable Diffusion可以产生的高质量图像后，让我们尝试更好地了解模型的功能。Stable Diffusion基于一种特殊类型的扩散模型，称为<strong>Latent Diffusion</strong>（<strong>潜在扩散</strong>）, 由本文提出：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>。</p>
<p>一般来说，扩散模型是机器学习系统，经过训练可以逐步对随机高斯噪声<em>进行去噪</em>，以获得感兴趣的样本，例如<em>图像</em>。要更详细地了解它们的工作原理，请查看<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">此 colab</a>。</p>
<p>扩散模型已显示可实现生成图像数据的最先进结果。但扩散模型的一个缺点是反向去噪过程很慢，因为它具有重复的、连续的性质。此外，这些模型会消耗大量内存，因为它们在像素空间中运行，在生成高分辨率图像时像素空间会变得很大。因此，训练这些模型并将它们用于推理具有挑战性。</p>
<p><em>潜在扩散可以通过在较低维度的潜在</em>空间上应用扩散过程而不是使用实际像素空间来降低内存和计算复杂性。这是标准扩散模型和潜在扩散模型之间的主要区别：<strong>在潜在扩散中，模型经过训练以生成图像的潜在（压缩）表示。</strong></p>
<p>潜在扩散有三个主要组成部分。</p>
<ol>
<li>A autoencoder (VAE).</li>
<li>A <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq">U-Net</a>.</li>
<li>A text-encoder, <em>e.g.</em> <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIP’s Text Encoder</a>.</li>
</ol>
<p><strong>1. The autoencoder (VAE)</strong></p>
<p>VAE模型有两部分，编码器和解码器。编码器用于将图像转换为低维潜在表示，作为<em>U-Net</em>模型的输入。相反，解码器将潜在表示转换回图像。</p>
<p>在潜在扩散<em>训练</em>期间，编码器用于获取图像的潜在表示（<em>潜在）用于前向扩散过程，它在每一步应用越来越多的噪声。</em>在<em>推理过程</em>中，使用 VAE 解码器将反向扩散过程生成的去噪潜伏转换回图像。我们在推理过程中，我们<strong>只需要 VAE 解码器</strong>。</p>
<p><strong>2. The U-Net</strong></p>
<p>U-Net 的编码器部分和解码器部分均由 ResNet 块组成。编码器将图像表示压缩为较低分辨率的图像表示，解码器将较低分辨率的图像表示解码回据称噪声较小的原始高分辨率图像表示。更具体地说，U-Net 输出预测可用于计算预测去噪图像表示的噪声残差。</p>
<p>为了防止 U-Net 在下采样时丢失重要信息，通常在编码器的下采样 ResNet 和解码器的上采样 ResNet 之间添加快捷连接。此外，稳定扩散 U-Net 能够通过交叉注意层在文本嵌入上调节其输出。通常在 ResNet 块之间将交叉注意层添加到 U-Net 的编码器和解码器部分。</p>
<p><strong>3.文本编码器</strong></p>
<p>文本编码器负责将输入提示（<em>例如</em>“An astronaut riding a horse”）转换为 U-Net 可以理解的嵌入空间。它通常是一个简单的<em>基于转换器的</em>编码器，将输入标记序列映射到潜在文本嵌入序列。</p>
<p>受<a target="_blank" rel="noopener" href="https://imagen.research.google/">Imagen</a>的启发，Stable Diffusion 在训练期间不<strong>训练</strong>文本编码器，而只是使用 CLIP 已经训练好的文本编码器<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a>。</p>
<p><strong>为什么潜在扩散快速有效？</strong></p>
<p>由于潜在扩散在低维空间上运行，因此与像素空间扩散模型相比，它大大降低了内存和计算要求。例如，Stable Diffusion 中使用的自动编码器的缩减系数为 8。这意味着形状图像<code>(3, 512, 512)</code>进入<code>(3, 64, 64)</code>潜在空间，这需要<code>8 × 8 = 64</code>更少的内存。</p>
<p>这就是为什么可以<code>512 × 512</code>如此快速地生成图像，即使在 16GB Colab GPU 上也是如此！</p>
<p><strong>推理过程中的稳定扩散</strong></p>
<img src="/2023/06/17/Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion.png" class="" title="SD管道">

<p>稳定扩散模型将latent seed和 text prompt作为输入。然后使用latent seed生成大小为64×64的随机潜在图像，而text prompt通过 CLIP 的文本编码器转换为大小77×768的text embeddings。</p>
<p>接下来，U-Net在以text embeddings为条件的同时迭代地对随机潜在图像表示<em>进行去噪。</em>U-Net 的输出是噪声残差，用于通过调度程序算法计算去噪的潜在图像表示。许多不同的调度程序算法可用于此计算，每个算法都有其优点和缺点。对于稳定扩散，我们建议使用以下之一：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py">PNDM scheduler</a> (used by default)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py">DDIM scheduler</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py">K-LMS scheduler</a></li>
</ul>
<p>关于调度程序算法功能如何超出本笔记本范围的理论，但简而言之，应该记住他们根据先前的噪声表示和预测的噪声残差计算预测的去噪图像表示。有关更多信息，建议查看<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364">Elucidating the Design Space of Diffusion-Based Generative Models</a></a></p>
<p>重复去噪<em>过程**。</em>50 次逐步检索更好的潜在图像表示。完成后，潜在图像表示由变分自动编码器的解码器部分解码。</p>
<h2 id="编写自己的推理管道"><a href="#编写自己的推理管道" class="headerlink" title="编写自己的推理管道"></a>编写自己的推理管道</h2><p>最后，我们展示如何将 Stable Diffusion 与不同的调度器一起使用，即<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/pull/185">本 PR中添加的</a><a target="_blank" rel="noopener" href="https://github.com/crowsonkb">Katherine Crowson 的</a>K-LMS 调度器。</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main">预训练模型</a>包括设置完整扩散管道所需的所有组件。它们存储在以下文件夹中：</p>
<ul>
<li><code>text_encoder</code>: Stable Diffusion 使用 CLIP，但其他扩散模型可能使用其他编码器，例如<code>BERT</code>.</li>
<li><code>tokenizer</code>. 它必须与模型使用的相匹配<code>text_encoder</code>。</li>
<li><code>scheduler</code>：用于在训练期间逐步向图像添加噪声的调度算法。</li>
<li><code>unet</code>：用于生成输入的潜在表示的模型。</li>
<li><code>vae</code>：自动编码器模块，我们将使用它来将潜在表示解码为真实图像。</li>
</ul>
<p>我们可以通过引用保存组件的文件夹来加载组件，<code>from_pretrained</code> 中传递<code>subfolder</code></p>
<p>首先，您应该安装<code>diffusers==0.10.2</code>以运行以下代码片段：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install diffusers==0.10.2 transformers scipy  accelerate</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPTextModel, CLIPTokenizer</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoencoderKL, UNet2DConditionModel, PNDMScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load the autoencoder model which will be used to decode the latents into image space. </span></span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;vae&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Load the tokenizer and text encoder to tokenize and encode the text. </span></span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. The UNet model for generating the latents.</span></span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>,  subfolder=<span class="string">&quot;unet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>现在我们不再加载预定义的调度程序，而是加载具有一些拟合参数的<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/71ba8aec55b52a7ba5a1ff1db1265ffdd3c65ea2/src/diffusers/schedulers/scheduling_lms_discrete.py#L26">K-LMS 调度程序</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line">scheduler = LMSDiscreteScheduler(beta_start=<span class="number">0.00085</span>, beta_end=<span class="number">0.012</span>, beta_schedule=<span class="string">&quot;scaled_linear&quot;</span>, num_train_timesteps=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p>接下来，让我们将模型移动到 GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch_device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">vae.to(torch_device)</span><br><span class="line">text_encoder.to(torch_device)</span><br><span class="line">unet.to(torch_device) </span><br></pre></td></tr></table></figure>

<p>我们现在定义我们将用于生成图像的参数。</p>
<p>请注意，<code>guidance_scale</code>它的定义类似于<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11487.pdf">Imagen 论文</a><code>w</code>中等式 (2) 的指导权重。对应于不进行无分类器指导。在这里，我们将其设置为 7.5，就像之前所做的那样。<code>guidance_scale == 1</code></p>
<p>与前面的示例相比，我们设置<code>num_inference_steps</code>为 100 以获得更清晰的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">prompt = [<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>]</span><br><span class="line"></span><br><span class="line">height = <span class="number">512</span>                        <span class="comment"># default height of Stable Diffusion</span></span><br><span class="line">width = <span class="number">512</span>                         <span class="comment"># default width of Stable Diffusion</span></span><br><span class="line"></span><br><span class="line">num_inference_steps = <span class="number">100</span>           <span class="comment"># Number of denoising steps</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">7.5</span>                <span class="comment"># Scale for classifier-free guidance</span></span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">0</span>)    <span class="comment"># Seed generator to create the inital latent noise</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="built_in">len</span>(prompt)</span><br></pre></td></tr></table></figure>



<p>首先，我们得到<code>text_embeddings</code>传递的提示。这些嵌入将用于调整 UNet 模型并引导图像生成类似于输入提示的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text_input = tokenizer(prompt, padding=<span class="string">&quot;max_length&quot;</span>, max_length=tokenizer.model_max_length, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p>我们还将获得用于无分类器指导的无条件文本嵌入，它们只是填充标记（空文本）的嵌入。它们需要与条件<code>text_embeddings</code>(<code>batch_size</code>和<code>seq_length</code>)具有相同的形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_length = text_input.input_ids.shape[-<span class="number">1</span>]</span><br><span class="line">uncond_input = tokenizer(</span><br><span class="line">    [<span class="string">&quot;&quot;</span>] * batch_size, padding=<span class="string">&quot;max_length&quot;</span>, max_length=max_length, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line">uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[<span class="number">0</span>]   </span><br></pre></td></tr></table></figure>



<p>对于无分类器指导，我们需要进行两次前向传递：一次使用条件输入 ( <code>text_embeddings</code>)，另一个使用无条件嵌入 ( <code>uncond_embeddings</code>)。在实践中，我们可以将两者连接成一个批次，以避免进行两次前向传递。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br></pre></td></tr></table></figure>



<p>接下来，我们生成初始随机噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">latents = torch.randn(</span><br><span class="line">    (batch_size, unet.in_channels, height // <span class="number">8</span>, width // <span class="number">8</span>),</span><br><span class="line">    generator=generator,</span><br><span class="line">)</span><br><span class="line">latents = latents.to(torch_device)</span><br></pre></td></tr></table></figure>



<p>如果我们在这个阶段检查 ，<code>latents</code>我们会看到它们的形状是<code>torch.Size([1, 4, 64, 64])</code>，比我们想要生成的图像小得多。<code>512 × 512</code>该模型稍后会将这种潜在表示（纯噪声）转换为图像。</p>
<p>接下来，我们用我们选择的<code>num_inference_steps</code>. 这将计算<code>sigmas</code>去噪过程中要使用的确切时间步长值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br></pre></td></tr></table></figure>



<p>K-LMS 调度程序需要将 乘以<code>latents</code>它的<code>sigma</code>值。让我们在这里这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latents = latents * scheduler.init_noise_sigma</span><br></pre></td></tr></table></figure>



<p>我们准备编写去噪循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm(scheduler.timesteps):</span><br><span class="line">    <span class="comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span><br><span class="line">    latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict the noise residual</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform guidance</span></span><br><span class="line">    noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">    latents = scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>



<p>我们现在使用<code>vae</code>将生成的解码<code>latents</code>回图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scale and decode the image latents with vae</span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    image = vae.decode(latents).sample</span><br></pre></td></tr></table></figure>



<p>最后，让我们将图像转换为 PIL，以便我们可以显示或保存它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image = (image / <span class="number">2</span> + <span class="number">0.5</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">image = image.detach().cpu().permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).numpy()</span><br><span class="line">images = (image * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line">pil_images = [Image.fromarray(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">pil_images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_k_lms.png"><img src="/./Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion_k_lms.png" alt="PNG"></a></p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
    
<nav class="article-nav pt-4 mt-3" id="article-nav">
  
  
    <a href="/2023/06/17/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%83%AD%E9%97%A8%E9%A1%B9%E7%9B%AE/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">AIGC图像生成热门项目</div>
    </a>
  
</nav>


  
</article>
</div>
    </section>
    <footer class="footer pt-5 mt-5">
  <div class="container">
    <div class="py-3">
      <div class="row justify-content-between">
        <div class="col-6">
          <img class="filter-gray mb-3 lazyload" height="40" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
          <p class="mb-4">这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。</p>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="javascript:;">
                  <img 0="微信" src="/images/icons/contact_wechat.svg">
                </a>
              </li>
            
              <li class="list-inline-item">
                <a href="mailto:a@abc.com">
                  <img 0="邮箱" src="/images/icons/contact_email.svg">
                </a>
              </li>
            
          </ul>
        </div>
        <div class="col-4">
          <h5>友情链接</h5>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="https://acorn.imaging.xin/" title="Acorn" target="_blank" rel="noopener">Acorn</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://github.com/" title="GitHub" target="_blank" rel="noopener">GitHub</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://duoyu.wang/" title="To Base64" target="_blank" rel="noopener">To Base64</a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
    <hr class="hr" style="opacity: .25;">
    <div class="pt-3 pb-5">
      <ul class="list-inline mb-0 text-center">
        <li class="list-inline-item">&copy; 2023 AI架构 | AI系统基础架构设计与优化</li>
        
        <li class="list-inline-item">Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
        <li class="list-inline-item">Designer <a href="https://acorn.imaging.xin/" target="_blank">罗平</a></li>
      </ul>
    </div>
  </div>
</footer>
  </main>
  <div id="mobile-nav-dimmer"></div>
<div id="mobile-nav">
	<div id="mobile-nav-inner">
		<ul class="mobile-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		
	</div>
</div>

  <script src="/libs/feather/feather.min.js"></script>
<script src="/libs/lazysizes/lazysizes.min.js"></script>

	<script src="/libs/tocbot/tocbot.min.js"></script>
	<script>
    tocbot.init({
      // Where to render the table of contents.
      tocSelector: '.js-toc',
      // Where to grab the headings to build the table of contents.
      contentSelector: '.js-toc-content',
      // Which headings to grab inside of the contentSelector element.
      headingSelector: 'h2, h3',
      // For headings inside relative or absolute positioned containers within content.
      hasInnerContainers: true,
    });
	</script>





<script src="/js/mobile-nav.js"></script>


<script src="/js/script.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178892506-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178892506-1');
</script>
</body>
</html>