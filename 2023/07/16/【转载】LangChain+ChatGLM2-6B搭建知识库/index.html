<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="AI架构师, ai infra, AI系统设计, AI系统优化, 机器学习, 深度学习, 大数据处理, 高性能AI架构, 可扩展AI系统, ChatGPT, Stable Diffusion, AI 绘画, 大模型">
  
  
    <meta name="description" content="这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <title> AI架构 | AI系统基础架构设计与优化</title>
  
    <link rel="apple-touch-icon" sizes="57x57" href="/images/webclip/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/images/webclip/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/images/webclip/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/images/webclip/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/images/webclip/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/images/webclip/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/images/webclip/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/webclip/apple-touch-icon-180x180.png">
    <link rel="apple-touch-icon" sizes="167x167" href="/images/webclip/apple-touch-icon-167x167.png">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <main class="main">
    
	<header class="header">

	<div class="container">
		<nav class="navbar d-flex align-items-center">
			<a class="brand" href="/">
				<img class="logo lazyload" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
			</a>
			<ul class="main-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		</nav>
		<a id="mobile-nav-toggle">
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
		</a>
	</div>
</header>

    <section>
      <div class="container">
  <article id="post-【转载】LangChain+ChatGLM2-6B搭建知识库" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="artcle-cover mb-5">
    
  </div>
  
  <div class="article-inner">
    
    <div class="article-entry" itemprop="articleBody">
      <p>title: 【转载】LangChain+ChatGLM2-6B搭建知识库<br>date: 2023-06-26 06:50:32<br>tags: 大模型, ChatGLM2-6B</p>
<p>本文来自博客园，原文链接：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html">https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html</a></p>
<h2 id="ChatGLM2-6B-介绍"><a href="#ChatGLM2-6B-介绍" class="headerlink" title="ChatGLM2-6B 介绍"></a>ChatGLM2-6B 介绍</h2><p>ChatGLM2-6B 在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了如下新特性：</p>
<ul>
<li>• <strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，全面升级了基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li>
</ul>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632435-1491933983.png" class="" title="图片">

<ul>
<li>• <strong>更长的上下文</strong>：基于 FlashAttention 技术，将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。</li>
<li>• <strong>更高效的推理</strong>：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。</li>
</ul>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632905-1098197173.png" class="" title="图片">

<ul>
<li>• <strong>更开放的协议</strong>：ChatGLM2-6B 权重对学术研究完全开放，在获得官方的书面许可后，亦<strong>允许商业使用</strong>。</li>
</ul>
<p>相比于初代模型，ChatGLM2-6B 多个维度的能力都取得了提升，以下是一些官方对比示例。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632839-1115027413.png" class="" title="图片">

<p>总的来说，看起来效果还不错，下面跟着树先生一起来试试水~</p>
<p>本文我将分 3 步带着大家一起实操一遍，并与之前 ChatGLM-6B 进行对比。</p>
<ul>
<li>• ChatGLM2-6B 部署</li>
<li>• ChatGLM2-6B 微调</li>
<li>• LangChain + ChatGLM2-6B 构建个人专属知识库</li>
</ul>
<h2 id="ChatGLM2-6B-部署"><a href="#ChatGLM2-6B-部署" class="headerlink" title="ChatGLM2-6B 部署"></a>ChatGLM2-6B 部署</h2><p>这里我们还是白嫖阿里云的机器学习 PAI 平台，使用 A10 显卡，这部分内容之前文章中有介绍。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4NDg5OTg1Mg==&mid=2247484248&idx=1&sn=711d8ea75fc825ae7a1ade82b6eb5f2f&scene=21#wechat_redirect">免费部署一个开源大模型 MOSS</a></p>
<p>环境准备好了以后，就可以开始准备部署工作了。</p>
<h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/THUDM/ChatGLM2-6B</span><br></pre></td></tr></table></figure>

<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ChatGLM2-6B</span><br><span class="line"><span class="comment"># 其中 transformers 库版本推荐为 4.30.2，torch 推荐使用 2.0 及以上的版本，以获得最佳的推理性能</span></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<h3 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我将下载的模型文件放到了本地的 chatglm-6b 目录下</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm2-6b <span class="variable">$PWD</span>/chatglm2-6b</span><br></pre></td></tr></table></figure>

<h3 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为前面改了模型默认下载地址，所以这里需要改下路径参数</span></span><br><span class="line"><span class="comment"># 修改 web_demo.py 文件</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想要本地访问，需要修改此处</span></span><br><span class="line">demo.queue().launch(share=<span class="literal">True</span>, inbrowser=<span class="literal">True</span>, server_name=<span class="string">&#x27;0.0.0.0&#x27;</span>, server_port=<span class="number">7860</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Web-模式启动"><a href="#Web-模式启动" class="headerlink" title="Web 模式启动"></a>Web 模式启动</h3><p>官方推荐用 Streamlit 启动会更流程一些，但受限于 PAI 平台没有分配弹性公网，所以还是用老的 gradio 启动吧。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python web_demo.py</span><br></pre></td></tr></table></figure>

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632951-287181832.png" class="" title="图片">img

<h3 id="ChatGLM2-6B-对比-ChatGLM-6B"><a href="#ChatGLM2-6B-对比-ChatGLM-6B" class="headerlink" title="ChatGLM2-6B 对比 ChatGLM-6B"></a>ChatGLM2-6B 对比 ChatGLM-6B</h3><p>先让 ChatGPT 作为考官，出几道题。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632967-1521522527.png" class="" title="图片">

<p>ChatGLM-6B 回答：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632495-642956216.png" class="" title="图片">

<p>ChatGLM2-6B 回答：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633082-1769147658.png" class="" title="图片">

<p>明显可以看出，ChatGLM2-6B 相比于上一代模型响应速度更快，问题回答精确度更高，且拥有更长的（32K）上下文！</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632541-2120443628.png" class="" title="图片">

<h2 id="基于-P-Tuning-微调-ChatGLM2-6B"><a href="#基于-P-Tuning-微调-ChatGLM2-6B" class="headerlink" title="基于 P-Tuning 微调 ChatGLM2-6B"></a>基于 P-Tuning 微调 ChatGLM2-6B</h2><p>ChatGLM2-6B 环境已经有了，接下来开始模型微调，这里我们使用官方的 P-Tuning v2 对 ChatGLM2-6B 模型进行参数微调，P-Tuning v2 将需要微调的参数量减少到原来的 0.1%，再通过模型量化、Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行。</p>
<h3 id="安装依赖-1"><a href="#安装依赖-1" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行微调需要 4.27.1 版本的 transformers</span></span><br><span class="line">pip <span class="keyword">install </span>transformers==<span class="number">4</span>.<span class="number">27</span>.<span class="number">1</span></span><br><span class="line">pip <span class="keyword">install </span>rouge_chinese nltk <span class="keyword">jieba </span>datasets</span><br></pre></td></tr></table></figure>

<h3 id="禁用-W-amp-B"><a href="#禁用-W-amp-B" class="headerlink" title="禁用 W&amp;B"></a>禁用 W&amp;B</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁用 W&amp;B，如果不禁用可能会中断微调训练，以防万一，还是禁了吧</span></span><br><span class="line"><span class="built_in">export</span> WANDB_DISABLED=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h3><p>这里为了简化，我只准备了5条测试数据，分别保存为 train.json 和 dev.json，放到 ptuning 目录下，实际使用的时候肯定需要大量的训练数据。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;你好，你是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;你是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;树先生是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;介绍下树先生&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;树先生&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="参数调整-1"><a href="#参数调整-1" class="headerlink" title="参数调整"></a>参数调整</h3><p>修改 <code>train.sh</code> 和 <code>evaluate.sh</code> 中的 <code>train_file</code>、<code>validation_file</code>和<code>test_file</code>为你自己的 JSON 格式数据集路径，并将 <code>prompt_column</code> 和 <code>response_column</code> 改为 JSON 文件中输入文本和输出文本对应的 KEY。可能还需要增大 <code>max_source_length</code> 和 <code>max_target_length</code> 来匹配你自己的数据集中的最大输入输出长度。并将模型路径 <code>THUDM/chatglm2-6b</code> 改为你本地的模型路径。</p>
<h4 id="1、train-sh-文件修改"><a href="#1、train-sh-文件修改" class="headerlink" title="1、train.sh 文件修改"></a>1、train.sh 文件修改</h4><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=<span class="number">32</span></span><br><span class="line">LR=<span class="number">2e-2</span></span><br><span class="line">NUM_GPUS=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">torchrun <span class="comment">--standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \</span></span><br><span class="line">    <span class="comment">--do_train \</span></span><br><span class="line">    <span class="comment">--train_file train.json \</span></span><br><span class="line">    <span class="comment">--validation_file dev.json \</span></span><br><span class="line">    <span class="comment">--preprocessing_num_workers 10 \</span></span><br><span class="line">    <span class="comment">--prompt_column content \</span></span><br><span class="line">    <span class="comment">--response_column summary \</span></span><br><span class="line">    <span class="comment">--overwrite_cache \</span></span><br><span class="line">    <span class="comment">--model_name_or_path /mnt/workspace/chatglm2-6b \</span></span><br><span class="line">    <span class="comment">--output_dir output/adgen-chatglm2-6b-pt-$PRE_SEQ_LEN-$LR \</span></span><br><span class="line">    <span class="comment">--overwrite_output_dir \</span></span><br><span class="line">    <span class="comment">--max_source_length 128 \</span></span><br><span class="line">    <span class="comment">--max_target_length 128 \</span></span><br><span class="line">    <span class="comment">--per_device_train_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--per_device_eval_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--gradient_accumulation_steps 16 \</span></span><br><span class="line">    <span class="comment">--predict_with_generate \</span></span><br><span class="line">    <span class="comment">--max_steps 3000 \</span></span><br><span class="line">    <span class="comment">--logging_steps 10 \</span></span><br><span class="line">    <span class="comment">--save_steps 1000 \</span></span><br><span class="line">    <span class="comment">--learning_rate $LR \</span></span><br><span class="line">    <span class="comment">--pre_seq_len $PRE_SEQ_LEN \</span></span><br><span class="line">    <span class="comment">--quantization_bit 4</span></span><br></pre></td></tr></table></figure>

<p><code>train.sh</code> 中的 <code>PRE_SEQ_LEN</code> 和 <code>LR</code> 分别是 soft prompt 长度和训练的学习率，可以进行调节以取得最佳的效果。P-Tuning-v2 方法会冻结全部的模型参数，可通过调整 <code>quantization_bit</code> 来改变原始模型的量化等级，不加此选项则为 FP16 精度加载。</p>
<h4 id="2、evaluate-sh-文件修改"><a href="#2、evaluate-sh-文件修改" class="headerlink" title="2、evaluate.sh 文件修改"></a>2、evaluate.sh 文件修改</h4><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=<span class="number">32</span></span><br><span class="line">CHECKPOINT=adgen-chatglm2<span class="number">-6</span>b-pt<span class="number">-32</span><span class="number">-2e-2</span></span><br><span class="line">STEP=<span class="number">3000</span></span><br><span class="line">NUM_GPUS=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">torchrun <span class="comment">--standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \</span></span><br><span class="line">    <span class="comment">--do_predict \</span></span><br><span class="line">    <span class="comment">--validation_file dev.json \</span></span><br><span class="line">    <span class="comment">--test_file dev.json \</span></span><br><span class="line">    <span class="comment">--overwrite_cache \</span></span><br><span class="line">    <span class="comment">--prompt_column content \</span></span><br><span class="line">    <span class="comment">--response_column summary \</span></span><br><span class="line">    <span class="comment">--model_name_or_path /mnt/workspace/chatglm2-6b \</span></span><br><span class="line">    <span class="comment">--ptuning_checkpoint ./output/$CHECKPOINT/checkpoint-$STEP \</span></span><br><span class="line">    <span class="comment">--output_dir ./output/$CHECKPOINT \</span></span><br><span class="line">    <span class="comment">--overwrite_output_dir \</span></span><br><span class="line">    <span class="comment">--max_source_length 128 \</span></span><br><span class="line">    <span class="comment">--max_target_length 128 \</span></span><br><span class="line">    <span class="comment">--per_device_eval_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--predict_with_generate \</span></span><br><span class="line">    <span class="comment">--pre_seq_len $PRE_SEQ_LEN \</span></span><br><span class="line">    <span class="comment">--quantization_bit 4</span></span><br></pre></td></tr></table></figure>

<p><code>CHECKPOINT</code> 实际就是 <code>train.sh</code> 中的 <code>output_dir</code>。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>train.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<p>5 条数据大概训练了 50 分钟左右。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633066-1570481016.png" class="" title="图片">

<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>evaluate.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633061-1802964012.png" class="" title="图片">

<p>执行完成后，会生成评测文件，评测指标为中文 Rouge score 和 BLEU-4。生成的结果保存在 .&#x2F;output&#x2F;adgen-chatglm2-6b-pt-32-2e-2&#x2F;generated_predictions.txt。我们准备了 5 条推理数据，所以相应的在文件中会有 5 条评测数据，labels 是 dev.json 中的预测输出，predict 是 ChatGLM2-6B 生成的结果，对比预测输出和生成结果，评测模型训练的好坏。如果不满意调整训练的参数再次进行训练。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="部署微调后的模型"><a href="#部署微调后的模型" class="headerlink" title="部署微调后的模型"></a>部署微调后的模型</h3><p>这里我们先修改 web_demo.sh 的内容以符合实际情况，将 <code>pre_seq_len</code> 改成你训练时的实际值，将 <code>THUDM/chatglm2-6b</code> 改成本地的模型路径。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=32</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 web_demo.py \</span><br><span class="line">    --model_name_or_path /mnt/workspace/chatglm2-6b \</span><br><span class="line">    --ptuning_checkpoint output/adgen-chatglm2-6b-pt-32-2e-2/checkpoint-3000 \</span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span></span><br></pre></td></tr></table></figure>

<p>然后再执行。</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>web_demo.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<h3 id="结果对比"><a href="#结果对比" class="headerlink" title="结果对比"></a>结果对比</h3><h4 id="原始模型"><a href="#原始模型" class="headerlink" title="原始模型"></a>原始模型</h4><img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633070-939675307.png" class="" title="图片">

<h4 id="微调后模型"><a href="#微调后模型" class="headerlink" title="微调后模型"></a>微调后模型</h4><img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633099-1579471952.png" class="" title="图片">

<h2 id="LangChain-ChatGLM2-6B-构建知识库"><a href="#LangChain-ChatGLM2-6B-构建知识库" class="headerlink" title="LangChain + ChatGLM2-6B 构建知识库"></a>LangChain + ChatGLM2-6B 构建知识库</h2><h3 id="LangChain-知识库技术原理"><a href="#LangChain-知识库技术原理" class="headerlink" title="LangChain 知识库技术原理"></a>LangChain 知识库技术原理</h3><p>目前市面上绝大部分知识库都是 LangChain + LLM + embedding 这一套，实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的<code>top k</code>个 -&gt; 匹配出的文本作为上下文和问题一起添加到 prompt 中 -&gt; 提交给 LLM 生成回答。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632909-1342948348.png" class="" title="图片">

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632518-1835023463.png" class="" title="图片">

<p>从上面就能看出，其核心技术就是向量 embedding，将用户知识库内容经过 embedding 存入向量知识库，然后用户每一次提问也会经过 embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 promt 提交给 LLM 回答，很好理解吧。一个典型的 prompt 模板如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">已知信息：</span></span><br><span class="line"><span class="string">&#123;context&#125; </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 </span></span><br><span class="line"><span class="string">问题是：&#123;question&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>更多关于向量 embedding 的内容可以参考我之前写的一篇文章。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4NDg5OTg1Mg==&mid=2247484333&idx=1&sn=213a245558ba7b52e5736682a2ec45a9&chksm=cfb06acef8c7e3d8b08099a29d93891d455f4d7c6cc4dd72ea391b23a183bb09bcdb880a2423&scene=21#wechat_redirect">ChatGPT 引爆向量数据库赛道</a></p>
<h3 id="项目部署"><a href="#项目部署" class="headerlink" title="项目部署"></a>项目部署</h3><p><strong>下载源码</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/imClumsyPanda/langchain-ChatGLM.git</span><br></pre></td></tr></table></figure>

<p><strong>安装依赖</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> langchain-ChatGLM</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p><strong>下载模型</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 git lfs</span></span><br><span class="line">git lfs install</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 LLM 模型</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm2-6b <span class="variable">$PWD</span>/chatglm2-6b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 Embedding 模型</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/GanymedeNil/text2vec-large-chinese <span class="variable">$PWD</span>/text2vec</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型需要更新时，可打开模型所在文件夹后拉取最新模型文件/代码</span></span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>

<p><strong>参数调整</strong></p>
<p>模型下载完成后，请在 <code>configs/model_config.py</code> 文件中，对<code>embedding_model_dict</code>和<code>llm_model_dict</code>参数进行修改。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">embedding_model_dict = &#123;</span><br><span class="line">    <span class="string">&quot;ernie-tiny&quot;</span>: <span class="string">&quot;nghuyong/ernie-3.0-nano-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ernie-base&quot;</span>: <span class="string">&quot;nghuyong/ernie-3.0-base-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec-base&quot;</span>: <span class="string">&quot;shibing624/text2vec-base-chinese&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec&quot;</span>: <span class="string">&quot;/mnt/workspace/text2vec&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-small&quot;</span>: <span class="string">&quot;moka-ai/m3e-small&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-base&quot;</span>: <span class="string">&quot;moka-ai/m3e-base&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">llm_model_dict = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">&quot;chatglm2-6b&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;chatglm2-6b&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pretrained_model_name&quot;</span>: <span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>,</span><br><span class="line">        <span class="string">&quot;local_model_path&quot;</span>: None,</span><br><span class="line">        <span class="string">&quot;provides&quot;</span>: <span class="string">&quot;ChatGLM&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># LLM 名称改成 chatglm2-6b</span></span><br><span class="line">LLM_MODEL = <span class="string">&quot;chatglm2-6b&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="项目启动"><a href="#项目启动" class="headerlink" title="项目启动"></a>项目启动</h3><p><strong>Web 模式启动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python webui.py</span><br></pre></td></tr></table></figure>

<p>如果报了这个错：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632966-402914823.png" class="" title="图片">

<p>升级下 protobuf 即可。</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="comment">--upgrade protobuf==3.19.6</span></span><br></pre></td></tr></table></figure>

<p>启动成功！</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632524-1702966774.png" class="" title="图片">

<p><strong>模型配置</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632543-184630604.png" class="" title="图片">

<p><strong>上传知识库</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632952-239703160.png" class="" title="图片">

<p><strong>基于</strong> <strong>ChatGLM2-6B 的知识库问答</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632941-638928516.png" class="" title="图片">

<h3 id="定制-UI"><a href="#定制-UI" class="headerlink" title="定制 UI"></a><strong>定制</strong> <strong>UI</strong></h3><p>由于 LangChain 项目更新了接口，树先生之前开发的定制 UI 也同步更新进行了适配。</p>
<p><strong>选择知识库</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632525-1874686678.png" class="" title="图片">

<p><strong>基于知识库问答</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632893-1731487740.png" class="" title="图片">

<p><strong>显示答案来源</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632499-811642262.png" class="" title="图片">

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633092-2050305602.png" class="" title="图片">

<p>好了，这一篇还挺长的，不过很多内容之前文章中都有提到，相当于是一篇 LangChain + LLM + embedding 构建知识库的<strong>总结篇</strong>了，大家收藏好这一篇就行了~</p>
<p>本文来自博客园，原文链接：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html">https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html</a></p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
    
<nav class="article-nav pt-4 mt-3" id="article-nav">
  
  
    <a href="/2023/07/14/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>


  
</article>
</div>
    </section>
    <footer class="footer pt-5 mt-5">
  <div class="container">
    <div class="py-3">
      <div class="row justify-content-between">
        <div class="col-6">
          <img class="filter-gray mb-3 lazyload" height="40" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
          <p class="mb-4">这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。</p>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="javascript:;">
                  <img 0="微信" src="/images/icons/contact_wechat.svg">
                </a>
              </li>
            
              <li class="list-inline-item">
                <a href="mailto:a@abc.com">
                  <img 0="邮箱" src="/images/icons/contact_email.svg">
                </a>
              </li>
            
          </ul>
        </div>
        <div class="col-4">
          <h5>友情链接</h5>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="https://acorn.imaging.xin/" title="Acorn" target="_blank" rel="noopener">Acorn</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://github.com/" title="GitHub" target="_blank" rel="noopener">GitHub</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://duoyu.wang/" title="To Base64" target="_blank" rel="noopener">To Base64</a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
    <hr class="hr" style="opacity: .25;">
    <div class="pt-3 pb-5">
      <ul class="list-inline mb-0 text-center">
        <li class="list-inline-item">&copy; 2023 AI架构 | AI系统基础架构设计与优化</li>
        
        <li class="list-inline-item">Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
        <li class="list-inline-item">Designer <a href="https://acorn.imaging.xin/" target="_blank">罗平</a></li>
      </ul>
    </div>
  </div>
</footer>
  </main>
  <div id="mobile-nav-dimmer"></div>
<div id="mobile-nav">
	<div id="mobile-nav-inner">
		<ul class="mobile-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		
	</div>
</div>

  <script src="/libs/feather/feather.min.js"></script>
<script src="/libs/lazysizes/lazysizes.min.js"></script>

	<script src="/libs/tocbot/tocbot.min.js"></script>
	<script>
    tocbot.init({
      // Where to render the table of contents.
      tocSelector: '.js-toc',
      // Where to grab the headings to build the table of contents.
      contentSelector: '.js-toc-content',
      // Which headings to grab inside of the contentSelector element.
      headingSelector: 'h2, h3',
      // For headings inside relative or absolute positioned containers within content.
      hasInnerContainers: true,
    });
	</script>





<script src="/js/mobile-nav.js"></script>


<script src="/js/script.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178892506-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178892506-1');
</script>
</body>
</html>