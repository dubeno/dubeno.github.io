<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="AI架构师, ai infra, AI系统设计, AI系统优化, 机器学习, 深度学习, 大数据处理, 高性能AI架构, 可扩展AI系统, ChatGPT, Stable Diffusion, AI 绘画, 大模型">
  
  
    <meta name="description" content="这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <title> AI架构 | AI系统基础架构设计与优化</title>
  
    <link rel="apple-touch-icon" sizes="57x57" href="/images/webclip/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/images/webclip/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/images/webclip/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/images/webclip/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/images/webclip/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/images/webclip/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/images/webclip/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/webclip/apple-touch-icon-180x180.png">
    <link rel="apple-touch-icon" sizes="167x167" href="/images/webclip/apple-touch-icon-167x167.png">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <main class="main">
    
	<header id="header" class="header header-absolute">

	<div class="container">
		<nav class="navbar d-flex align-items-center">
			<a class="brand" href="/">
				<img class="logo lazyload" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
			</a>
			<ul class="main-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		</nav>
		<a id="mobile-nav-toggle">
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
		</a>
	</div>
</header>

    <section>
      <!-- Index -->

<div class="hero">
	<figure class="hero-figure">
		<img class="hero-figure-img" src="/images/banner/banner.jpg" alt="AI架构 | AI系统基础架构设计与优化">
		<figcaption>
			<div class="container">
				<div class="figure-inset">
					<h2 class="h1">深入AI架构的奥秘</h2>
					<p>创造卓越的智能解决方案</p>
				</div>
			</div>
		</figcaption>
		<div class="learn-more">
			<a class="anchor" href="#landingpage">
				<img id="landingpage" src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='24' height='24' fill='white' viewBox='0 0 16 16'><path d='M8 3a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 3zm4 8a4 4 0 0 1-8 0V5a4 4 0 1 1 8 0v6zM8 0a5 5 0 0 0-5 5v6a5 5 0 0 0 10 0V5a5 5 0 0 0-5-5z'/></svg>" alt="">
			</a>
		</div>
	</figure>
</div>


<section class="section py-5 bg-light">
  <div class="container">
    <!-- 
  Data Files: source/_data/culture.yml
-->
<div class="row">
  
</div>
  </div>
</section>


  <section class="section py-5 " id="">
    <div class="container">
      <div class="section-heading text-center mb-5">
        <h3>文章</h3>
        <p class="text-gray">汇聚热点话题 打造创新思路</p>
      </div>

      
        <div class="section-body">
          
  <div class="row flex-wrap">
    
      <div class="col-3">
        <article id="post-sd-webui-只能由一个用户同时使用吗？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/20/sd-webui-%E5%8F%AA%E8%83%BD%E7%94%B1%E4%B8%80%E4%B8%AA%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%90%97%EF%BC%9F/">sd-webui 只能由一个用户同时使用吗？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-19T23:36:30.000Z" itemprop="datePublished">
  2023-06-20
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>有人在AUTOMATIC1111问：<br>我在带有 NVIDIA Tesla T4 的 GPU 服务器上部署了一个 sd webui，并配置了一个远程服务，这意味着任何地方的任何人都可以通过 GPU 服务器的公共 IP 地址访问 sd webui url。</p>
<p>例如，Li 从纽约访问 webui url，而 James 从芝加哥访问 webui url</p>
<p>但是我发现如果李正在生成图像，同时，詹姆斯点击生成按钮，他必须等待李完成她的生成工作（webui 显示“等待，排队”）。真是令人费解，李在生成图像时GPU服务器几乎空闲，为什么詹姆斯不能立即开始生成？如果我希望 2 或 200 人可以同时从不同的地方开始生成图像，我该怎么办？</p>
<p>Q：如何配置为“启动多个 WebUI 实例，我是新手？</p>
<p>A: 如果启动多个WebUI，它们会默认挂载在不同的端口上，可以并行工作。 由于 Python 的性质，单实例并行是不可能的。</p>
<p>总结：在不同的 web 端口上启动 web 服务，并部署负载平衡以响应多用户生成的请求。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-controlnet更新" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/18/controlnet%E6%9B%B4%E6%96%B0/">controlnet更新</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T23:50:57.000Z" itemprop="datePublished">
  2023-06-18
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>在你沒注意的時候ControlNet又更新了幾次? ControlNet更新總整理!! - YouTube<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=PTyWM15W7mg">https://www.youtube.com/watch?v=PTyWM15W7mg</a></p>
<p>Transcript:</p>
<p>大家好，我是艾粒，我是杰克。今天我们来更新一下信息。从上一篇tile模型更新后，ControlNet又用非常惊人的速度持续更新。今天我们来看看，到底又更新了些什么吧。在开始之前，我们拍这片的时候，ControlNet的版本是1.1.201版。那就开始吧。首先，ControlNet最让人诟病的问题就是模型跟预处理器非常的乱。现在的ControlNet预处理器大概有40个以上，然后模型也有十几个。每一个选择都非常花时间。现在终于出现了非常方便的选择器，只需要在画面上点击你要的内容，例如我选OpenPose，系统就会把OpenPose能够对应的预处理器放上去。如果你有对应的模型，系统也会帮你把模型放好，非常方便。<br>(00:16) 大家好我是艾粒 我是杰克 今天我們來更新一下資訊 從上一篇tile模型更新後 ControlNet又用非常驚人的速度持續更新 今天我們來看看 到底又更新了些什麼吧 在開始前說一下 我們拍這片的時候 Controlnet的版本是1.1.201版 那就開始吧 首先Controlnet最讓人詬病的問題 就是模型跟預處理器非常的亂 現在的Controlnet預處理器 大概有40個以上 然後模型也有十幾個 每一個選擇都非常花時間 現在終於出現了非常方便的選擇器 只需要在畫面上 點選你要的內容 例如我選openpose 系統就會把openpose 能夠對應的預處理器放上去 如果你有對應的模型 系統也會幫你把模型放好 非常的方便<br>(01:19) 之後不用在預處理器之海迷航啦 接下來我們來說說 又增加了什麼預處理器吧 第一個是Depth 他新增了一個leres++ 這個預處理器 對畫面的細分度更高 可以針對每一個區域 做出更明確的分類 我們來看一下對照圖 這是每一個預處理器 分別畫出五張 從中間挑出三張比較好的結果 在我看來 細緻度排名分別是這樣子 什麼意思呢 就是leres++ 對畫面的還原度最高 不過主要還是看你的需求 再選擇要使用的預處理器 接著我們看Tile Tile多了兩個預處理器 過去有使用過tile應該會知道 tile在放大動漫圖像的時候 會有顏色失真的問題 原本的顏色會跑掉很多 還有一些報告指出 某些部分會模糊 通常是因為CFG不足導致<br>(02:24) 所以目前tile針對這兩個問題 做了對應的處理 經過調整 已經不需要提高CFG去控制Tile了 另外增加了兩個預處理器 一個是tile_colorfix 他會針對顏色做補正 讓顏色不會跑掉 另一個是tile_colorfix+sharp 這個是顏色補正加上銳利化 下方還有可以調整銳利度的滑塊 這也是目前官方最推薦的預處理器 大家可以參考一下 目前在顏色修復上 我認為效果非常明顯 但是要注意 colorfix會把原圖的顏色鎖死 是沒有辦法靠提詞改變顏色的喔 再來是inpaint 在過去要使用inpaint模型的時候 通常我們會使用圖生圖的局部重繪 但是這樣會有一個問題 我們得要上面塗一次遮罩 下面ControlNet也圖一次遮罩 現在ControlNet不用這麼麻煩了<br>(03:27) 我們只需要在局部重繪的地方塗好遮罩 下方ControlNet的部分 不需要放入任何東西 只需要選擇局部重繪 ControlNet的啟用打勾 直接按下生產就可以了 系統會自動把局部重繪的圖像 送到ControlNet裡面 就不用像過去的教學一樣 上下都塗遮罩了 而inpaint目前多了一個 inpaint_only預處理器 這個預處理器是用在 不使用webui的局部重繪功能時 如果你用inpaint_only預處理器 系統就不會重繪遮罩以外的部分 最後是Reference 這是一個全新功能 他只有預處理器 不需要模型 Reference是一個概念簡單 但是非常強大的功能 我們先來說一下原理 Reference的意思是參考 那他怎麼參考的呢 其實就是將我們用來參考的圖像 先送進SD的注意力層 等到我們的提詞和參數進入的時候 做為參考<br>(04:31) 得出相近的結果 目前Reference有三種 參考的方式有點不同 我們一樣直接上對照圖 Reference_only 讀取的比較像是風格 Reference_adain 比較像是構圖 最後是Reference_adain+attn 這是前面兩個的結合 所以構圖跟風格都會轉移 這個功能有一個重點 他對參考圖像的品質要求很高 如果你的圖片是網頁上面的照片 很容易產生品質不佳的狀況 今天就介紹到這邊(? 等一下!! 在影片快做完的時候 ControlNet又更新了4次 我們又有一個重大的更新 大家知道Adobe的Filefly 前陣子發布了 生成式填色的功能吧 不使用任何提詞 就可以獲得很棒的重繪結果 這個功能甚至可以輕鬆的向外繪製 前幾天如果你問 SD能不能做一樣的事 那答案是否定的 我們不使用任何提詞<br>(05:34) 就沒有辦法達成這個效果 而且結果並沒有辦法非常多樣化 但就在前不久 ControlNet做到了 我們現在可以用SD 重現類似的成果 做法很簡單 只要再文生圖的地方使用inpaint 需要的只有高解析度修復打勾 CFG官方建議小於5 接著打開ControlNet 模型選擇局部重繪 下方的控制模式 改成ControlNet更重要 接著放上你想要填色的圖片 這邊要注意你的預處理器了 使用inpaint only 才不會改變遮罩以外的圖像 針對要填色的區域塗上遮罩後 只要按下產生就可以了 如果你想要畫面多做一些改變 只要加上少量的提 詞 就能夠獲得很好的成果 那如果想要向外繪製呢 我們現在也可以用inpaint來達成了 只要把ControlNet的縮放模式<br>(06:38) 改為調整大小並填充 接著調整你需要擴張的像素 按下一次產生 就會獲得一張向外擴充的圖片 接著再把這張圖片 放入ControlNet中 把需要擴充的內容圖上遮罩 當然也可以再其他繪圖軟體中先做好 再放到SD裡面 就不需要只是為了把圖像擴張(口誤) 而多執行一次 說不定你看到這部影片的時候 已經不需要這樣做了 最後只要按下產生就完成了 今天真的就介紹到這邊 各位看完這篇的時候 ControlNet可能又更新了好幾次了 等過陣子 我們再來整理ControlNet又會了些什麼吧 那就這樣啦 下次見 掰</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-Stable-Diffusion的工作原理" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/17/Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">Stable Diffusion的工作原理</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T08:07:29.000Z" itemprop="datePublished">
  2023-06-17
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 是一种文本到图像的潜在扩散模型，由<a target="_blank" rel="noopener" href="https://github.com/CompVis">CompVis</a>、<a target="_blank" rel="noopener" href="https://stability.ai/">Stability AI</a>和<a target="_blank" rel="noopener" href="https://laion.ai/">LAION</a>的研究人员和工程师创建。它使用来自<a target="_blank" rel="noopener" href="https://laion.ai/blog/laion-5b/">LAION-5B</a>数据库子集的 512x512 图像进行训练。 <em>LAION-5B</em>是目前最大的、可免费访问的多模态数据集。</p>
<p>看过Stable Diffusion可以产生的高质量图像后，让我们尝试更好地了解模型的功能。Stable Diffusion基于一种特殊类型的扩散模型，称为<strong>Latent Diffusion</strong>（<strong>潜在扩散</strong>）, 由本文提出：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>。</p>
<p>一般来说，扩散模型是机器学习系统，经过训练可以逐步对随机高斯噪声<em>进行去噪</em>，以获得感兴趣的样本，例如<em>图像</em>。要更详细地了解它们的工作原理，请查看<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">此 colab</a>。</p>
<p>扩散模型已显示可实现生成图像数据的最先进结果。但扩散模型的一个缺点是反向去噪过程很慢，因为它具有重复的、连续的性质。此外，这些模型会消耗大量内存，因为它们在像素空间中运行，在生成高分辨率图像时像素空间会变得很大。因此，训练这些模型并将它们用于推理具有挑战性。</p>
<p><em>潜在扩散可以通过在较低维度的潜在</em>空间上应用扩散过程而不是使用实际像素空间来降低内存和计算复杂性。这是标准扩散模型和潜在扩散模型之间的主要区别：<strong>在潜在扩散中，模型经过训练以生成图像的潜在（压缩）表示。</strong></p>
<p>潜在扩散有三个主要组成部分。</p>
<ol>
<li>A autoencoder (VAE).</li>
<li>A <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq">U-Net</a>.</li>
<li>A text-encoder, <em>e.g.</em> <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIP’s Text Encoder</a>.</li>
</ol>
<p><strong>1. The autoencoder (VAE)</strong></p>
<p>VAE模型有两部分，编码器和解码器。编码器用于将图像转换为低维潜在表示，作为<em>U-Net</em>模型的输入。相反，解码器将潜在表示转换回图像。</p>
<p>在潜在扩散<em>训练</em>期间，编码器用于获取图像的潜在表示（<em>潜在）用于前向扩散过程，它在每一步应用越来越多的噪声。</em>在<em>推理过程</em>中，使用 VAE 解码器将反向扩散过程生成的去噪潜伏转换回图像。我们在推理过程中，我们<strong>只需要 VAE 解码器</strong>。</p>
<p><strong>2. The U-Net</strong></p>
<p>U-Net 的编码器部分和解码器部分均由 ResNet 块组成。编码器将图像表示压缩为较低分辨率的图像表示，解码器将较低分辨率的图像表示解码回据称噪声较小的原始高分辨率图像表示。更具体地说，U-Net 输出预测可用于计算预测去噪图像表示的噪声残差。</p>
<p>为了防止 U-Net 在下采样时丢失重要信息，通常在编码器的下采样 ResNet 和解码器的上采样 ResNet 之间添加快捷连接。此外，稳定扩散 U-Net 能够通过交叉注意层在文本嵌入上调节其输出。通常在 ResNet 块之间将交叉注意层添加到 U-Net 的编码器和解码器部分。</p>
<p><strong>3.文本编码器</strong></p>
<p>文本编码器负责将输入提示（<em>例如</em>“An astronaut riding a horse”）转换为 U-Net 可以理解的嵌入空间。它通常是一个简单的<em>基于转换器的</em>编码器，将输入标记序列映射到潜在文本嵌入序列。</p>
<p>受<a target="_blank" rel="noopener" href="https://imagen.research.google/">Imagen</a>的启发，Stable Diffusion 在训练期间不<strong>训练</strong>文本编码器，而只是使用 CLIP 已经训练好的文本编码器<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a>。</p>
<p><strong>为什么潜在扩散快速有效？</strong></p>
<p>由于潜在扩散在低维空间上运行，因此与像素空间扩散模型相比，它大大降低了内存和计算要求。例如，Stable Diffusion 中使用的自动编码器的缩减系数为 8。这意味着形状图像<code>(3, 512, 512)</code>进入<code>(3, 64, 64)</code>潜在空间，这需要<code>8 × 8 = 64</code>更少的内存。</p>
<p>这就是为什么可以<code>512 × 512</code>如此快速地生成图像，即使在 16GB Colab GPU 上也是如此！</p>
<p><strong>推理过程中的稳定扩散</strong></p>
<img src="/2023/06/17/Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion.png" class="" title="SD管道">

<p>稳定扩散模型将latent seed和 text prompt作为输入。然后使用latent seed生成大小为64×64的随机潜在图像，而text prompt通过 CLIP 的文本编码器转换为大小77×768的text embeddings。</p>
<p>接下来，U-Net在以text embeddings为条件的同时迭代地对随机潜在图像表示<em>进行去噪。</em>U-Net 的输出是噪声残差，用于通过调度程序算法计算去噪的潜在图像表示。许多不同的调度程序算法可用于此计算，每个算法都有其优点和缺点。对于稳定扩散，我们建议使用以下之一：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py">PNDM scheduler</a> (used by default)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py">DDIM scheduler</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py">K-LMS scheduler</a></li>
</ul>
<p>关于调度程序算法功能如何超出本笔记本范围的理论，但简而言之，应该记住他们根据先前的噪声表示和预测的噪声残差计算预测的去噪图像表示。有关更多信息，建议查看<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364">Elucidating the Design Space of Diffusion-Based Generative Models</a></a></p>
<p>重复去噪<em>过程**。</em>50 次逐步检索更好的潜在图像表示。完成后，潜在图像表示由变分自动编码器的解码器部分解码。</p>
<h2 id="编写自己的推理管道"><a href="#编写自己的推理管道" class="headerlink" title="编写自己的推理管道"></a>编写自己的推理管道</h2><p>最后，我们展示如何将 Stable Diffusion 与不同的调度器一起使用，即<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/pull/185">本 PR中添加的</a><a target="_blank" rel="noopener" href="https://github.com/crowsonkb">Katherine Crowson 的</a>K-LMS 调度器。</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main">预训练模型</a>包括设置完整扩散管道所需的所有组件。它们存储在以下文件夹中：</p>
<ul>
<li><code>text_encoder</code>: Stable Diffusion 使用 CLIP，但其他扩散模型可能使用其他编码器，例如<code>BERT</code>.</li>
<li><code>tokenizer</code>. 它必须与模型使用的相匹配<code>text_encoder</code>。</li>
<li><code>scheduler</code>：用于在训练期间逐步向图像添加噪声的调度算法。</li>
<li><code>unet</code>：用于生成输入的潜在表示的模型。</li>
<li><code>vae</code>：自动编码器模块，我们将使用它来将潜在表示解码为真实图像。</li>
</ul>
<p>我们可以通过引用保存组件的文件夹来加载组件，<code>from_pretrained</code> 中传递<code>subfolder</code></p>
<p>首先，您应该安装<code>diffusers==0.10.2</code>以运行以下代码片段：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install diffusers==0.10.2 transformers scipy  accelerate</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPTextModel, CLIPTokenizer</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoencoderKL, UNet2DConditionModel, PNDMScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load the autoencoder model which will be used to decode the latents into image space. </span></span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;vae&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Load the tokenizer and text encoder to tokenize and encode the text. </span></span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. The UNet model for generating the latents.</span></span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>,  subfolder=<span class="string">&quot;unet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>现在我们不再加载预定义的调度程序，而是加载具有一些拟合参数的<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/71ba8aec55b52a7ba5a1ff1db1265ffdd3c65ea2/src/diffusers/schedulers/scheduling_lms_discrete.py#L26">K-LMS 调度程序</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line">scheduler = LMSDiscreteScheduler(beta_start=<span class="number">0.00085</span>, beta_end=<span class="number">0.012</span>, beta_schedule=<span class="string">&quot;scaled_linear&quot;</span>, num_train_timesteps=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p>接下来，让我们将模型移动到 GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch_device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">vae.to(torch_device)</span><br><span class="line">text_encoder.to(torch_device)</span><br><span class="line">unet.to(torch_device) </span><br></pre></td></tr></table></figure>

<p>我们现在定义我们将用于生成图像的参数。</p>
<p>请注意，<code>guidance_scale</code>它的定义类似于<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11487.pdf">Imagen 论文</a><code>w</code>中等式 (2) 的指导权重。对应于不进行无分类器指导。在这里，我们将其设置为 7.5，就像之前所做的那样。<code>guidance_scale == 1</code></p>
<p>与前面的示例相比，我们设置<code>num_inference_steps</code>为 100 以获得更清晰的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">prompt = [<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>]</span><br><span class="line"></span><br><span class="line">height = <span class="number">512</span>                        <span class="comment"># default height of Stable Diffusion</span></span><br><span class="line">width = <span class="number">512</span>                         <span class="comment"># default width of Stable Diffusion</span></span><br><span class="line"></span><br><span class="line">num_inference_steps = <span class="number">100</span>           <span class="comment"># Number of denoising steps</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">7.5</span>                <span class="comment"># Scale for classifier-free guidance</span></span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">0</span>)    <span class="comment"># Seed generator to create the inital latent noise</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="built_in">len</span>(prompt)</span><br></pre></td></tr></table></figure>



<p>首先，我们得到<code>text_embeddings</code>传递的提示。这些嵌入将用于调整 UNet 模型并引导图像生成类似于输入提示的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text_input = tokenizer(prompt, padding=<span class="string">&quot;max_length&quot;</span>, max_length=tokenizer.model_max_length, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p>我们还将获得用于无分类器指导的无条件文本嵌入，它们只是填充标记（空文本）的嵌入。它们需要与条件<code>text_embeddings</code>(<code>batch_size</code>和<code>seq_length</code>)具有相同的形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_length = text_input.input_ids.shape[-<span class="number">1</span>]</span><br><span class="line">uncond_input = tokenizer(</span><br><span class="line">    [<span class="string">&quot;&quot;</span>] * batch_size, padding=<span class="string">&quot;max_length&quot;</span>, max_length=max_length, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line">uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[<span class="number">0</span>]   </span><br></pre></td></tr></table></figure>



<p>对于无分类器指导，我们需要进行两次前向传递：一次使用条件输入 ( <code>text_embeddings</code>)，另一个使用无条件嵌入 ( <code>uncond_embeddings</code>)。在实践中，我们可以将两者连接成一个批次，以避免进行两次前向传递。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br></pre></td></tr></table></figure>



<p>接下来，我们生成初始随机噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">latents = torch.randn(</span><br><span class="line">    (batch_size, unet.in_channels, height // <span class="number">8</span>, width // <span class="number">8</span>),</span><br><span class="line">    generator=generator,</span><br><span class="line">)</span><br><span class="line">latents = latents.to(torch_device)</span><br></pre></td></tr></table></figure>



<p>如果我们在这个阶段检查 ，<code>latents</code>我们会看到它们的形状是<code>torch.Size([1, 4, 64, 64])</code>，比我们想要生成的图像小得多。<code>512 × 512</code>该模型稍后会将这种潜在表示（纯噪声）转换为图像。</p>
<p>接下来，我们用我们选择的<code>num_inference_steps</code>. 这将计算<code>sigmas</code>去噪过程中要使用的确切时间步长值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br></pre></td></tr></table></figure>



<p>K-LMS 调度程序需要将 乘以<code>latents</code>它的<code>sigma</code>值。让我们在这里这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latents = latents * scheduler.init_noise_sigma</span><br></pre></td></tr></table></figure>



<p>我们准备编写去噪循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm(scheduler.timesteps):</span><br><span class="line">    <span class="comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span><br><span class="line">    latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict the noise residual</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform guidance</span></span><br><span class="line">    noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">    latents = scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>



<p>我们现在使用<code>vae</code>将生成的解码<code>latents</code>回图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scale and decode the image latents with vae</span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    image = vae.decode(latents).sample</span><br></pre></td></tr></table></figure>



<p>最后，让我们将图像转换为 PIL，以便我们可以显示或保存它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image = (image / <span class="number">2</span> + <span class="number">0.5</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">image = image.detach().cpu().permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).numpy()</span><br><span class="line">images = (image * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line">pil_images = [Image.fromarray(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">pil_images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_k_lms.png"><img src="/./Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion_k_lms.png" alt="PNG"></a></p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-图像生成热门项目" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/17/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%83%AD%E9%97%A8%E9%A1%B9%E7%9B%AE/">AIGC图像生成热门项目</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T04:17:42.000Z" itemprop="datePublished">
  2023-06-17
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <h2 id="Popular-Tasks-amp-Pipelines"><a href="#Popular-Tasks-amp-Pipelines" class="headerlink" title="Popular Tasks &amp; Pipelines"></a>Popular Tasks &amp; Pipelines</h2><table>
<thead>
<tr>
<th>Task</th>
<th>Pipeline</th>
<th>🤗 Hub</th>
</tr>
</thead>
<tbody><tr>
<td>Unconditional Image Generation</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm">DDPM</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google/ddpm-ema-church-256">google&#x2F;ddpm-ema-church-256</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img">Stable Diffusion Text-to-Image</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/unclip">unclip</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/kakaobrain/karlo-v1-alpha">kakaobrain&#x2F;karlo-v1-alpha</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/if">if</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0">DeepFloyd&#x2F;IF-I-XL-v1.0</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet">Controlnet</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/lllyasviel/sd-controlnet-canny">lllyasviel&#x2F;sd-controlnet-canny</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/pix2pix">Instruct Pix2Pix</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/timbrooks/instruct-pix2pix">timbrooks&#x2F;instruct-pix2pix</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img">Stable Diffusion Image-to-Image</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5</a></td>
</tr>
<tr>
<td>Text-guided Image Inpainting</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint">Stable Diffusion Inpaint</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-inpainting">runwayml&#x2F;stable-diffusion-inpainting</a></td>
</tr>
<tr>
<td>Image Variation</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation">Stable Diffusion Image Variation</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers">lambdalabs&#x2F;sd-image-variations-diffusers</a></td>
</tr>
<tr>
<td>Super Resolution</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale">Stable Diffusion Upscale</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler">stabilityai&#x2F;stable-diffusion-x4-upscaler</a></td>
</tr>
<tr>
<td>Super Resolution</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale">Stable Diffusion Latent Upscale</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler">stabilityai&#x2F;sd-x2-latent-upscaler</a></td>
</tr>
</tbody></table>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-Stable Diffusion：什么是 LoRA 模型以及如何使用它们？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/">Stable Diffusion：什么是 LoRA 模型以及如何使用它们？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-15T14:00:00.000Z" itemprop="datePublished">
  2023-06-15
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 已经风靡全球，让任何人都可以<a target="_blank" rel="noopener" href="https://softwarekeep.com/help-center/stable-diffusion-for-free-beginner-guide">免费生成 AI 驱动的艺术作品</a>。但是，如果您曾想生成知名人物、概念或使用特定风格的图像，您可能会对结果感到失望。Stable Diffusion 强大的 AI 本身并不能很好地使角色和风格栩栩如生，这很常见。幸运的是，LoRA 模型可以帮助解决这个问题。</p>
<p>有这么多惊人的扩展和模型可用于增强稳定扩散，LoRA 模型如此受欢迎也就不足为奇了。但是 LoRA 模型到底做了什么？您如何使用它使您的艺术更加壮观？</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/howtouselorainsd-1686843893059-1.png" class="" title="稳定扩散：什么是 LoRA 模型以及如何使用它们？">

<p>在本初学者指南中，我们探讨了什么是 LoRA 模型、在哪里可以找到它们以及如何在 Automatic1111 的 Web GUI 中使用它们，以及一些 LoRA 模型演示。</p>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a><strong>目录</strong></h4><ol>
<li>什么是 LoRA？</li>
<li>LoRA 模型类型</li>
<li>在哪里可以找到和下载 LoRA 模型</li>
<li>LoRA 模型安装到 Automatic1111</li>
<li>如何在 Automatic1111 中使用 LoRA 模型</li>
</ol>
<h2 id="什么是-LoRA？"><a href="#什么是-LoRA？" class="headerlink" title="什么是 LoRA？"></a><strong>什么是 LoRA？</strong></h2><p>LoRA 代表低秩适应。它允许您使用低秩自适应技术来快速微调扩散模型。简单来说，LoRA 训练模型可以更轻松地针对不同的概念（例如角色或特定风格）训练 Stable Diffusion。然后，这些训练有素的模型可以导出并供他们这一代的其他人使用。</p>
<p>稳定扩散模型因其生成高质量图像和文本的能力而在机器学习领域越来越受欢迎。然而，这些模型的一个主要缺点是它们的文件很大，这使得用户很难在他们的个人计算机上维护一个集合。这就是 LoRA 作为一种训练技术发挥作用的地方，它可以在保持可管理的文件大小的同时微调稳定扩散模型。</p>
<p><a target="_blank" rel="noopener" href="https://softwarekeep.com/help-center/best-stable-diffusion-models-to-try">LoRA 模型是小型稳定扩散模型，它对标准检查点模型</a>应用较小的更改，导致文件大小减少 2-500 MB，比检查点文件小得多。LoRA 在文件大小和训练能力之间提供了一个很好的权衡，使它们成为对拥有大量模型的用户有吸引力的解决方案。</p>
<h2 id="LoRA-模型类型"><a href="#LoRA-模型类型" class="headerlink" title="LoRA 模型类型"></a><strong>LoRA 模型类型</strong></h2><p>我们可以将 LoRA 模型分为几种不同的类型：</p>
<h3 id="Character-LoRA"><a href="#Character-LoRA" class="headerlink" title="Character LoRA"></a><strong>Character LoRA</strong></h3><p>在特定角色（例如卡通或视频游戏角色）上训练的模型。Character LoRA 能够准确地重现角色的外观和感觉，以及与其相关的任何关键特征。这是最常见的 LoRA 类型，因为在没有这种训练数据的情况下生成字符通常很棘手且不一致。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_character-1686843893059-3.png" class="" title="字符LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/1274/dreamlike-diffusion-10">Dreamlike Diffusion 1.0</a></li>
<li><strong>使用 LoRA</strong> ： <a target="_blank" rel="noopener" href="https://civitai.com/models/55339/the-joker-or-photorealistic-joker-lora">The Joker | 真实感</a></li>
<li><strong>使用提示</strong>：小丑肖像，高质量，8k</li>
</ul>
<p>应用角色 LoRA 可以让您快速生成具有真实外观的角色，使它们非常适合 AI 插图、角色概念艺术，甚至参考表。根据模型的训练，角色可能适合一套衣服、一种特定的发型，甚至是某种面部表情。然而，某些角色 LoRA 可以将您选择的角色放入新的服装和设置中，从而增加他们的魅力。</p>
<p>Character LoRA 适用于各种媒体，包括流行和鲜为人知的标题。您会发现来自超级马里奥、漫威和神奇宝贝等热门系列的角色，以及众多日本动漫角色，甚至是漫画书中的英雄。</p>
<p>当然，角色LoRA也可以应用于原始角色，只要有足够的训练数据即可。虽然低训练数据的实验正在进行中，但最好使用至少 10-20 个不同的图像来创建角色 LoRA。这将为您的训练过程增添多样性，提高生成角色的质量。</p>
<h3 id="Style-LoRA"><a href="#Style-LoRA" class="headerlink" title="Style LoRA"></a><strong>Style LoRA</strong></h3><p>Style LoRA 与角色 LoRA 有许多相似之处，但它不是针对特定角色或对象进行训练，而是侧重于艺术风格。这种类型的模型通常由特定艺术家进行艺术训练，让您可以在自己的作品中使用他们的标志性风格。Style LoRA 可用于任何事情，从风格化参考图像到创建相同风格的原创艺术作品。</p>
<p>顾名思义，这些模型针对特定风格进行训练，例如动画表演、水彩画、线稿等的特定外观。使用这种类型的 LoRA 模型，您可以轻松地为您的 AI 作品赋予与众不同的独特风格！</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_style-1686843893060-5.png" class="" title="风格LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/23900?modelVersionId=29792">AnyLoRA - Checkpoint</a></li>
<li><strong>使用的 LoRA</strong> : <a target="_blank" rel="noopener" href="https://civitai.com/models/7094/arcane-style-lora">Arcane Style LoRA</a></li>
<li><strong>使用提示</strong>：神秘风格，1girl，粉红色头发，长发，一条辫子，白衬衫，外套，黄色眼睛，看着观众，城市街道</li>
</ul>
<p>我们使用根据 Netflix 节目 Arcane 的风格训练的 LoRA 模型生成了一幅新的 AI 艺术品。该模型能够在原剧中没有出现的角色上捕捉到剧中鲜艳的色彩和独特的角色设计。</p>
<p>样式 LoRA 的优点在于它们与常规的稳定扩散检查点一起工作，使您无需合并大型模型即可创建令人惊叹的独特作品。例如，使用现实主义检查点和绘画风格 LoRA 将生成看起来像绘画的逼真图像。</p>
<h3 id="Concept-LoRA"><a href="#Concept-LoRA" class="headerlink" title="Concept LoRA"></a><strong>Concept LoRA</strong></h3><p>Concept LoRA 是一种特殊的 LoRA，它是针对特定概念或想法进行训练的。这些模型通常旨在概念化一些特定的东西，而这些东西很难通过简单的即时工程实现。例如，这种类型的 LoRA 可以针对特定的情绪、动作或非常特定的项目进行训练。</p>
<p>当您尝试创建传达特定概念的原创艺术作品时，这种类型的模型特别有用。例如，如果您想生成玻璃雕塑的图像，您可以使用针对该确切想法训练的概念 LoRA。结果将是一件独特而有趣的艺术品，清楚地传达了您的目标概念。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_concept-1686843893060-7.png" class="" title="概念LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/1274/dreamlike-diffusion-10">Dreamlike Diffusion 1.0</a></li>
<li><strong>使用的 LoRA</strong> ：<a target="_blank" rel="noopener" href="https://civitai.com/models/11203/glass-sculptures">玻璃雕塑</a></li>
<li><strong>使用提示</strong>：芭蕾舞演员、舞蹈、半透明、玻璃雕塑、倒影</li>
</ul>
<p>Concept LoRA 使创建风格化和概念性强的艺术作品变得更加容易。它们也非常适合创建更小、更模糊的作品，而这些作品很难用其他模型生成。因此，它们通常可以让您的作品在独特性和艺术价值方面更具优势。</p>
<h3 id="Pose-LoRA"><a href="#Pose-LoRA" class="headerlink" title="Pose LoRA"></a><strong>Pose LoRA</strong></h3><p>将姿势 LoRA 应用到你这一代就像听起来一样 - 它会以某种方式摆出你的角色。这对于生成动态场景非常有用，您可以在其中生成特定的姿势和动作，而这些姿势和动作通常很难通过常规的提示工程来实现。</p>
<p>Pose LoRA 模型更关注所述角色的姿势，而不是其风格或特征。例如，如果您要将姿势 LoRA 模型应用于人形角色，它会为他们创建不同的姿势，例如跑步、跳跃或坐着，但不会改变他们的特征、服装或改变模型的风格你正在使用。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_pose-1686843893060-9.png" class="" title="姿势LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/36520/ghostmix">GhostMix</a></li>
<li><strong>使用 LoRA</strong> ： <a target="_blank" rel="noopener" href="https://civitai.com/models/51900/shinji-in-a-chair-or-meme">Shinji in a Chair | 模因</a></li>
<li><strong>使用提示</strong>：独奏，男性焦点，坐着，低着头，黑色短发，连帽夹克，牛仔裤，运动鞋</li>
</ul>
<p>Pose LoRA 是一种很好的方式，可以更好地控制你的世代，而无需安装和学习更高级的解决方案，如 ControlNet。这种类型的 LoRA 只需对原始提示进行一些简单的更改，就可以帮助您创建动态有趣的场景。</p>
<h3 id="Clothing-LoRa"><a href="#Clothing-LoRa" class="headerlink" title="Clothing LoRa"></a><strong>Clothing LoRa</strong></h3><p>另一个有用的模型是服装 LoRA。如您所料，这种类型的 LoRA 模型旨在改变一个人的衣服和配饰。有了它，您可以快速轻松地为任何角色赋予新衣服，无论是现代风格还是历史风格。</p>
<p>这些模型的优点在于它们适用于任何类型的角色。只需一个模型，您就可以应用各种不同的人的风格和设计！例如，如果您想创建一个角色穿着中国传统服饰的场景，只需将您选择的服装 LoRA 应用到您的这一代，瞧——即时的中国传统服装！</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_clothing-1686843893060-11.png" class="" title="服装LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/36520/ghostmix">GhostMix</a></li>
<li><strong>使用的LoRA</strong> ：<a target="_blank" rel="noopener" href="https://civitai.com/models/15365/hanfu">汉服</a></li>
<li><strong>使用提示</strong>：女孩，蓝色汉服，全身</li>
</ul>
<p>很多时候，即使您清楚地描述了角色的着装，Stable Diffusion 也可能无法最好地将您的想法变为现实。然而，在服装 LoRA 的帮助下，您可以微调角色的确切外观，并为您的作品带来额外的真实感。</p>
<h3 id="Object-LoRA"><a href="#Object-LoRA" class="headerlink" title="Object LoRA"></a><strong>Object LoRA</strong></h3><p>最后但同样重要的是，我们有对象 LoRA。这是一个广泛的 LoRA 模型类别，用于生成家具、植物甚至车辆等对象。当然，您可以使用这些模型创建的项目类型取决于您使用的特定模型和您提供的提示。</p>
<p>但是，该术语也适用于用于创建更抽象对象的 LoRA，例如游戏或网站的 UI 元素。这对于为您的项目创建更具凝聚力的外观非常有用。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_object-1686843893060-13.png" class="" title="Object LoRA">

<ul>
<li><strong>使用型号</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/57931/szechuan-special-sauce?modelVersionId=62378">川菜特制酱料</a></li>
<li><strong>LoRA 使用</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/58247/product-design-dark-minimalism-eddiemauro-lora">产品设计（黑暗极简主义-eddiemauro）</a></li>
<li><strong>使用提示</strong>：未来派水壶，计算机渲染，极简主义，4k</li>
</ul>
<p>Object LoRA 不仅是艺术家的宝贵工具，也是游戏开发人员、网页设计师和其他需要高效创建资产的创意专业人士的宝贵工具。能够生成具有自定义设计的对象让您可以自由地试验和探索不同的视觉效果，直到找到最适合您的项目的视觉效果。</p>
<h2 id="在哪里可以找到和下载-LoRA-模型"><a href="#在哪里可以找到和下载-LoRA-模型" class="headerlink" title="在哪里可以找到和下载 LoRA 模型"></a><strong>在哪里可以找到和下载 LoRA 模型</strong></h2><p>LoRA 模型在各种开源存储库中可用，包括 <a target="_blank" rel="noopener" href="https://civitai.com/">Civitai</a>和<a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a>。这些模型可免费使用，只需几个简单的步骤即可轻松下载。这些模型最好的地方在于它们的大小——大多数 LoRA 模型不超过几兆字节，这使得它们非常轻巧且易于使用。</p>
<p>下载您要使用的模型后，您必须将它们安装到正确的文件夹中。根据您的设置，这可能会发生变化。在本文中，我们将探索 LoRA 模型与 Automatic1111 webUI 的结合使用，但您可以研究您的平台以获取有关使用 LoRA 模型的具体说明。</p>
<h2 id="如何将-LoRA-模型安装到-Automatic1111-中"><a href="#如何将-LoRA-模型安装到-Automatic1111-中" class="headerlink" title="如何将 LoRA 模型安装到 Automatic1111 中"></a><strong>如何将 LoRA 模型安装到 Automatic1111 中</strong></h2><p>在将模型放入 webUI 之前，无论您使用什么平台生成图像，您很可能需要安装 LoRA 扩展本身。以下是安装 Automatic1111 扩展的方法：</p>
<ol>
<li>首先，启动 Automatic1111 网络用户界面。</li>
<li>打开“<strong>扩展”选项卡，然后</strong>从可用选项中单击“<strong>从 URL 安装”。</strong></li>
<li>将以下链接粘贴到“<strong>扩展的 git 存储库的 URL</strong> ”输入字段中，然后按“<strong>安装</strong>”按钮：<a target="_blank" rel="noopener" href="https://github.com/kohya-ss/sd-webui-additional-networks.git">https://github.com/kohya-ss/sd-webui-additional-networks.git</a><img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/Photoshop_s6W3JR02z5-1686843893060-15.png" class="" title="安装 lora 扩展"></li>
<li>切换到“ <strong>Installed</strong> ”选项卡，然后单击“ <strong>Apply and restart UI</strong> ”按钮。现在，等待 Automatic1111 Web UI 重新启动。<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/Photoshop_MYVL9qfa6j-1686843893060-17.png" class="" title="重新启动用户界面"></li>
<li>执行这些步骤后，您应该注意到“ <strong>models</strong> ”文件夹中有一些新的子文件夹。这些模型将存储您的 LoRA。但是，您需要配置此文件夹，以便 Automatic1111 Web UI 可以读取它。</li>
<li>打开“<strong>设置</strong>”选项卡，然后切换到“<strong>其他网络</strong>”选项卡。找到“<strong>用于扫描 LoRA 模型的额外路径</strong>”输入字段并粘贴到正确的文件夹中。<strong>您会在“ stable-diffusion-webui&#x2F;models&#x2F;Lora ”目录</strong>中找到它。</li>
<li>输入 LoRA 文件夹的正确完整路径后，单击“<strong>应用设置</strong>”。</li>
</ol>
<p>这负责安装 LoRA 扩展，但是，这还不足以开始生成图像。您还需要将实际的 LoRA 模型安装到正确的文件夹中。为此，请获取下载的 LoRA 文件并将其放在“ <strong>stable-diffusion-webui&#x2F;models&#x2F;Lora</strong> ”文件夹中。</p>
<h2 id="如何在-Automatic1111-中使用-LoRA-模型"><a href="#如何在-Automatic1111-中使用-LoRA-模型" class="headerlink" title="如何在 Automatic1111 中使用 LoRA 模型"></a><strong>如何在 Automatic1111 中使用 LoRA 模型</strong></h2><p>一旦安装了您要使用的 LoRA 模型，您就可以开始使用它创建图像。</p>
<ol>
<li>启动 Automatic1111 Web UI 并选择所需的检查点模型。一些 LoRA 需要使用特定的检查点；始终检查您的 LoRA 的描述和说明。</li>
<li>输入您的提示。确保包括 LoRA 的触发词（如果有的话）。创作者通常会把这个词放在描述中，或者你可以在Civitai 上的“<strong>触发词”参数中找到它。</strong></li>
<li>单击“*<em>生成”按钮下的“*</em>**附加网络**”图标，然后切换到“ <strong>Lora</strong> ”选项卡。在这里，单击要使用的 Lora 以将其插入到提示符中。</li>
<li><strong>如果需要，通过将默认值“ 1</strong> ”修改为更低或更高的数字来更改 LoRA 的权重。例如，一些 LoRA 在“ <strong>0.6</strong> ”或“ <strong>1.2</strong> ”这样的权重上效果更好，这取决于它是如何训练的，以及你正在寻找什么样的结果。</li>
<li>完成生成设置的配置，然后单击“<strong>生成</strong>”按钮。</li>
</ol>
<p>您应该注意到 LoRA 已应用于生成的图像，使您能够处理更具体和独特的概念。您为配置 LoRA 所花费的时间和精力是非常值得的；结果可能是惊人的！</p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h4><p>LoRA 模型是为 AI 生成的图像添加更多细节和准确性的好方法。只需几个简单的步骤，您就可以开始将这些模型整合到您的 Automatic1111 工作流程中，让您的项目进入一个充满可能性的全新世界。我们建议密切关注最新版本，因为平台上总是会添加新的和有趣的模型。快乐创造！</p>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%95%86%E4%B8%9A%E5%8C%96%E5%9C%BA%E6%99%AF/" rel="tag">商业化场景</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-如何免费使用 Stable Diffusion – 新手指南" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/">如何免费使用 Stable Diffusion – 新手指南</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-15T14:00:00.000Z" itemprop="datePublished">
  2023-06-15
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 是最近开发的文本到图像模型，它利用深度学习从文本描述中生成高质量图像。这种新模型变得越来越流行，因为它能够从最模糊的文本输入中生成逼真的图像。</p>
<p>在当今快节奏的世界中，保持领先地位很重要，而做到这一点的方法之一就是掌握 Stable Diffusion 的终极力量。发现 Stable Diffusion 的终极力量 - 它的优势，它如何改善您和您的业务，以及免费的入门方式。</p>
<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/freestablediffusionhowto-1686843807242-1.png" class="" title="img">

<p>在本文中，我们将免费指导您完成在本地或云中成功运行 Stable Diffusion 所需的一切。</p>
<h2 id="什么是Stable-Diffusion？"><a href="#什么是Stable-Diffusion？" class="headerlink" title="什么是Stable Diffusion？"></a><strong>什么是Stable Diffusion？</strong></h2><p>Stable Diffusion 是一种深度学习模型，它利用一种称为扩散过程的技术从文本描述中生成图像。简而言之，当您给 Stable Diffusion 一个提示（例如“美丽的日落”）时，模型会被训练为生成与您的描述相匹配的事物的逼真图像。</p>
<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/chrome_m2o81cBmUS-1686843807243-3.png" class="" title="img">

<p>扩散过程涉及向输入图像添加噪声，然后随着时间的推移逐渐降低噪声以产生最终图像。这个过程产生的图像比传统的深度学习模型更详细、更逼真。</p>
<p>Stable Diffusion 是对之前文本到图像模型的重大改进，因为它可以处理复杂和抽象的文本描述。它通过使用一种称为稳定训练的新方法来实现这一点，该方法使模型能够生成与文本输入一致的高质量图像。</p>
<h3 id="如何使用Stable-Diffusion？"><a href="#如何使用Stable-Diffusion？" class="headerlink" title="如何使用Stable Diffusion？"></a><strong>如何使用Stable Diffusion？</strong></h3><p>Stable Diffusion 具有广泛的应用，包括电子商务、广告和视频游戏。例如，电子商务网站可以使用 Stable Diffusion 生成基于文本描述的产品图像，使它们能够展示可能没有真实照片的产品。</p>
<p>广告代理商可以使用 Stable Diffusion 为其活动创建独特且引人注目的视觉效果。同样，游戏开发人员可以使用该模型从文本描述中生成角色和环境等游戏资产。</p>
<p>Stability AI 是 Stable Diffusion 背后的革命性公司，已在许可下发布了该模型。由于 Stability AI 的商业使用政策，用户可以为任何目的（商业或非商业）生成图像。您可以放心地使用 Stable Diffusion，并在需要时创建令人惊叹的视觉效果！</p>
<h2 id="Stable-Diffusion入门"><a href="#Stable-Diffusion入门" class="headerlink" title="Stable Diffusion入门"></a><strong>Stable Diffusion入门</strong></h2><p>根据您的需要和资源，Stable Diffusion 可以以不同的方式运行。运行 Stable Diffusion 的两种最常见的方式是通过基于云的服务或在您的机器上本地运行。</p>
<h3 id="在本地使用Stable-Diffusion"><a href="#在本地使用Stable-Diffusion" class="headerlink" title="在本地使用Stable Diffusion"></a><strong>在本地使用Stable Diffusion</strong></h3><p>在本地运行 Stable Diffusion 需要安装和配置必要的库和依赖项。在本地使用时，初学者可以从安装 GUI（图形用户界面）中获益匪浅，该界面简化了配置过程并使管理和运行实验变得更加容易。</p>
<p>在本地运行模型的最小推荐 VRAM（视频随机存取存储器）为 4GB，这对某些用户来说可能太慢了。如果您确实可以访问具有超过 4GB VRAM 的机器，那么在本地运行 Stable Diffusion 可能是更好的选择。这样，您将节省成本并更好地控制图像生成。</p>
<p>当您在本地计算机上运行 Stable Diffusion 时，安装新模型、扩展、超网络和训练您自己的模型变得轻而易举。</p>
<p>使用最小设置会遇到的一个缺点是速度。虽然具有 4GB 或更少 VRAM 的设备可以运行 Stable Diffusion，但即使是少量生成也可能需要长达几分钟的时间。如果您急于获得结果并且没有备用硬件，那么云服务可能是您的最佳选择。</p>
<h3 id="在云中使用Stable-Diffusion"><a href="#在云中使用Stable-Diffusion" class="headerlink" title="在云中使用Stable Diffusion"></a><strong>在云中使用Stable Diffusion</strong></h3><p>基于云的服务提供了一种无需任何重大硬件投资即可运行 Stable Diffusion 的便捷方式。与在本地运行模型类似，在采用云路线时，您有许多选项可供选择。Google Colab、Runpod 和 RunDiffusion 等服务为用户提供了运行实验和生成图像的交互式环境。</p>
<p>云服务让您可以利用强大的硬件，例如最新的 NVIDIA Tesla GPU、NVIDIA 3090s 和 AMD Ryzen Threadripper。使用这种类型的硬件，您可以期望以比在本地运行快得多的速度使用 Stable Diffusion 生成图像。</p>
<p>然而，云服务有其自身的成本和局限性。例如，大多数云提供商无法上传您的自定义Stable Diffusion模型和其他高级模型，例如自定义 LoRa 和 VAE。如果您的后代严重依赖您微调的模型，某些云提供商可能无法提供您需要的支持。</p>
<h3 id="在线使用Stable-Diffusion"><a href="#在线使用Stable-Diffusion" class="headerlink" title="在线使用Stable Diffusion"></a><strong>在线使用Stable Diffusion</strong></h3><p><a target="_blank" rel="noopener" href="https://clipdrop.co/stable-diffusion">Clipdrop</a>、<a target="_blank" rel="noopener" href="https://www.happyaccidents.ai/">HappyAccidents</a>和<a target="_blank" rel="noopener" href="https://leonardo.ai/">Leonardo.ai</a>等网站为用户提供了一个易于使用的平台，用于根据文本输入生成图像。为了让每个人都可以生成图像，一些在线服务为各种模型提供“立即运行”功能。</p>
<p>对于那些没有时间或资源自行学习和配置 Stable Diffusion 的人来说，这些平台可能很有用。但是，这些在线服务的结果不如在本地或云端运行。在线服务通常使用过时的模型并提供有限的定制选项（如果有的话）。</p>
<p>大多数在线服务提供免费积分，或在需要订阅或购买积分之前提供有限数量的免费生成。这使得无需预先花任何钱就可以轻松加入并生成 AI 图像。</p>
<h3 id="本地、在线还是云端？如何决定"><a href="#本地、在线还是云端？如何决定" class="headerlink" title="本地、在线还是云端？如何决定"></a><strong>本地、在线还是云端？如何决定</strong></h3><p>无论您选择哪个选项，请务必记住，您的硬件越好，模型运行的速度就越快，您的结果也会越详细。您还应该考虑云提供商的限制和条款，因为它们可能差异很大。</p>
<p>决定是在本地还是在云中运行 Stable Diffusion 的最简单方法就是查看您的硬件。在本地运行模型的最小推荐 VRAM（视频随机存取存储器）为 4GB，这对某些用户来说可能太慢了。</p>
<p>总之，如果您正在寻找一种快速简便的方法来开始使用 Stable Diffusion，云服务可能是您的正确选择。但是，如果您愿意在硬件上进行投资，那么在本地运行 Stable Diffusion 始终是一个选项，它可以为您的图像生成提供更多功能和控制。</p>
<h2 id="如何免费运行稳定的扩散"><a href="#如何免费运行稳定的扩散" class="headerlink" title="如何免费运行稳定的扩散"></a><strong>如何免费运行稳定的扩散</strong></h2><p>在决定采用哪条路线后，您可能想知道如何开始免费使用 Stable Diffusion。幸运的是，有几种方法可以做到这一点。我们将为初学者探索一些开始使用该模型的选项。</p>
<h3 id="1-使用Automatc1111的Web-UI"><a href="#1-使用Automatc1111的Web-UI" class="headerlink" title="1.使用Automatc1111的Web UI"></a><strong>1.使用Automatc1111的Web UI</strong></h3><h4 id="你需要的东西"><a href="#你需要的东西" class="headerlink" title="你需要的东西"></a><strong>你需要的东西</strong></h4><ul>
<li>Windows操作系统</li>
<li>4GB 或更多显存（推荐）</li>
<li>20 GB 硬盘空间</li>
</ul>
<p>在没有用户界面的情况下运行 Stable Diffusion 对于初学者来说是相当困难的。虽然有许多不同的用户界面可用，但 Automatc1111 创建了一个用户友好的 Web UI，使您可以使用 Stable Diffusion 轻松生成图像、管理您的模型，甚至训练您自己的模型。</p>
<p>虽然存在许多其他界面，例如 Gradio 和 Invoke.ai，但 Automatic1111 Web UI 仍然处于领先地位，具有广泛的功能、稳定性和易用性。在本节中，我们将向您展示如何在本地计算机上安装它并开始生成图像。</p>
<ol>
<li>打开 Web 浏览器并转到<a target="_blank" rel="noopener" href="https://www.python.org/downloads/release/python-3107/">Python 下载页面</a>。向下滚动到页面底部的“<strong>文件”部分，找到“</strong> <strong>Windows Installer（64 位）</strong>”。单击一次开始下载。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/chrome_yKDCrnHm3Z-1686843807243-5.png" class="" title="img"></li>
<li>下载完成后，打开安装程序。您应该会看到一个带有几个选项的弹出窗口。确保启用“<strong>将 Python 3.10 添加到 PATH</strong> ”选项。然后，单击“<strong>立即安装</strong>”按钮。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/chrome_d0mIHjb7rW-1686843807243-7.png" class="" title="img"></li>
<li>再次打开 Web 浏览器并导航到<a target="_blank" rel="noopener" href="https://git-scm.com/download/win">Git for Windows 下载页面</a>。在“ <strong>Standalone Installer</strong> ”类别下，单击“ <strong>64-bit Git for Windows Setup</strong> ”链接开始下载。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/chrome_1tmNdm65Em-1686843807243-9.png" class="" title="img"></li>
<li>下载完成后，打开安装程序。您可能需要本地管理员帐户或密码才能安装 Git。</li>
<li>按照提示安装 Git。当您到达“<strong>选择组件</strong>”屏幕时，确保通过选中它们旁边的框来启用每个组件。然后，单击“<strong>下一步</strong>”继续。</li>
<li>当您到达“<strong>选择 Git 使用的默认编辑器</strong>”页面时，从下拉菜单中选择“<strong>使用记事本作为 Git 的默认编辑器”。</strong><img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/34324234-1686843807243-11.png" class="" title="img"></li>
<li>单击“<strong>下一步</strong>”继续使用默认设置进行其余安装。最后点击“ <strong>Install</strong> ”开始安装Git。</li>
<li>在您的计算机上的某个位置创建一个新文件夹来存储 Stable Diffusion。请注意，您应该选择一个具有足够可用存储空间的驱动器，因为该模型至少需要 20GB 的存储空间。</li>
<li>打开您创建的文件夹，然后单击文件资源管理器地址栏。将文件夹路径替换为“ <strong>cmd</strong> ”，然后按<strong>Enter 键</strong>。您会看到屏幕上出现一个命令提示符窗口，其中显示文件夹路径，如下图所示。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/explorer_92AE7m26N1-1686843807243-13.png" class="" title="img"></li>
<li>输入以下命令，然后按键盘上的<strong>回车</strong>键执行：**git clone <a target="_blank" rel="noopener" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">https://github.com/AUTOMATIC1111/stable-diffusion-webui</a><img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/WindowsTerminal_JfohNmsgQN-1686843807244-15.png" class="" title="img">**</li>
<li>下载完成后，打开您的 Stable Diffusion 文件夹，打开“ <strong>stable-diffusion-webui</strong> ”文件夹，然后双击“ <strong>webui-user.bat</strong> ”文件。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/explorer_UV6psuqYKM-1686843807244-17.png" class="" title="img"></li>
<li>等待终端安装所有必要的文件。这可能需要 20-30 分钟，并且您的计算机有时可能会变得无响应。<strong>当您看到“在本地 URL 上运行</strong>”文本出现时，您就会知道该过程已完成。<img src="/2023/06/15/%E5%A6%82%E4%BD%95%E5%85%8D%E8%B4%B9%E4%BD%BF%E7%94%A8%20Stable%20Diffusion%20%E2%80%93%20%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/localurlstablediffusion11111-1686843807244-19.png" class="" title="img"></li>
<li>复制本地 URL 并将其放入 Web 浏览器，然后按<strong>Enter 键</strong>。您现在可以开始生成图像了！</li>
</ol>
<p>确保保持终端运行以生成图像。如果关闭终端，Stable Diffusion 将停止，您必须使用“ <strong>webui-user.bat</strong> ”文件重新启动模型。</p>
<ul>
<li><a href="https://dubeno.github.io/tags/Stable-Diffusion/">Stable Diffusion</a></li>
</ul>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%95%86%E4%B8%9A%E5%8C%96%E5%9C%BA%E6%99%AF/" rel="tag">商业化场景</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-StableDiffusion在电商场景生成模特搭配服装的方案" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/">StableDiffusion在电商场景生成模特搭配服装方案</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-12T14:00:00.000Z" itemprop="datePublished">
  2023-06-12
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>本文节选自亚马逊博客</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Stable Diffusion 作为一个开源项目，目前被广泛应用在各个行业和领域做应用探索，本文介绍的方案应用场景是针对电商行业，一些电商公司在日常运营工作中，需要处理大量的模特搭配服装的产品图片，这些图片需要电商公司耗费成本去准备且时间周期也比较长。本方案使用 Stable Diffusion WebUI 及相关的扩展组件应用探索一种可以快速生成 AI 模特适配服装产品的方案，供大家参考。</p>
<ul>
<li>Stable Diffusion Web UI 是由 AUTOMATIC1111 开发的基于 Stable Diffusion AI 模型的 AI 图片处理工具，支持文生图、图生图。 目前 Stable Diffusion 模型微调主要有 4 种方式：Dreambooth、LoRA（Low-Rank Adaptation of Large Language Models）、Textual Inversion、Hypernetworks。</li>
<li>LoRA，英文全称 Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶自适应，它是一种可以将扩散模型带向全新层次的技巧。LoRA 最初设计是为了教模型学习新概念，目前为止大多数用来训练角色风格化艺术。</li>
<li>ControlNet 是一种通过添加额外条件来控制扩散模型的神经网络结构。它提供了一种增强稳定扩散的方法，在文本到图像生成过程中使用条件输入，如涂鸦、边缘映射、分割映射、Pose 关键点等。可以让生成的图像将更接近输入图像，这比传统的图像到图像生成方法有了很大的改进。</li>
</ul>
<p>注：以下使用的图片来源于网络，配合实验做了对应的图片效果处理。所有的 Prompt 词和参数只是参考样例，可以对参数配置做不断地优化去提升图片输出效果。</p>
<h2 id="1-实验环境准备"><a href="#1-实验环境准备" class="headerlink" title="1. 实验环境准备"></a>1. 实验环境准备</h2><h3 id="a-搭建-Stable-Diffusion-WebUI，由于相关版本和扩展更新比较快，直接去-Github-下载最新版本可以获得更多的功能支持能力"><a href="#a-搭建-Stable-Diffusion-WebUI，由于相关版本和扩展更新比较快，直接去-Github-下载最新版本可以获得更多的功能支持能力" class="headerlink" title="a. 搭建 Stable Diffusion WebUI，由于相关版本和扩展更新比较快，直接去 Github 下载最新版本可以获得更多的功能支持能力"></a>a. 搭建 Stable Diffusion WebUI，由于相关版本和扩展更新比较快，直接去 Github 下载最新版本可以获得更多的功能支持能力</h3><p>使用 GPU 实例构建单机版本的 WebUI 环境做日常的测试使用，推荐使用 G4dn 机型（NVIDIA T4 GPU，16 GiB 显存）或者 G5 机型（NVIDIA A10G GPU，24 GiB 显存）。</p>
<img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios1.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios1">

<h3 id="b-下载-chilloutmix-模型：进入-stable-diffusion-webui-x2F-models-x2F-stable-diffusion-文件夹，通过如下命令行下载-chilloutmix-模型到本地"><a href="#b-下载-chilloutmix-模型：进入-stable-diffusion-webui-x2F-models-x2F-stable-diffusion-文件夹，通过如下命令行下载-chilloutmix-模型到本地" class="headerlink" title="b. 下载 chilloutmix 模型：进入 stable-diffusion-webui&#x2F;models&#x2F;stable-diffusion 文件夹，通过如下命令行下载 chilloutmix 模型到本地"></a>b. 下载 chilloutmix 模型：进入 stable-diffusion-webui&#x2F;models&#x2F;stable-diffusion 文件夹，通过如下命令行下载 chilloutmix 模型到本地</h3><h3 id="c-LoRA-扩展安装，最新版本的-WebUI-已经内置了-LoRA，只需要单独安装-Additional-Network-这个扩展即可，在-WebUI-界面的-Extensions-页面，使用-Install-from-URL-安装。链接：sd-webui-additional-networks"><a href="#c-LoRA-扩展安装，最新版本的-WebUI-已经内置了-LoRA，只需要单独安装-Additional-Network-这个扩展即可，在-WebUI-界面的-Extensions-页面，使用-Install-from-URL-安装。链接：sd-webui-additional-networks" class="headerlink" title="c. LoRA 扩展安装，最新版本的 WebUI 已经内置了 LoRA，只需要单独安装 Additional Network 这个扩展即可，在 WebUI 界面的 Extensions 页面，使用 Install from URL 安装。链接：sd-webui-additional-networks"></a>c. LoRA 扩展安装，最新版本的 WebUI 已经内置了 LoRA，只需要单独安装 Additional Network 这个扩展即可，在 WebUI 界面的 Extensions 页面，使用 Install from URL 安装。链接：<a target="_blank" rel="noopener" href="https://github.com/kohya-ss/sd-webui-additional-networks.git">sd-webui-additional-networks</a></h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios2.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios2">


<h4 id="风格模型下载："><a href="#风格模型下载：" class="headerlink" title="风格模型下载："></a>风格模型下载：</h4><ul>
<li>FashionGirl：<a target="_blank" rel="noopener" href="https://civitai.com/models/8217?modelVersionId=28342">https://civitai.com/models/8217?modelVersionId=28342</a></li>
<li>ShojoVibe：<a target="_blank" rel="noopener" href="https://civitai.com/models/13213/shojovibe">https://civitai.com/models/13213/shojovibe</a></li>
<li>不能下载可以去国内的炼丹阁找一下</li>
</ul>
<h3 id="d-ControlNet-和-3D-Open-Pose-Edit-扩展安装，可以直接通过-WebUI-的官方扩展搜索"><a href="#d-ControlNet-和-3D-Open-Pose-Edit-扩展安装，可以直接通过-WebUI-的官方扩展搜索" class="headerlink" title="d. ControlNet 和 3D Open Pose Edit 扩展安装，可以直接通过 WebUI 的官方扩展搜索"></a>d. ControlNet 和 3D Open Pose Edit 扩展安装，可以直接通过 WebUI 的官方扩展搜索</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios3.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios3">


<p>以上为单 GPU 实例部署方式</p>
<h2 id="2-图片处理"><a href="#2-图片处理" class="headerlink" title="2. 图片处理"></a>2. 图片处理</h2><h3 id="a-原服装图片（推荐使用与样例图片相仿的素材，通过假人模特去拍摄服装素材图片）"><a href="#a-原服装图片（推荐使用与样例图片相仿的素材，通过假人模特去拍摄服装素材图片）" class="headerlink" title="a. 原服装图片（推荐使用与样例图片相仿的素材，通过假人模特去拍摄服装素材图片）"></a>a. 原服装图片（推荐使用与样例图片相仿的素材，通过假人模特去拍摄服装素材图片）</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios4.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios4">

<h3 id="b-使用图片处理工具处理分别得到一张纯色背景图片和服装部分涂黑的蒙版图片，需要保证两张图片内服装的位置一致，且图片尺寸需要指定-宽-高-x3D-600-1000"><a href="#b-使用图片处理工具处理分别得到一张纯色背景图片和服装部分涂黑的蒙版图片，需要保证两张图片内服装的位置一致，且图片尺寸需要指定-宽-高-x3D-600-1000" class="headerlink" title="b. 使用图片处理工具处理分别得到一张纯色背景图片和服装部分涂黑的蒙版图片，需要保证两张图片内服装的位置一致，且图片尺寸需要指定 宽 * 高 &#x3D; 600 * 1000"></a>b. 使用图片处理工具处理分别得到一张纯色背景图片和服装部分涂黑的蒙版图片，需要保证两张图片内服装的位置一致，且图片尺寸需要指定 宽 * 高 &#x3D; 600 * 1000</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios5.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios5">

<h2 id="3-模特姿态线条图处理"><a href="#3-模特姿态线条图处理" class="headerlink" title="3. 模特姿态线条图处理"></a>3. 模特姿态线条图处理</h2><p>使用 WebUI 的 3D Openpose 扩展插件，导入处理好的图片作为背景。</p>
<h3 id="a-导入图片作为背景图，设置宽度和高度分别为-600-和-1000"><a href="#a-导入图片作为背景图，设置宽度和高度分别为-600-和-1000" class="headerlink" title="a. 导入图片作为背景图，设置宽度和高度分别为 600 和 1000"></a>a. 导入图片作为背景图，设置宽度和高度分别为 600 和 1000</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios6.jpg" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios6">

<h3 id="b-调整姿态结构对应服装相对合适穿着位置"><a href="#b-调整姿态结构对应服装相对合适穿着位置" class="headerlink" title="b. 调整姿态结构对应服装相对合适穿着位置"></a>b. 调整姿态结构对应服装相对合适穿着位置</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios7.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios7">

<h3 id="c-点击生成，选取第一种姿态图片下载到本地备用"><a href="#c-点击生成，选取第一种姿态图片下载到本地备用" class="headerlink" title="c. 点击生成，选取第一种姿态图片下载到本地备用"></a>c. 点击生成，选取第一种姿态图片下载到本地备用</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios8.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios8">

<h3 id="以上步骤完成了三张图片素材的准备工作，得到以下纯色背景服装图、蒙版图、姿态图："><a href="#以上步骤完成了三张图片素材的准备工作，得到以下纯色背景服装图、蒙版图、姿态图：" class="headerlink" title="以上步骤完成了三张图片素材的准备工作，得到以下纯色背景服装图、蒙版图、姿态图："></a>以上步骤完成了三张图片素材的准备工作，得到以下纯色背景服装图、蒙版图、姿态图：</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios9.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios9">

<h2 id="4-修改-ControlNet-的系统设置，提供-3-个控制选项-Tab-页，应用设置（注意：会重启-WebUI，改变-WebUI-访问临时域名）"><a href="#4-修改-ControlNet-的系统设置，提供-3-个控制选项-Tab-页，应用设置（注意：会重启-WebUI，改变-WebUI-访问临时域名）" class="headerlink" title="4. 修改 ControlNet 的系统设置，提供 3 个控制选项 Tab 页，应用设置（注意：会重启 WebUI，改变 WebUI 访问临时域名）"></a>4. 修改 ControlNet 的系统设置，提供 3 个控制选项 Tab 页，应用设置（注意：会重启 WebUI，改变 WebUI 访问临时域名）</h2><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios10.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios10">

<h2 id="5、选择-chilloutmax-模型（基于-sd-1-5-fine-tunning），填写样例-prompt-词"><a href="#5、选择-chilloutmax-模型（基于-sd-1-5-fine-tunning），填写样例-prompt-词" class="headerlink" title="5、选择 chilloutmax 模型（基于 sd 1.5 fine-tunning），填写样例 prompt 词"></a>5、选择 chilloutmax 模型（基于 sd 1.5 fine-tunning），填写样例 prompt 词</h2><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios11.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios11">

<h3 id="参考的正向词示例（仅供摘选参考）："><a href="#参考的正向词示例（仅供摘选参考）：" class="headerlink" title="参考的正向词示例（仅供摘选参考）："></a>参考的正向词示例（仅供摘选参考）：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">belly button,((thin thighs))),slim body,((on street)),long hair,pink hair,ockings,best quality, amazing, &#123;Scenes with extreme detail &#123;&#123;&#123;extremely deCG&#125;&#125;&#125;),&#123;&#123;beautiful detailed background&#125;&#125;</span><br></pre></td></tr></table></figure>
<h3 id="参考的负向词示例（仅供摘选参考）："><a href="#参考的负向词示例（仅供摘选参考）：" class="headerlink" title="参考的负向词示例（仅供摘选参考）："></a>参考的负向词示例（仅供摘选参考）：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lowres, (bad anatomy), bad hands, mutated hand, text, error, missing fingeewer digits, cropped, worst quality, low quality, normal quality, jpeg artatermark, username, blurry, artist name, out of focus, glowing eyes, ((multiple views))), ((bad proportions))), ((multiple legs))), (((multiple arms)), 3D, badprompt, (worst quality:2.0), (lbw quality:2.0), inaccurate limb, lowres, bad anatomy, bad hands, texterror, missing figers, extra digit, fewer digits, cropped, worst quality, low quality, nlity, jpeg artifacts, signature, watermark, username, blurry, artist name,nting by bad-artist-anime:0.9), (painting by bad-artist:0.9), bad-prompt:0.5, watermark, text, error, blurry, jpeg artifacts, cropped, normatifacts, signature, watermark, username, artist name, (worst quality, low natomy, low quality lowres, low quality lowres low polygon 3D game, low quality lowres monochrome sketch rough graffiti, pwres very ugly fat obesity scar, low quality lowres chibi, low quality load anatomy, low quality lowres graffiti unbecoming colorfully, low quality lowres incoherent background, low qualilong body, low quality lowres duplicate comparison, low quality lowres skeurtstyle doujinshi, low quality lowres sketch, low quality lowres text font ui error missing digit blurry, low quality lowres JPEG arazy bleary, low quality lowres monochrome parody meme, low quality lowres tture, low quality lowres disfigured mutated malformed twisted human body, low quality lowres futanari tranny, lobw quality lowres tentacle skeleton, low quality lowres vomit suicide death dirty, (nipples:1.2), lowres, bad anatomy, bad hands, text, error,nissing fingers extra digit, fewer digits, cropped, worst quality, low quality, normal quasignature, watermark, username, blurry, Wrong hands, wrong fingers,fingersed fingers, fingers missed, missed fingers, cheese fingers, chocolate fingers, fingers on fingers, closed fist, fighting fistwith fist, many fists, only hands with wrong fists, fingers crossed, lowres, (bad anatomy), bad hands, mutated hand, text, error, missing fingeewer digits, cropped, worst quality, low quality, normal quality, jpeg artatermark, username, blurry, artist name, out of focus, glowing eyes, ((multiple views))), ((bad proportions))), ((multiple legs))), (((multiple arms)), 3D, badprompt, (worst quality:2.0), (lbw quality:2.0), inaccurate limb, lowres, bad anatomy, bad hands, texterror, missing figers, extra digit, fewer digits, cropped, worst quality, low quality, nlity, jpeg artifacts, signature, watermark, username, blurry, artist name,nting by bad-artist-anime:0.9), (painting by bad-artist:0.9), bad-prompt:0.5, watermark, text, error, blurry, jpeg artifacts, cropped, normatifacts, signature, watermark, username, artist name, (worst quality, low natomy, low quality lowres, low quality lowres low polygon 3D game, low quality lowres monochrome sketch rough graffiti, pwres very ugly fat obesity scar, low quality lowres chibi, low quality load anatomy, low quality lowres graffiti unbecoming colorfully, low quality lowres incoherent background, low qualilong body, low quality lowres duplicate comparison, low quality lowres skeurtstyle doujinshi, low quality lowres sketch, low quality lowres text font ui error missing digit blurry, low quality lowres JPEG arazy bleary, low quality lowres monochrome parody meme, low quality lowres tture, low quality lowres disfigured mutated malformed twisted human body, low quality lowres futanari tranny, lobw quality lowres tentacle skeleton, low quality lowres vomit suicide death dirty, (nipples:1.2), lowres, bad anatomy, bad hands, text, error,nissing fingers extra digit, fewer digits, cropped, worst quality, low quality, normal quasignature, watermark, username, blurry, Wrong hands, wrong fingers,fingersed fingers, fingers missed, missed fingers, cheese fingers, chocolate fingers, fingers on fingers, closed fist, fighting fistwith fist, many fists, only hands with wrong fists, fingers crossed, lowres, (bad anatomy), bad hands, mutated hand, text, error, missing fingeewer digits, cropped, worst quality, low quality, normal quality, jpeg artatermark, username, blurry, artist name, out of focus, glowing eyes, ((multiple views))), ((bad proportions))), ((multiple legs))), (((multiple arms)), 3D, badprompt, (worst quality:2.0), (lbw quality:2.0), inaccurate limb, lowres, bad anatomy, bad hands, texterror, missing figers, extra digit, fewer digits, cropped, worst quality, low quality, nlity, jpeg artifacts, signature, watermark, username, blurry, artist name,nting by bad-artist-anime:0.9), (painting by bad-artist:0.9), bad-prompt:0.5, watermark, text, error, blurry, jpeg artifacts, cropped, normatifacts, signature, watermark, username, artist name, (worst quality, low natomy, low quality lowres, low quality lowres low polygon 3D game, low quality lowres monochrome sketch rough graffiti, pwres very ugly fat obesity scar, low quality lowres chibi, low quality load anatomy, low quality lowres graffiti unbecoming colorfully, low quality lowres incoherent background, low qualilong body, low quality lowres duplicate comparison, low quality lowres skeurtstyle doujinshi, low quality lowres sketch, low quality lowres text font ui error missing digit blurry, low quality lowres JPEG arazy bleary, low quality lowres monochrome parody meme, low quality lowres tture, low quality lowres disfigured mutated malformed twisted human body, low quality lowres futanari tranny, lobw quality lowres tentacle skeleton, low quality lowres vomit suicide death dirty, (nipples:1.2), lowres, bad anatomy, bad hands, text, error,nissing fingers extra digit, fewer digits, cropped, worst quality, low quality, normal quasignature, watermark, username, blurry, Wrong hands, wrong fingers,fingersed fingers, fingers missed, missed fingers, cheese fingers, chocolate fingers, fingers on fingers, closed fist, fighting fistwith fist, many fists, only hands with wrong fists, fingers crossed, lowres, (bad anatomy), bad hands, mutated hand, text, error, missing fingeewer digits, cropped, worst quality, low quality, normal quality, jpeg artatermark, username, blurry, artist name, out of focus, glowing eyes, ((multiple views))), ((bad proportions))), ((multiple legs))), (((multiple arms)), 3D, badprompt, (worst quality:2.0), (lbw quality:2.0), inaccurate limb, lowres, bad anatomy, bad hands, texterror, missing figers, extra digit, fewer digits, cropped, worst quality, low quality, nlity, jpeg artifacts, signature, watermark, username, blurry, artist name,nting by bad-artist-anime:0.9), (painting by bad-artist:0.9), bad-prompt:0.5, watermark, text, error, blurry, jpeg artifacts, cropped, normatifacts, signature, watermark, username, artist name, (worst quality, low natomy, low quality lowres, low quality lowres low polygon 3D game, low quality lowres monochrome sketch rough graffiti, pwres very ugly fat obesity scar, low quality lowres chibi, low quality load anatomy, low quality lowres graffiti unbecoming colorfully, low quality lowres incoherent background, low qualilong body, low quality lowres duplicate comparison, low quality lowres skeurtstyle doujinshi, low quality lowres sketch, low quality lowres text font ui error missing digit blurry, low quality lowres JPEG arazy bleary, low quality lowres monochrome parody meme, low quality lowres tture, low quality lowres disfigured mutated malformed twisted human body, low quality lowres futanari tranny, lobw quality lowres tentacle skeleton, low quality lowres vomit suicide death dirty, (nipples:1.2), lowres, bad anatomy, bad hands, text, error,nissing fingers extra digit, fewer digits, cropped, worst quality, low quality, normal quasignature, watermark, username, blurry, Wrong hands, wrong fingers,fingersed fingers, fingers missed, missed fingers, cheese fingers, chocolate fingers, fingers on fingers, closed fist, fighting fistwith fist, many fists, only hands with wrong fists, fingers crossed</span><br></pre></td></tr></table></figure>
<h2 id="6-相关配置（Inpaint-upload、LoRA、ControlNet）"><a href="#6-相关配置（Inpaint-upload、LoRA、ControlNet）" class="headerlink" title="6. 相关配置（Inpaint upload、LoRA、ControlNet）"></a>6. 相关配置（Inpaint upload、LoRA、ControlNet）</h2><h3 id="a-图片和蒙版"><a href="#a-图片和蒙版" class="headerlink" title="a. 图片和蒙版"></a>a. 图片和蒙版</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios12.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios12">


<h3 id="b-Inpaint-upload-参数配置"><a href="#b-Inpaint-upload-参数配置" class="headerlink" title="b. Inpaint upload 参数配置"></a>b. Inpaint upload 参数配置</h3><p>注意把宽、高分别设置到 600 * 1000，Batch size 推荐 1~3。</p>
<img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios13.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios13">


<h3 id="c-LoRA-配置（使用两个参考图片风格的模型）"><a href="#c-LoRA-配置（使用两个参考图片风格的模型）" class="headerlink" title="c. LoRA 配置（使用两个参考图片风格的模型）"></a>c. LoRA 配置（使用两个参考图片风格的模型）</h3><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios14.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios14">


<h3 id="d-ControlNet-配置（需要配置-MultiControlNet，参考步骤-3）"><a href="#d-ControlNet-配置（需要配置-MultiControlNet，参考步骤-3）" class="headerlink" title="d. ControlNet 配置（需要配置 MultiControlNet，参考步骤 3）"></a>d. ControlNet 配置（需要配置 MultiControlNet，参考步骤 3）</h3><p>在 Control Model – 0 tab 页配置服装图片，选择启用 Enable 和低内存 Low VRAM 开关，预处理和对应模型选择 canny 和 control_sd15_canny，画布宽高为 600 * 1000。</p>
<img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios15.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios15">


<p>在 Control Model – 1 tab 页配置人体形态图片，选择启用 Enable 和低内存 Low VRAM 开关，只需要修改模型选择 control_sd15_canny，画布宽高为 600 * 1000，Guidance End 设置为0.6。</p>
<img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios16.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios16">


<h2 id="7-点击生成完成图片生成，第一次会有多个模型加载，速度相对缓慢"><a href="#7-点击生成完成图片生成，第一次会有多个模型加载，速度相对缓慢" class="headerlink" title="7. 点击生成完成图片生成，第一次会有多个模型加载，速度相对缓慢"></a>7. 点击生成完成图片生成，第一次会有多个模型加载，速度相对缓慢</h2><img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios17.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios17">


<p>最终效果图片对比：</p>
<img src="/2023/06/12/StableDiffusion%E5%9C%A8%E7%94%B5%E5%95%86%E5%9C%BA%E6%99%AF%E7%94%9F%E6%88%90%E6%A8%A1%E7%89%B9%E6%90%AD%E9%85%8D%E6%9C%8D%E8%A3%85%E7%9A%84%E6%96%B9%E6%A1%88/generating-ai-models-to-match-outfits-in-e-commerce-scenarios18.png" class="" title="generating-ai-models-to-match-outfits-in-e-commerce-scenarios18">

<h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2><p>利用 Stable Diffusion WebUI + LoRA + ControlNet 做 AI 模特适配电商服装图片生成的方案，可以在一些电商运营场景下降低服装商品图片素材的生产成本和周期。在整个方案的流程中，图片处理还可以考虑去探索借助 Sagment Anything 去分隔服装和生成对应的蒙版。</p>
<h2 id="原作者"><a href="#原作者" class="headerlink" title="原作者"></a>原作者</h2><p>AWS：唐健，原野</p>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI%E5%95%86%E4%B8%9A%E5%8C%96%E5%9C%BA%E6%99%AF/" rel="tag">AI商业化场景</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-gongchengshizhuanxing" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/11/gongchengshizhuanxing/">工程师管理转型</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-11T05:00:00.000Z" itemprop="datePublished">
  2023-06-11
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>从功能，代码，架构 转到全局思考，视角切换，能力要求均衡</p>
<p>Leader：建立有战斗力的团队，放权，迅速补齐团队成员短板，但不能所有事情，亲历亲为，提供兜底</p>
<h2 id="职责："><a href="#职责：" class="headerlink" title="职责："></a>职责：</h2><ol>
<li>帮助公司拿结果，帮助员工成长</li>
</ol>
<h2 id="目标："><a href="#目标：" class="headerlink" title="目标："></a>目标：</h2><ol>
<li>向上：与公司目标对齐</li>
<li>向下：目标拆解成可执行的行动项</li>
<li>横向：与兄弟部门合作，目标对齐</li>
</ol>
<h2 id="能力："><a href="#能力：" class="headerlink" title="能力："></a>能力：</h2><ol>
<li>团队：有能力接收挑战，内部培养，外部引入</li>
<li>个人：补齐短板，扬长避短，最大化每个人的价值</li>
</ol>
<h2 id="招聘"><a href="#招聘" class="headerlink" title="招聘"></a>招聘</h2><ol>
<li>是否与公司文化契合</li>
<li>是否具备owner意识 </li>
<li>是否具备成长性</li>
</ol>
<h2 id="信任："><a href="#信任：" class="headerlink" title="信任："></a>信任：</h2><ol>
<li>上级、下级、平级之间形成双向信任关系，我做事你放心，你做事我放心，对人的认可，站在对方角度</li>
</ol>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/" rel="tag">职业规划</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-kuaisuchengzhang" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/11/kuaisuchengzhang/">快速成长诀窍</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-11T05:00:00.000Z" itemprop="datePublished">
  2023-06-11
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <ol>
<li>自我成长的责任：成长是一件很私人的事情，对自己的成长负责，自己是自己成长的主要责任</li>
<li>公司优秀时的机遇：利用公司提供的平台、资源和流程实现快速成长</li>
<li>公司不完善时的机遇：在问题中找到成长的机会，推动体系建设</li>
<li>整合时间、精力和资源：有效地管理时间和精力，并充分利用公司资源，超出期望的表现：通过努力使工作成果超出他人的期望</li>
<li>将问题转化为机会：善于解决问题，并将其视为个人成长的机会</li>
<li>主动性强，主动承担更多，更大的任务，获得更多成长</li>
<li>不要给给自己设限，划定边界，设置障碍，不要定义自己，明白程序只是解决问题的工具</li>
<li>重视deadline和owner意识</li>
<li>寻求资深人士帮助，所谓遇到贵人</li>
</ol>
<hr>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/" rel="tag">职业规划</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-shijianguanli" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/11/shijianguanli/">如何管理时间，持续进步？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-11T05:00:00.000Z" itemprop="datePublished">
  2023-06-11
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <ol>
<li><p>商业洞察：明确商业目标和战略，确保你所做的事与公司的核心价值相关。集中精力在具有最大商业影响力的任务上，避免分散注意力。</p>
</li>
<li><p>知识体系建设和扩展：建立一个系统化的知识体系，不断学习和深化领域知识。阅读书籍、参加培训课程、参与行业会议等方式扩展知识，并将学到的知识应用到实际工作中。</p>
</li>
<li><p>项目管理方法：采用有效的项目管理方法，如制定明确的目标和里程碑，制定详细的计划和时间表，分解任务，设定优先级，及时跟进进度并调整计划。使用项目管理工具和技术帮助提高效率和组织能力。</p>
</li>
<li><p>极致做事方法：注重细节，追求卓越。确保每个任务都精益求精，准确无误地完成。细心审查工作成果，检查错误并进行修正。持续反思并改进自己的工作方法和流程，以提高效率和质量。</p>
</li>
<li><p>家庭、生活和放松时间：平衡工作与个人生活，确保拥有足够的时间放松和休息。设定明确的工作时间和个人时间，优化时间利用，避免过度工作。培养兴趣爱好、锻炼身体和与家人朋友交流，以保持身心健康和持续进步。</p>
</li>
</ol>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/" rel="tag">职业规划</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
  </div>


  


        </div>
      
      
        <div class="d-flex justify-content-center pt-5">
          <a href="/archives/" title="→ 查看更多" class="btn btn-lg bg-white shadow-hover">→ 查看更多</a>
        </div>
      
    </div>
  </section>


    </section>
    <footer class="footer pt-5 mt-5">
  <div class="container">
    <div class="py-3">
      <div class="row justify-content-between">
        <div class="col-6">
          <img class="filter-gray mb-3 lazyload" height="40" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
          <p class="mb-4">这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。</p>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="javascript:;">
                  <img 0="微信" src="/images/icons/contact_wechat.svg">
                </a>
              </li>
            
              <li class="list-inline-item">
                <a href="mailto:a@abc.com">
                  <img 0="邮箱" src="/images/icons/contact_email.svg">
                </a>
              </li>
            
          </ul>
        </div>
        <div class="col-4">
          <h5>友情链接</h5>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="https://acorn.imaging.xin/" title="Acorn" target="_blank" rel="noopener">Acorn</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://github.com/" title="GitHub" target="_blank" rel="noopener">GitHub</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://duoyu.wang/" title="To Base64" target="_blank" rel="noopener">To Base64</a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
    <hr class="hr" style="opacity: .25;">
    <div class="pt-3 pb-5">
      <ul class="list-inline mb-0 text-center">
        <li class="list-inline-item">&copy; 2023 AI架构 | AI系统基础架构设计与优化</li>
        
        <li class="list-inline-item">Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
        <li class="list-inline-item">Designer <a href="https://acorn.imaging.xin/" target="_blank">罗平</a></li>
      </ul>
    </div>
  </div>
</footer>
  </main>
  <div id="mobile-nav-dimmer"></div>
<div id="mobile-nav">
	<div id="mobile-nav-inner">
		<ul class="mobile-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		
	</div>
</div>

  <script src="/libs/feather/feather.min.js"></script>
<script src="/libs/lazysizes/lazysizes.min.js"></script>

	<script src="/libs/tocbot/tocbot.min.js"></script>
	<script>
    tocbot.init({
      // Where to render the table of contents.
      tocSelector: '.js-toc',
      // Where to grab the headings to build the table of contents.
      contentSelector: '.js-toc-content',
      // Which headings to grab inside of the contentSelector element.
      headingSelector: 'h2, h3',
      // For headings inside relative or absolute positioned containers within content.
      hasInnerContainers: true,
    });
	</script>





<script src="/js/mobile-nav.js"></script>


<script src="/js/script.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178892506-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178892506-1');
</script>
</body>
</html>