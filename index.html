<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="AI架构师, ai infra, AI系统设计, AI系统优化, 机器学习, 深度学习, 大数据处理, 高性能AI架构, 可扩展AI系统, ChatGPT, Stable Diffusion, AI 绘画, 大模型">
  
  
    <meta name="description" content="这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <title> AI架构 | AI系统基础架构设计与优化</title>
  
    <link rel="apple-touch-icon" sizes="57x57" href="/images/webclip/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/images/webclip/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/images/webclip/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/images/webclip/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/images/webclip/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/images/webclip/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/images/webclip/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/webclip/apple-touch-icon-180x180.png">
    <link rel="apple-touch-icon" sizes="167x167" href="/images/webclip/apple-touch-icon-167x167.png">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <main class="main">
    
	<header id="header" class="header header-absolute">

	<div class="container">
		<nav class="navbar d-flex align-items-center">
			<a class="brand" href="/">
				<img class="logo lazyload" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
			</a>
			<ul class="main-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		</nav>
		<a id="mobile-nav-toggle">
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
		</a>
	</div>
</header>

    <section>
      <!-- Index -->

<div class="hero">
	<figure class="hero-figure">
		<img class="hero-figure-img" src="/images/banner/banner.jpg" alt="AI架构 | AI系统基础架构设计与优化">
		<figcaption>
			<div class="container">
				<div class="figure-inset">
					<h2 class="h1">深入AI架构的奥秘</h2>
					<p>创造卓越的智能解决方案</p>
				</div>
			</div>
		</figcaption>
		<div class="learn-more">
			<a class="anchor" href="#landingpage">
				<img id="landingpage" src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='24' height='24' fill='white' viewBox='0 0 16 16'><path d='M8 3a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 3zm4 8a4 4 0 0 1-8 0V5a4 4 0 1 1 8 0v6zM8 0a5 5 0 0 0-5 5v6a5 5 0 0 0 10 0V5a5 5 0 0 0-5-5z'/></svg>" alt="">
			</a>
		</div>
	</figure>
</div>


<section class="section py-5 bg-light">
  <div class="container">
    <!-- 
  Data Files: source/_data/culture.yml
-->
<div class="row">
  
</div>
  </div>
</section>


  <section class="section py-5 " id="">
    <div class="container">
      <div class="section-heading text-center mb-5">
        <h3>文章</h3>
        <p class="text-gray">汇聚热点话题 打造创新思路</p>
      </div>

      
        <div class="section-body">
          
  <div class="row flex-wrap">
    
      <div class="col-3">
        <article id="post-提示工程" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/07/18/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/">Prompt Engineering</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-07-18T05:00:00.000Z" itemprop="datePublished">
  2023-07-18
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p><strong>提示工程</strong>，也称为<strong>上下文提示</strong>，是指如何在不更新模型权重<em>的情况下与</em>LLM通信以引导其行为以获得所需结果的方法。这是一门实证科学，快速工程方法的效果在不同模型之间可能有很大差异，因此需要大量的实验和启发式。</p>
<h1 id="Basic-Prompting（基本提示）"><a href="#Basic-Prompting（基本提示）" class="headerlink" title="Basic Prompting（基本提示）"></a>Basic Prompting（基本提示）</h1><h2 id="Zero-shot（零镜头）"><a href="#Zero-shot（零镜头）" class="headerlink" title="Zero-shot（零镜头）"></a>Zero-shot（零镜头）</h2><p>零镜头(Zero-shot)和少镜头( few-shot)学习是提示模型的两种最基本的方法，由许多LLM论文开创，通常用于基准测试LLM性能。</p>
<p><strong>零镜头学习</strong>是简单地将任务文本提供给模型并要求结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Text: i&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></table></figure>

<h2 id="few-shot（少镜头）"><a href="#few-shot（少镜头）" class="headerlink" title="few-shot（少镜头）"></a>few-shot（少镜头）</h2><p><strong>少镜头学习</strong>提供了一组关于目标任务的高质量演示，每个演示都包含<strong>输入和期望</strong>输出。当模型首先看到好的例子时，它可以<strong>更好地理解人类的意图和想要什么样的答案的标准</strong>。因此，少镜头学习通常比零镜头学习带来更好的表现。但是，它以消耗更多的令牌为代价，并且当输入和输出文本较长时，可能会达到上下文长度限制。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Text: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.</span><br><span class="line">Sentiment: negative</span><br><span class="line"></span><br><span class="line">Text: for the first time in years, de niro digs deep emotionally, perhaps because he&#x27;s been stirred by the powerful work of his co-stars.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: i&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></table></figure>

<p>许多研究研究了如何构建上下文示例以最大限度地提高性能，并观察到<strong>提示格式、训练示例和示例顺序的选择会导致性能大不相同</strong>，从近乎随机猜测到接近 SoTA。</p>
<h1 id="Instruction-Prompting（指令提示）"><a href="#Instruction-Prompting（指令提示）" class="headerlink" title="Instruction Prompting（指令提示）"></a>Instruction Prompting（指令提示）</h1><p>在提示中呈现几个镜头示例的目的是向模型解释我们的意图;换句话说，以<strong>演示</strong>的形式向模型描述任务指令。但是，在令牌使用方面，<strong>少数镜头可能很昂贵</strong>，并且由于上下文<strong>长度有限</strong>而限制了输入长度。那么，为什么不直接下达指令呢？</p>
<p><em>Instructed LM</em> (e.g. <a target="_blank" rel="noopener" href="https://openai.com/research/instruction-following">InstructGPT</a>, <a target="_blank" rel="noopener" href="https://github.com/allenai/natural-instructions">natural instruction</a>) ,用高质量的元组（task instruction, input, ground truth output）微调预训练模型，以使LM更好地理解用户意图并遵循<a target="_blank" rel="noopener" href="https://github.com/allenai/natural-instructions">指令</a>。<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences">RLHF</a>（来自人类反馈的强化学习）是一种常用的方法。遵循风格微调的指令的好处是改进了模型，使其更符合人类的意图，并大大降低了沟通成本。</p>
<p>在与指令模型交互时，我们应该详细描述任务需求，尽量<em>具体</em>和<em>精确</em>，避免说“不做某事”，而是指定要做什么。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Definition: Determine the speaker of the dialogue, &quot;agent&quot; or &quot;customer&quot;.</span><br><span class="line">Input: I have successfully booked your tickets.</span><br><span class="line">Ouput: agent</span><br><span class="line"></span><br><span class="line">Definition: Determine which category the question asks for, &quot;Quantity&quot; or &quot;Location&quot;.</span><br><span class="line">Input: What&#x27;s the oldest building in US?</span><br><span class="line">Ouput: Location</span><br><span class="line"></span><br><span class="line">Definition: Classify the sentiment of the given movie review, &quot;positive&quot; or &quot;negative&quot;.</span><br><span class="line">Input: i&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="line">Output:</span><br></pre></td></tr></table></figure>

<h1 id="Self-Consistency-Sampling-自一致性采样"><a href="#Self-Consistency-Sampling-自一致性采样" class="headerlink" title="Self-Consistency Sampling(自一致性采样)"></a>Self-Consistency Sampling(自一致性采样)</h1><p><strong>自一致性采样</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.11171">Wang 等人，2022</a>）是在 temperature &gt; 0的情况下对多个输出进行采样，然后从这些候选输出中选择最佳输出。 选择最佳候选人的标准可能因任务而异。一般的解决方案是选择<strong>多数票</strong>。对于易于验证的任务，例如带有单元测试的编程问题，我们可以简单地运行解释器并通过单元测试验证正确性。</p>
<h1 id="Chain-of-Thought-思维链"><a href="#Chain-of-Thought-思维链" class="headerlink" title="Chain-of-Thought (思维链 )"></a>Chain-of-Thought (思维链 )</h1><p><strong>思维链 （CoT） 提示</strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.11903">Wei 等人，2022</a> 年）生成一系列短句，逐步描述推理逻辑，称为<em>推理链</em>或<em>理由</em>，最终导致最终答案。CoT的好处对于<strong>复杂的推理任务</strong>更为明显，同时使用<strong>大型模型</strong>（例如，参数超过50B）。简单任务仅从 CoT 提示中略有受益。</p>
<h2 id="CoT-提示的类型"><a href="#CoT-提示的类型" class="headerlink" title="CoT 提示的类型"></a>CoT 提示的类型</h2><p><strong>Few-shot CoT</strong>: 它是通过一些演示来提示模型，每个演示都包含手动编写（或模型生成的）高质量推理链。</p>
<p>（所有数学推理示例均来自<a target="_blank" rel="noopener" href="https://github.com/openai/grade-school-math">GSM8k</a>)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Question: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?</span><br><span class="line">Answer: It takes Tom 30*4 = &lt;&lt;30*4=120&gt;&gt;120 minutes to climb the hill.</span><br><span class="line">It takes Tom 120/60 = &lt;&lt;120/60=2&gt;&gt;2 hours to climb the hill.</span><br><span class="line">So the answer is 2.</span><br><span class="line">===</span><br><span class="line">Question: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?</span><br><span class="line">Answer: The total cost of two pairs of socks is $9.50 x 2 = $&lt;&lt;9.5*2=19&gt;&gt;19.</span><br><span class="line">The total cost of the socks and the shoes is $19 + $92 = $&lt;&lt;19+92=111&gt;&gt;111.</span><br><span class="line">Jack need $111 - $40 = $&lt;&lt;111-40=71&gt;&gt;71 more.</span><br><span class="line">So the answer is 71.</span><br><span class="line">===</span><br><span class="line">Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?</span><br><span class="line">Answer:</span><br></pre></td></tr></table></figure>

<p><strong>Zero-shot CoT</strong>:使用自然语言语句来明确鼓励模型首先生成推理链，然后提示产生答案（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.11916">Kojima 等人，2022</a> 年）。或类似的说法（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01910">周等人，2022</a> 年）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be? Answer: Let&#x27;s think step by step.</span><br></pre></td></tr></table></figure>

<h1 id="Automatic-Prompt-Design-自动提示设计"><a href="#Automatic-Prompt-Design-自动提示设计" class="headerlink" title="Automatic Prompt Design(自动提示设计)"></a>Automatic Prompt Design(自动提示设计)</h1><p>提示是一系列前缀标记，可增加给定输入获得所需输出的可能性。因此，我们可以将它们视为可训练的参数，并通过梯度下降直接在嵌入空间上<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">优化它们</a>，例如 <strong>AutoPrompt</strong> （Shin 等人，2020 年，<strong><strong>Prefix-Tuning</strong></strong>（Li &amp; Liang （2021 年））、<strong><strong>P-tuning</strong></strong>（Liu 等人，2021 年）和<strong><strong>Prompt-Tuning</strong></strong>（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.08691">Lester 等人，</a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00190">2021 年</a>）。<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">我的“可控神经文本生成”帖子中的这一部分</a>很好地涵盖了它们。从自动提示到提示调整的趋势是设置逐渐简化。</p>
<p><strong>APE</strong>（自动提示工程师;<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.01910">周等人，2022）</a>是一种搜索模型生成的指令候选池的方法，然后根据所选的分数函数过滤候选集，以最终选择得分最高的最佳候选者。</p>
<h1 id="Augmented-Language-Models-增强语言模型"><a href="#Augmented-Language-Models-增强语言模型" class="headerlink" title="Augmented Language Models(增强语言模型)"></a>Augmented Language Models(增强语言模型)</h1><h2 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h2><p>通常，我们需要在模型预训练时间截止或内部&#x2F;私有知识库之后完成需要最新知识的任务。在这种情况下，如果我们不在提示中显式提供上下文，模型将不知道上下文。<a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2020-10-29-odqa/">开放域问答</a>的许多方法都依赖于首先通过知识库进行检索，然后将检索到的内容合并为提示的一部分。这种过程的准确性取决于检索和生成步骤的质量。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.05115">Lazaridou 等人（2022 年）</a>研究了如何使用谷歌搜索进行文档检索以增强 LLM。给定一个问题�，从 Google 返回的 20 个网址中提取干净的文本，从而生成一组文档。由于这些文件很长，因此每个文档被分成6个句子的段落，{�}.段落按基于TF-IDF的证据段落和查询之间的余弦相似度进行排名。提示中仅使用最相关的段落来生成答案一个.</p>
<h2 id="程序设计语言"><a href="#程序设计语言" class="headerlink" title="程序设计语言"></a>程序设计语言</h2><p>两种 <strong>PAL</strong>（程序辅助语言模型）;<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.10435">高等人，2022）</a>和<strong>PoT</strong>（思想程序提示;<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.12588">陈等人，2022）</a>要求LLM生成编程语言语句来解决自然语言推理问题，从而将解决方案步骤卸载到运行时，例如Python解释器。这种设置将复杂的计算和推理解耦。它依赖于具有足够好的编码技能的 LM。</p>
<img src="/2023/07/18/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/PoT.png" class="" title="img">

<h2 id="外部接口"><a href="#外部接口" class="headerlink" title="外部接口"></a>外部接口</h2><p><strong>TALM</strong>（工具增强语言模型;<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.12255">帕里西等人，2022</a>）是一种通过文本到文本 API 调用增强的语言模型。LM 被引导生成任务输入文本并以此为条件来构造 API 调用请求。显示时，将调用指定的工具 API，并将返回的结果追加到文本序列中。最终输出是按照令牌生成的。</p>
<img src="/2023/07/18/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/TALM.png" class="" title="img">

<p><strong>Toolformer</strong> （<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.04761">Schick 等人，2023</a> 年）是一个 LM，可以通过简单的 API 使用外部工具，它以自我监督的方式构建，每个 API 只需要少量演示。Toolformer 的工具箱包括：</p>
<ul>
<li><em>计算器</em>可帮助LM解决缺乏精确数学技能的问题;</li>
<li><em>问答系统</em>，以帮助处理不忠的内容和幻觉;</li>
<li><em>搜索引擎</em>在预培训截止时间后提供最新信息;</li>
<li><em>翻译系统</em>，以提高低资源语言的性能;</li>
<li><em>日历，</em>使LM了解时间进度。</li>
</ul>
<img src="/2023/07/18/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B/toolformer-16896364542184.png" class="" title="img">

<p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#augmented-language-models">快速工程 |利尔日志 (lilianweng.github.io)</a></p>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E3%80%81Prompt/" rel="tag">大模型、Prompt</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-ChatGPT微调实战" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/07/16/ChatGPT%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/">ChatGPT微调实战</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-07-16T12:05:14.000Z" itemprop="datePublished">
  2023-07-16
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <h3 id="第一部分-LLaMA-x2F-ChatLLaMA的整体技术架构与代码逐行解读"><a href="#第一部分-LLaMA-x2F-ChatLLaMA的整体技术架构与代码逐行解读" class="headerlink" title="第一部分 LLaMA&#x2F;ChatLLaMA的整体技术架构与代码逐行解读"></a>第一部分 LLaMA&#x2F;ChatLLaMA的整体技术架构与代码逐行解读</h3><ul>
<li><h4 id="第1课-实战必备-夯实基础：ChatGPT背后的原理解析"><a href="#第1课-实战必备-夯实基础：ChatGPT背后的原理解析" class="headerlink" title="第1课 实战必备 夯实基础：ChatGPT背后的原理解析"></a>第1课 实战必备 夯实基础：ChatGPT背后的原理解析</h4><h4 id="知识点1：-ChatGPT底层强大的语言模型：从transformer到GPT1-x2F-2-x2F-3"><a href="#知识点1：-ChatGPT底层强大的语言模型：从transformer到GPT1-x2F-2-x2F-3" class="headerlink" title="知识点1： ChatGPT底层强大的语言模型：从transformer到GPT1&#x2F;2&#x2F;3"></a>知识点1： ChatGPT底层强大的语言模型：从transformer到GPT1&#x2F;2&#x2F;3</h4><p>ChatGPT是一种基于GPT模型的聊天机器人，它的底层语言模型采用了Transformer结构，并且经历了多个版本的迭代和升级，包括GPT-1、GPT-2和GPT-3等。下面分别介绍一下这几个版本的特点和改进。</p>
<ol>
<li>Transformer模型</li>
</ol>
<p>Transformer是一种基于注意力机制的神经网络模型，它可以对序列数据进行编码和解码，被广泛应用于自然语言处理领域。Transformer模型的核心是自注意力机制和多头注意力机制，可以有效地捕捉序列数据中的长程依赖关系和语义信息。</p>
<ol>
<li>GPT-1模型</li>
</ol>
<p>GPT-1是基于Transformer模型的语言模型，它采用了单向的Transformer结构，并且使用了基于最大似然估计的预训练方式。GPT-1可以生成自然流畅的文本，但是对于长文本的生成和理解能力有所不足。</p>
<ol>
<li>GPT-2模型</li>
</ol>
<p>GPT-2是在GPT-1的基础上进行了改进和升级的语言模型，它采用了双向的Transformer结构，并且使用了更大的模型和更多的数据进行预训练。GPT-2可以生成更加自然、连贯、准确的文本，并且具备更强的长文本生成和理解能力。</p>
<ol>
<li>GPT-3模型</li>
</ol>
<p>GPT-3是在GPT-2的基础上进一步改进和升级的语言模型，它采用了更加复杂的Transformer结构，并且使用了超过1750亿个参数进行预训练。GPT-3可以生成非常自然、流畅、准确的文本，并且具备非常强的泛化能力和多样性，可以适应各种不同的任务和场景。</p>
<h4 id="知识点2：-揭秘为何可以做推理和debug：从GPT3到GPT3-5的指令微调、思维链、代码训练"><a href="#知识点2：-揭秘为何可以做推理和debug：从GPT3到GPT3-5的指令微调、思维链、代码训练" class="headerlink" title="知识点2： 揭秘为何可以做推理和debug：从GPT3到GPT3.5的指令微调、思维链、代码训练"></a>知识点2： 揭秘为何可以做推理和debug：从GPT3到GPT3.5的指令微调、思维链、代码训练</h4><p>GPT-3和GPT-3.5是基于Transformer结构的神经网络模型，它们具备非常强大的自然语言处理能力，并且可以进行推理和debug。这些能力是通过指令微调、思维链和代码训练等技术来实现的。</p>
<ol>
<li>指令微调</li>
</ol>
<p>指令微调是一种将模型应用于特定任务的技术。在GPT-3和GPT-3.5中，指令微调被用来训练模型完成各种任务，如问答、翻译、文本生成等。通过将模型微调到特定的任务上，可以进一步提高模型的性能和表现。</p>
<ol>
<li>思维链</li>
</ol>
<p>思维链是一种基于模型的推理和推断技术。它通过将模型的输入和输出映射到一个中间的概念空间中，从而可以推理和推断出输入和输出之间的关系。在GPT-3和GPT-3.5中，思维链被用来解决推理和debug问题。例如，可以通过思维链来分析模型生成的输出，并找出其中的错误和潜在问题。</p>
<ol>
<li>代码训练</li>
</ol>
<p>代码训练是一种将模型应用到代码生成和代码理解等任务的技术。在GPT-3.5中，代码训练被用来训练模型生成和理解代码。通过代码训练，模型可以学习到代码的语法和语义，并且可以通过与程序员的对话来进一步提高自己的性能和表现。</p>
<p>总之，GPT-3和GPT-3.5之所以具备推理和debug能力，是因为它们采用了指令微调、思维链和代码训练等技术，使得模型可以适应不同的任务和场景，并且可以进行推理、推断和代码理解等高级任务。这些技术的应用，使得GPT-3和GPT-3.5成为了非常强大的自然语言处理和人工智能模型。</p>
<h4 id="知识点3：-ChatGPT是如何训练而成的：InstructGPT训练三阶段的全面理解"><a href="#知识点3：-ChatGPT是如何训练而成的：InstructGPT训练三阶段的全面理解" class="headerlink" title="知识点3： ChatGPT是如何训练而成的：InstructGPT训练三阶段的全面理解"></a>知识点3： ChatGPT是如何训练而成的：InstructGPT训练三阶段的全面理解</h4><p>ChatGPT是一种基于GPT模型的聊天机器人，它的训练过程可以分为三个阶段，分别是预训练、微调和人工纠错。下面分别介绍一下这三个阶段的具体内容和作用。</p>
<ol>
<li>预训练</li>
</ol>
<p>预训练是指在大规模的语料库上对模型进行无监督的训练，从而让模型学习到自然语言的基本规律和语义知识。ChatGPT采用了GPT-3模型进行预训练，使用了超过1750亿个参数，在大量的互联网文本数据上进行了训练。预训练的目的是让模型具备强大的语言理解和生成能力，为后续的微调和人工纠错打下基础。</p>
<ol>
<li>微调</li>
</ol>
<p>微调是指在特定的任务上对模型进行有监督的训练，从而让模型更好地适应该任务的特点和要求。ChatGPT的微调是基于InstructGPT框架进行的，它可以自动从人类的对话中学习到模型如何生成自然流畅的回复，并且可以根据对话的上下文和意图进行灵活的生成。微调的目的是让模型具备更好的聊天能力和交互能力，进一步提高模型的性能和表现。</p>
<ol>
<li>人工纠错</li>
</ol>
<p>人工纠错是指通过人工的方式对模型生成的回复进行修正和改进，从而进一步提高模型的质量和准确率。ChatGPT采用了基于人类操作的远程人工纠错方法，让人类操作员对机器人的回复进行审核和纠错。人工纠错的目的是让模型生成的回复更加准确、自然、流畅，并且更符合人类的语言习惯和风格。</p>
</li>
<li><h4 id="第2课-Meta-LLaMA的复现与解读：参数少但多数任务的效果好于GPT3"><a href="#第2课-Meta-LLaMA的复现与解读：参数少但多数任务的效果好于GPT3" class="headerlink" title="第2课 Meta LLaMA的复现与解读：参数少但多数任务的效果好于GPT3"></a>第2课 Meta LLaMA的复现与解读：参数少但多数任务的效果好于GPT3</h4><h4 id="知识点1：-代码级解读：LLaMA的模型架构——RMSNorm-x2F-SwiGLU-x2F-RoPE-x2F-Transformer"><a href="#知识点1：-代码级解读：LLaMA的模型架构——RMSNorm-x2F-SwiGLU-x2F-RoPE-x2F-Transformer" class="headerlink" title="知识点1： 代码级解读：LLaMA的模型架构——RMSNorm&#x2F;SwiGLU&#x2F;RoPE&#x2F;Transformer"></a>知识点1： 代码级解读：LLaMA的模型架构——RMSNorm&#x2F;SwiGLU&#x2F;RoPE&#x2F;Transformer</h4><p>LLaMA是一种基于Transformer结构的语言模型，它采用了一些特殊的技术和结构优化，包括RMSNorm、SwiGLU、RoPE和Transformer等。</p>
<ol>
<li>RMSNorm</li>
</ol>
<p>RMSNorm是一种用于归一化神经网络的技术，它可以在保证网络稳定性的同时，提高模型的收敛速度和性能。在LLaMA中，RMSNorm被用来代替标准的BatchNorm和LayerNorm，从而进一步提高模型的性能和表现。</p>
<ol>
<li>SwiGLU</li>
</ol>
<p>SwiGLU是一种基于门控线性单元（GLU）的激活函数，它可以有效地捕捉序列数据中的长程依赖关系和语义信息。在LLaMA中，SwiGLU被用来代替标准的激活函数，从而进一步提高模型的准确率和效率。</p>
<ol>
<li>RoPE</li>
</ol>
<p>RoPE是一种相对位置编码技术，它可以对序列数据的相对位置进行编码，从而进一步提高模型的长程依赖关系和语义理解能力。在LLaMA中，RoPE被用来代替标准的绝对位置编码，从而可以更好地捕捉序列数据中的语义信息和长程依赖关系。</p>
<ol>
<li>Transformer</li>
</ol>
<p>Transformer是一种基于注意力机制的神经网络模型，它可以对序列数据进行编码和解码，被广泛应用于自然语言处理领域。在LLaMA中，Transformer被用作模型的核心结构，可以有效地捕捉序列数据中的长程依赖关系和语义信息。</p>
<h4 id="知识点2：-到底如何理解旋转位置编码并编码实现"><a href="#知识点2：-到底如何理解旋转位置编码并编码实现" class="headerlink" title="知识点2： 到底如何理解旋转位置编码并编码实现"></a>知识点2： 到底如何理解旋转位置编码并编码实现</h4><p>旋转位置编码（Rotary Position Embedding）是一种相对位置编码技术，它可以对序列数据的相对位置进行编码。相对位置编码的目的是为了使模型能够更好地理解序列数据中的语义信息和长程依赖关系。</p>
<p>旋转位置编码的实现过程如下：</p>
<ol>
<li>将序列数据通过词嵌入（Word Embedding）转换为向量形式。</li>
<li>对于每个位置的向量，将其分成两个部分：实部和虚部。</li>
<li>对于每个位置，定义一个旋转角度。旋转角度可以通过余弦函数和正弦函数进行计算。</li>
<li>将实部和虚部分别进行旋转，旋转的角度为该位置的旋转角度。</li>
<li>将旋转后的实部和虚部进行拼接，得到该位置的旋转向量。</li>
<li>将旋转向量与位置编码向量相加，得到最终的位置编码向量。</li>
</ol>
<p>在实现过程中，旋转角度的计算可以通过一个可学习的参数向量进行实现。具体地，对于每个位置，可以通过该位置的序号和一个可学习的参数向量进行计算，从而得到该位置的旋转角度。</p>
<p>旋转位置编码的优点是可以减少相对位置编码中的冲突，并且在某些任务中具有更好的表现。例如，在机器翻译任务中，使用旋转位置编码可以显著提高模型的性能和准确率。</p>
<h4 id="知识点3：-Transformer架构的实现：Attention计算、SA、FFN"><a href="#知识点3：-Transformer架构的实现：Attention计算、SA、FFN" class="headerlink" title="知识点3： Transformer架构的实现：Attention计算、SA、FFN"></a>知识点3： Transformer架构的实现：Attention计算、SA、FFN</h4><p>Transformer是一种基于注意力机制的神经网络模型，它包括了多层的自注意力机制和前馈神经网络。下面分别介绍Transformer架构中Attention计算、自注意力机制（Self-Attention）、多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Network）的实现。</p>
<ol>
<li>Attention计算</li>
</ol>
<p>Attention计算是Transformer中的一种基本操作，它可以计算序列数据中不同位置之间的关联程度。Attention计算包括三个重要的步骤：计算注意力权重、对序列数据进行加权求和和进行线性变换。具体地，给定一个查询向量Q、一组键向量K和一组数值向量V，Attention计算可以表示为：</p>
<p>Attention(Q,K,V) &#x3D; softmax(QK^T&#x2F;sqrt(d_k))V</p>
<p>其中，softmax函数用来计算注意力权重，sqrt(d_k)是一个缩放因子，可以使得内积在较大的维度时不会过于缩小，从而避免梯度消失的问题。</p>
<ol>
<li>自注意力机制（Self-Attention）</li>
</ol>
<p>自注意力机制是一种基于Attention计算的机制，它可以对序列数据中不同位置之间的关联程度进行建模，并且提取序列数据中的语义信息。在自注意力机制中，查询向量、键向量和数值向量均来自于输入序列数据，因此可以将自注意力机制视为一种对输入序列数据的编码操作。</p>
<p>具体地，在自注意力机制中，输入序列数据经过三个线性变换得到查询向量Q、键向量K和数值向量V，然后通过Attention计算得到加权求和的结果，即自注意力向量。自注意力向量可以被视为输入序列数据中不同位置之间的关联程度，从而可以提取出序列数据中的语义信息。</p>
<ol>
<li>多头自注意力机制（Multi-Head Self-Attention）</li>
</ol>
<p>多头自注意力机制是一种对自注意力机制的扩展，它可以同时学习多组注意力权重，从而可以更好地捕捉序列数据中的不同方面的语义信息。在多头自注意力机制中，输入序列数据通过多个线性变换得到多组查询向量Q、键向量K和数值向量V，然后分别进行Attention计算得到多个自注意力向量，并将这些向量进行拼接后再进行一次线性变换得到最终的多头自注意力向量。</p>
<ol>
<li>前馈神经网络（Feed-Forward Network）</li>
</ol>
<p>前馈神经网络是Transformer中的另一个重要组成部分，它可以对自注意力向量进行进一步的处理和提取。前馈神经网络包括两个线性变换和一个激活函数，其中第一个线性变换将自注意力向量的维度进行缩放，第二个线性变换将维度恢复到原来的大小，然后通过激活函数进行非线性变换。在Transformer中，前馈神经网络通常采用ReLU激活函数。</p>
<p>总之，Transformer架构包括Attention计算、自注意力机制、多头自注意力机制和前馈神经网络等部分，这些部分共同作用可以对序列数据进行编码和提取，从而实现自然语言处理任务。其中，Attention计算是基本操作，自注意力机制和多头自注意力机制可以对输入序列数据进行编码，前馈神经网络可以对编码结果进行进一步的处理和提取。</p>
<h4 id="知识点4：-LLaMA的Optimizer设计、模型加速优化与微型版本"><a href="#知识点4：-LLaMA的Optimizer设计、模型加速优化与微型版本" class="headerlink" title="知识点4： LLaMA的Optimizer设计、模型加速优化与微型版本"></a>知识点4： LLaMA的Optimizer设计、模型加速优化与微型版本</h4><p>LLaMA是一种非常强大的自然语言处理模型，它采用了一些特殊的技术和结构优化，包括RMSNorm、SwiGLU、RoPE和Transformer等。除此之外，LLaMA还采用了一些优化策略和技巧，包括Optimizer设计、模型加速优化和微型版本等。</p>
<ol>
<li>Optimizer设计</li>
</ol>
<p>LLaMA采用了一种自适应学习率的优化器，即AdamW优化器。AdamW优化器是基于Adam优化器的一种变体，它可以在保证收敛性的同时，进一步提高模型的泛化性能和稳定性。在AdamW优化器中，对于每个参数，都会维护一个自适应的学习率，从而可以根据每个参数的不同特性进行不同程度的更新。</p>
<ol>
<li>模型加速优化</li>
</ol>
<p>LLaMA采用了一些模型加速优化技巧，包括梯度累积、梯度裁剪和权重衰减等。梯度累积可以将多个小批量的梯度累积起来，从而可以在减少GPU内存占用的同时，增加批量大小。梯度裁剪可以防止梯度爆炸的问题，从而可以提高模型的稳定性和收敛速度。权重衰减可以防止模型过拟合，从而可以提高模型的泛化性能和稳定性。</p>
<ol>
<li>微型版本</li>
</ol>
<p>为了使LLaMA可以在资源受限的设备上运行，例如移动设备和嵌入式设备，LLaMA还提供了一些微型版本，包括MobileBERT和TinyBERT等。这些微型版本可以在保证模型性能的同时，减小模型的大小和计算量，从而可以在资源受限的设备上运行。</p>
<p>总之，LLaMA采用了一些特殊的技术和结构优化，包括RMSNorm、SwiGLU、RoPE和Transformer等。除此之外，LLaMA还采用了一些优化策略和技巧，包括Optimizer设计、模型加速优化和微型版本等。这些优化策略和技巧的应用，使得LLaMA成为了一种非常强大的自然语言处理模型，可以适用于各种不同的任务和场景。</p>
</li>
<li><h4 id="第3课-LLaMA的RLHF版：ChatLLaMA-英文版-x2F-ColossalChat-x2F-DeepSpeed-Chat"><a href="#第3课-LLaMA的RLHF版：ChatLLaMA-英文版-x2F-ColossalChat-x2F-DeepSpeed-Chat" class="headerlink" title="第3课 LLaMA的RLHF版：ChatLLaMA(英文版)&#x2F;ColossalChat&#x2F;DeepSpeed Chat"></a>第3课 LLaMA的RLHF版：ChatLLaMA(英文版)&#x2F;ColossalChat&#x2F;DeepSpeed Chat</h4><h4 id="知识点1：-ChatLLaMA三套数据集：分别训练actor、reward、rlhf"><a href="#知识点1：-ChatLLaMA三套数据集：分别训练actor、reward、rlhf" class="headerlink" title="知识点1： ChatLLaMA三套数据集：分别训练actor、reward、rlhf"></a>知识点1： ChatLLaMA三套数据集：分别训练actor、reward、rlhf</h4><p>ChatLLaMA是一个基于强化学习的对话生成模型，它采用了三个不同的数据集来进行训练，分别是Actor、Reward和RLHF。</p>
<ol>
<li>Actor</li>
</ol>
<p>Actor数据集是ChatLLaMA中的第一个数据集，它用于训练对话生成模型中的Actor部分。Actor是ChatLLaMA中的一个强化学习模块，它可以根据当前状态和用户输入，生成一个对话回复。在Actor数据集中，每个对话回复都被标注了一个对应的奖励分数，用于指导模型的训练和优化。</p>
<ol>
<li>Reward</li>
</ol>
<p>Reward数据集是ChatLLaMA中的第二个数据集，它用于训练对话生成模型中的Reward部分。Reward是ChatLLaMA中的另一个强化学习模块，它可以根据当前状态和用户输入，计算一个对话回复的奖励分数。在Reward数据集中，每个对话回复都被标注了一个对应的奖励分数，用于指导模型的训练和优化。</p>
<ol>
<li>RLHF</li>
</ol>
<p>RLHF数据集是ChatLLaMA中的第三个数据集，它用于训练对话生成模型中的强化学习模块。RLHF是一种基于强化学习的对话生成模型，它可以根据当前状态和用户输入，生成一个对话回复，并根据该回复的奖励分数进行优化。在RLHF数据集中，每个对话回复都被标注了一个对应的奖励分数，用于指导模型的训练和优化。</p>
<p>总之，ChatLLaMA采用了三个不同的数据集来进行训练，分别是Actor、Reward和RLHF。这些数据集都包括对话回复和对应的奖励分数，用于指导模型的训练和优化。通过这些数据集的训练，ChatLLaMA可以生成高质量的对话回复，并且可以在不同的应用场景中得到广泛的应用。</p>
<h4 id="知识点2：-ChatLLaMA训练流程：SFT、RM、RL-x2F-PPO训练三步骤"><a href="#知识点2：-ChatLLaMA训练流程：SFT、RM、RL-x2F-PPO训练三步骤" class="headerlink" title="知识点2： ChatLLaMA训练流程：SFT、RM、RL&#x2F;PPO训练三步骤"></a>知识点2： ChatLLaMA训练流程：SFT、RM、RL&#x2F;PPO训练三步骤</h4><p>ChatLLaMA的训练流程包括三个主要步骤：SFT（Supervised Fine-Tuning）、RM（Reinforcement Learning with Monte Carlo Tree Search）和RL&#x2F;PPO（Reinforcement Learning with Proximal Policy Optimization）训练。下面将分别介绍这三个步骤的具体内容。</p>
<ol>
<li>SFT（Supervised Fine-Tuning）</li>
</ol>
<p>在SFT阶段，ChatLLaMA使用有标注的对话数据对模型进行有监督的微调。这个阶段的目的是为了让模型更好地学习对话生成的基本技能，例如回复生成、流畅性等。在SFT阶段，模型的损失函数是交叉熵损失函数，优化器采用Adam优化器。</p>
<ol>
<li>RM（Reinforcement Learning with Monte Carlo Tree Search）</li>
</ol>
<p>在RM阶段，ChatLLaMA使用强化学习方法来让模型学习如何生成更加合理和自然的对话回复。在这个阶段，模型需要与一个人类评价者进行交互，评价者会给出每个对话回复的奖励分数，作为模型优化的指导。在这个阶段，ChatLLaMA采用Monte Carlo Tree Search（MCTS）算法来进行决策，即在每一步中，使用MCTS来搜索模型的最佳动作。在RM阶段，模型的损失函数是策略梯度损失函数，优化器采用Adam优化器。</p>
<ol>
<li>RL&#x2F;PPO（Reinforcement Learning with Proximal Policy Optimization）</li>
</ol>
<p>在RL&#x2F;PPO阶段，ChatLLaMA使用Proximal Policy Optimization（PPO）算法来进行强化学习。与RM阶段不同的是，在RL&#x2F;PPO阶段，模型的奖励分数不再是由人类评价者给出，而是由模型自行预测。在这个阶段，模型需要学会如何在不同的对话情境下生成合理和自然的对话回复，并且最大化预测到的奖励分数。在RL&#x2F;PPO阶段，模型的损失函数是PPO损失函数，优化器同样采用Adam优化器。</p>
<p>总之，ChatLLaMA的训练流程包括SFT、RM和RL&#x2F;PPO训练三个步骤。在SFT阶段，模型进行有监督的微调；在RM阶段，模型与人类评价者进行交互，使用MCTS算法进行决策；在RL&#x2F;PPO阶段，模型自行预测奖励分数，使用PPO算法进行强化学习。通过这三个步骤的训练，ChatLLaMA可以生成高质量的对话回复，并且可以在不同的应用场景中得到广泛的应用。</p>
<h4 id="知识点3：-ColossalChat技术架构：通过self-instruct生成的中英双语数据集-三阶段训练方式"><a href="#知识点3：-ColossalChat技术架构：通过self-instruct生成的中英双语数据集-三阶段训练方式" class="headerlink" title="知识点3： ColossalChat技术架构：通过self-instruct生成的中英双语数据集 + 三阶段训练方式"></a>知识点3： ColossalChat技术架构：通过self-instruct生成的中英双语数据集 + 三阶段训练方式</h4><p>ColossalChat是一种基于自我学习的对话生成模型，它采用了自我指导的方式来生成大规模的中英双语对话数据集，并且使用三阶段训练方式来提高模型的对话生成能力。下面将分别介绍ColossalChat的技术架构和训练方式。</p>
<ol>
<li>技术架构</li>
</ol>
<p>ColossalChat的技术架构主要包括以下几个部分：</p>
<ul>
<li>自我学习：ColossalChat使用自我学习的方式来生成大规模的中英双语对话数据集。具体来说，它采用了一种自我指导的方法，即先使用一个已经训练好的对话生成模型生成一些对话数据，再使用这些数据来训练下一个对话生成模型，以此类推，最终生成大规模的中英双语对话数据集。</li>
<li>对话生成模型：ColossalChat采用了Transformer模型来进行对话生成。Transformer是一种基于自注意力机制的神经网络模型，可以很好地处理自然语言处理任务，在对话生成方面也有很好的应用。</li>
<li>三阶段训练：ColossalChat使用三阶段训练方式来提高模型的对话生成能力。具体来说，训练分为无监督预训练、有监督微调和强化学习三个阶段。在无监督预训练阶段，模型使用自我生成的对话数据进行无监督预训练；在有监督微调阶段，模型使用人工标注的对话数据进行有监督微调；在强化学习阶段，模型通过与人类评价者进行交互，进行强化学习训练。</li>
</ul>
<ol>
<li>训练方式</li>
</ol>
<p>ColossalChat的训练方式主要分为三个阶段：</p>
<ul>
<li>无监督预训练：在这个阶段，ColossalChat使用自我生成的对话数据进行无监督预训练。具体来说，模型使用中文和英文的对话数据进行预训练，预训练的目的是让模型学习对话生成的基本技能。</li>
<li>有监督微调：在这个阶段，ColossalChat使用人工标注的对话数据进行有监督微调。具体来说，模型使用人工标注的中英文对话数据进行微调，微调的目的是让模型更好地适应真实对话数据的特点，并提高对话生成的质量。</li>
<li>强化学习：在这个阶段，ColossalChat通过与人类评价者进行交互，进行强化学习训练。具体来说，模型生成一系列对话回复，并由人类评价者对每个回复进行评分。根据评分，模型调整自身参数，以提高对话生成的质量和流畅性。</li>
</ul>
<p>总之，ColossalChat采用了自我学习的方式来生成大规模的中英双语对话数据集，并且使用三阶段训练方式来提高模型的对话生成能力。通过这种方式的训练，ColossalChat可以生成高质量的对话回复，并且可以在不同的应用场景中得到广泛的应用。</p>
<h4 id="知识点4：-ColossalChat的代码实现：SFT模型-奖励模型-PPO-training"><a href="#知识点4：-ColossalChat的代码实现：SFT模型-奖励模型-PPO-training" class="headerlink" title="知识点4： ColossalChat的代码实现：SFT模型 + 奖励模型 + PPO training"></a>知识点4： ColossalChat的代码实现：SFT模型 + 奖励模型 + PPO training</h4><p>ColossalChat的代码实现主要包括SFT模型、奖励模型和PPO训练三个部分。下面将分别介绍这三个部分的代码实现。</p>
<ol>
<li>SFT模型</li>
</ol>
<p>SFT模型是ColossalChat的第一个模型，它用于对对话生成模型进行有监督的微调。SFT模型的代码实现主要包括以下几个步骤：</p>
<ul>
<li>加载数据集：首先需要加载人工标注的中英文对话数据集。</li>
<li>模型定义：使用Transformer模型定义对话生成模型。</li>
<li>损失函数定义：使用交叉熵损失函数作为模型的损失函数。</li>
<li>优化器定义：使用Adam优化器进行模型的优化。</li>
<li>训练过程：使用加载的数据集对模型进行训练，训练过程中计算损失函数和进行模型优化。</li>
</ul>
<ol>
<li>奖励模型</li>
</ol>
<p>奖励模型是ColossalChat的第二个模型，它用于计算对话回复的奖励分数。奖励模型的代码实现主要包括以下几个步骤：</p>
<ul>
<li>加载数据集：首先需要加载人工标注的中英文对话数据集。</li>
<li>模型定义：使用Transformer模型定义奖励模型。</li>
<li>损失函数定义：使用均方误差（MSE）损失函数作为模型的损失函数。</li>
<li>优化器定义：使用Adam优化器进行模型的优化。</li>
<li>训练过程：使用加载的数据集对模型进行训练，训练过程中计算损失函数和进行模型优化。</li>
</ul>
<ol>
<li>PPO training</li>
</ol>
<p>PPO训练是ColossalChat的第三个部分，它用于强化学习训练对话生成模型。PPO训练的代码实现主要包括以下几个步骤：</p>
<ul>
<li>加载数据集：首先需要加载自我生成的中英文对话数据集。</li>
<li>模型定义：使用Transformer模型定义对话生成模型。</li>
<li>损失函数定义：使用PPO损失函数作为模型的损失函数。</li>
<li>优化器定义：使用Adam优化器进行模型的优化。</li>
<li>训练过程：使用加载的数据集对模型进行训练，训练过程中计算损失函数和进行模型优化。在训练过程中，模型需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。</li>
</ul>
<p>总之，ColossalChat的代码实现主要包括SFT模型、奖励模型和PPO训练三个部分。通过这些部分的实现，可以生成高质量的中英双语对话回复，并且可以在不同的应用场景中得到广泛的应用。</p>
<h4 id="知识点5：-微软DeepSpeed-Chat的讲解与实现：结合RLHF一键式训练自己的ChatGPT"><a href="#知识点5：-微软DeepSpeed-Chat的讲解与实现：结合RLHF一键式训练自己的ChatGPT" class="headerlink" title="知识点5： 微软DeepSpeed Chat的讲解与实现：结合RLHF一键式训练自己的ChatGPT"></a>知识点5： 微软DeepSpeed Chat的讲解与实现：结合RLHF一键式训练自己的ChatGPT</h4><p>微软DeepSpeed Chat是一种基于深度强化学习的对话生成模型，它采用了DeepSpeed框架进行训练，并结合了RLHF（Reinforcement Learning with Human Feedback）算法进行强化学习训练。下面将分别介绍DeepSpeed Chat的讲解和实现。</p>
<ol>
<li>讲解</li>
</ol>
<p>DeepSpeed Chat的训练过程主要分为以下几个步骤：</p>
<ul>
<li>数据准备：首先需要准备对话数据集，可以使用已有的对话数据集，也可以使用自己的数据集。</li>
<li>模型定义：使用GPT-2或GPT-3作为对话生成模型，并使用DeepSpeed框架定义模型。</li>
<li>RLHF算法训练：使用RLHF算法进行强化学习训练，训练过程中需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。</li>
<li>模型微调：使用微调算法对模型进行微调，以提高对话生成的质量和流畅性。</li>
<li>模型评估：使用人类评价者对模型生成的对话回复进行评估，以进一步提高模型的质量和流畅性。</li>
</ul>
<ol>
<li>实现</li>
</ol>
<p>DeepSpeed Chat的实现主要包括以下几个步骤：</p>
<ul>
<li>安装DeepSpeed：首先需要安装DeepSpeed框架，可以通过pip命令进行安装。</li>
<li>数据准备：准备对话数据集，可以使用已有的对话数据集，也可以使用自己的数据集。</li>
<li>模型定义：使用GPT-2或GPT-3作为对话生成模型，并使用DeepSpeed框架定义模型。</li>
<li>RLHF算法训练：使用RLHF算法进行强化学习训练，可以使用DeepSpeed提供的训练脚本进行训练。在训练过程中，需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。</li>
<li>模型微调：使用微调算法对模型进行微调，可以使用DeepSpeed提供的微调脚本进行微调。</li>
<li>模型评估：使用人类评价者对模型生成的对话回复进行评估，可以使用DeepSpeed提供的评估脚本进行评估。</li>
</ul>
<p>总之，DeepSpeed Chat是一种基于深度强化学习的对话生成模型，它采用了DeepSpeed框架进行训练，并结合了RLHF算法进行强化学习训练。通过这种方式的训练，可以生成高质量的对话回复，并且可以在不同的应用场景中得到广泛的应用。</p>
<h4 id="知识点6：-如何结合PPO算法从零起步实现RLHF"><a href="#知识点6：-如何结合PPO算法从零起步实现RLHF" class="headerlink" title="知识点6： 如何结合PPO算法从零起步实现RLHF"></a>知识点6： 如何结合PPO算法从零起步实现RLHF</h4><p>结合PPO算法从零起步实现RLHF主要包括以下几个步骤：</p>
<ol>
<li>数据准备：首先需要准备对话数据集，可以使用已有的对话数据集，也可以使用自己的数据集。</li>
<li>模型定义：使用GPT-2或GPT-3作为对话生成模型，并使用PyTorch框架定义模型。</li>
<li>环境定义：定义对话生成环境，包括对话生成模型、对话历史记录和人类评价者等。</li>
<li>PPO算法训练：使用PPO算法进行强化学习训练，训练过程中需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。</li>
<li>RLHF算法训练：使用RLHF算法进行强化学习训练，训练过程中需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。</li>
<li>模型微调：使用微调算法对模型进行微调，以提高对话生成的质量和流畅性。</li>
<li>模型评估：使用人类评价者对模型生成的对话回复进行评估，以进一步提高模型的质量和流畅性。</li>
</ol>
<p>具体实现步骤如下：</p>
<ol>
<li>数据准备</li>
</ol>
<p>准备对话数据集，可以使用已有的对话数据集，也可以使用自己的数据集。对话数据集应包含对话历史记录、当前对话回复和人类评价者的评价结果。</p>
<ol>
<li>模型定义</li>
</ol>
<p>使用GPT-2或GPT-3作为对话生成模型，并使用PyTorch框架定义模型。模型应包括对话历史记录和当前对话回复，以及与评价者进行交互的接口。</p>
<ol>
<li>环境定义</li>
</ol>
<p>定义对话生成环境，包括对话生成模型、对话历史记录和人类评价者等。对话历史记录应包含前几轮的对话记录，以提供上下文信息。人类评价者应提供对每个对话回复的评价结果。</p>
<ol>
<li>PPO算法训练</li>
</ol>
<p>使用PPO算法进行强化学习训练，训练过程中需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。在训练过程中，可以使用PyTorch框架实现PPO算法。</p>
<ol>
<li>RLHF算法训练</li>
</ol>
<p>使用RLHF算法进行强化学习训练，训练过程中需要与人类评价者进行交互，评价者会给出每个对话回复的奖励分数，用于指导模型优化。在训练过程中，可以使用PyTorch框架实现RLHF算法。</p>
<ol>
<li>模型微调</li>
</ol>
<p>使用微调算法对模型进行微调，以提高对话生成的质量和流畅性。微调算法可以是传统的有监督学习算法，也可以是强化学习算法。</p>
<ol>
<li>模型评估</li>
</ol>
<p>使用人类评价者对模型生成的对话回复进行评估，以进一步提高模型的质量和流畅性。评估结果可以用于指导模型优化和改进。</p>
<p>总之，结合PPO算法从零起步实现RLHF需要对强化学习和深度学习算法有一定的了解，并需要熟练使用相关的框架和工具。这种方法可以生成高质量的对话回复，并且可以在不同的应用场景中得到广泛的应用。如果对相关技术不是特别熟悉，可以考虑先学习一些基础知识，如强化学习、深度强化学习、PPO算法、RLHF算法等，然后根据需求和具体情况进行实践。同时，也可以参考相关资料、论文和开源代码，加深对技术的理解和应用。</p>
</li>
</ul>
<h3 id="第二部分-各种微调LLaMA：Alpaca、Vicuna、BELLE、中文LLaMA、姜子牙"><a href="#第二部分-各种微调LLaMA：Alpaca、Vicuna、BELLE、中文LLaMA、姜子牙" class="headerlink" title="第二部分 各种微调LLaMA：Alpaca、Vicuna、BELLE、中文LLaMA、姜子牙"></a>第二部分 各种微调LLaMA：Alpaca、Vicuna、BELLE、中文LLaMA、姜子牙</h3><ul>
<li><h4 id="第4课-Stanford-Alpaca：结合英文语料通过Self-Instruct方式微调LLaMA-7B"><a href="#第4课-Stanford-Alpaca：结合英文语料通过Self-Instruct方式微调LLaMA-7B" class="headerlink" title="第4课 Stanford Alpaca：结合英文语料通过Self Instruct方式微调LLaMA 7B"></a>第4课 Stanford Alpaca：结合英文语料通过Self Instruct方式微调LLaMA 7B</h4><h4 id="知识点1：-什么是self-instruct方式：提示GPT3-x2F-GPT3-5-x2F-GPT4的API收集数据"><a href="#知识点1：-什么是self-instruct方式：提示GPT3-x2F-GPT3-5-x2F-GPT4的API收集数据" class="headerlink" title="知识点1： 什么是self-instruct方式：提示GPT3&#x2F;GPT3.5&#x2F;GPT4的API收集数据"></a>知识点1： 什么是self-instruct方式：提示GPT3&#x2F;GPT3.5&#x2F;GPT4的API收集数据</h4><p>Self-instruct方式是指通过使用GPT-3&#x2F;GPT-3.5&#x2F;GPT-4等自然语言处理模型的API接口来收集数据，以改进模型的性能。具体而言，这种方式是指使用自然语言处理模型来生成一些文本，然后将这些文本作为训练数据集或测试数据集，用于训练或评估模型。</p>
<p>以GPT-3为例，它拥有1750亿个参数，并且可以生成高质量的自然语言文本。通过使用GPT-3的API接口，可以向模型提供一些提示文本，然后让模型自动生成一些有关该提示文本的补充文本。这些补充文本可以用来扩充数据集，从而提高模型的性能。</p>
<p>在使用Self-instruct方式时，需要注意以下几个问题：</p>
<ol>
<li>数据质量：生成的文本质量会直接影响模型的性能，因此需要确保生成的文本质量高。</li>
<li>数据样本：需要选择适当的提示文本和生成的文本，以保证生成的文本与提示文本相关，从而提高数据集的质量。</li>
<li>数据量：需要收集足够的数据量，以保证数据集的充分性和多样性。</li>
</ol>
<p>总之，Self-instruct方式是一种有效的收集数据的方法，可以利用自然语言处理模型的强大能力来扩充数据集，从而提高模型的性能。但是，在使用该方法时需要注意数据质量、数据样本和数据量等问题，并对生成的文本进行仔细的筛选和处理，以保证最终的数据集质量。</p>
<h4 id="知识点2：-微调LLM的一般都会用到Huggingface实现的Transformers库的Trainer类"><a href="#知识点2：-微调LLM的一般都会用到Huggingface实现的Transformers库的Trainer类" class="headerlink" title="知识点2： 微调LLM的一般都会用到Huggingface实现的Transformers库的Trainer类"></a>知识点2： 微调LLM的一般都会用到Huggingface实现的Transformers库的Trainer类</h4><p>微调LLM（Language Model）一般都会用到Huggingface实现的Transformers库的Trainer类。Trainer类是一个高级API，可以用于训练和评估自然语言处理模型，而不需要手动编写训练循环。它提供了许多功能，包括：</p>
<ol>
<li>数据加载：可以从各种数据源加载数据，例如文件、内存、网络等。</li>
<li>模型训练：支持多种训练策略，例如梯度累积、学习率调度、early stopping等，可以轻松地进行模型训练。</li>
<li>模型评估：支持多种评估策略，例如准确率、F1值、BLEU值等。</li>
<li>模型保存：支持将训练后的模型保存到本地或云端，以便后续使用。</li>
</ol>
<p>使用Trainer类进行微调LLM的一般步骤如下：</p>
<ol>
<li>准备数据集：将数据集处理为Huggingface库支持的格式，例如tokenize、编码、分割等。</li>
<li>定义模型：使用Huggingface库中的模型类定义LLM模型，例如GPT-2、BERT等。</li>
<li>定义训练参数：定义训练参数，例如批处理大小、学习率、梯度累积步数等。</li>
<li>实例化Trainer类：使用Trainer类的构造函数创建一个训练器对象。</li>
<li>训练模型：使用trainer对象的train()方法进行模型训练。</li>
<li>评估模型：使用trainer对象的evaluate()方法对模型进行评估。</li>
<li>保存模型：使用trainer对象的save_model()方法将模型保存到本地或云端。</li>
</ol>
<p>总之，使用Huggingface实现的Transformers库的Trainer类可以大大简化微调LLM的过程，使得模型训练和评估更加高效和方便。同时，Trainer类提供了许多高级功能，可以帮助用户轻松地进行模型训练和评估，并支持将训练后的模型保存到本地或云端，以便后续使用。</p>
<h4 id="知识点3：-Alpaca-LoRA：通过PEFT库在消费级GPU上微调「基于LLaMA的Alpaca」"><a href="#知识点3：-Alpaca-LoRA：通过PEFT库在消费级GPU上微调「基于LLaMA的Alpaca」" class="headerlink" title="知识点3： Alpaca-LoRA：通过PEFT库在消费级GPU上微调「基于LLaMA的Alpaca」"></a>知识点3： Alpaca-LoRA：通过PEFT库在消费级GPU上微调「基于LLaMA的Alpaca」</h4><p>Alpaca-LoRA是基于LLaMA的Alpaca模型的一个微调方法，可以在消费级GPU上进行微调。该方法使用了PEFT（Performance Estimation Tool）库，通过对GPU性能进行建模，优化模型训练过程，从而提高模型的微调效率和性能。</p>
<p>具体而言，Alpaca-LoRA方法的主要步骤如下：</p>
<ol>
<li>数据准备：准备用于微调的数据集，并将其处理为Huggingface库支持的格式。</li>
<li>模型定义：使用Huggingface库中的模型类定义基于LLaMA的Alpaca模型。</li>
<li>训练参数定义：定义训练参数，例如批处理大小、学习率、梯度累积步数等。</li>
<li>GPU性能建模：使用PEFT库对GPU性能进行建模，并确定最佳的训练超参数。</li>
<li>微调模型：使用确定的训练超参数和GPU性能建模结果，使用Trainer类进行模型微调。</li>
<li>模型评估：使用Trainer类进行模型评估，并选择最佳的微调模型。</li>
<li>模型保存：使用Trainer类将最佳的微调模型保存到本地或云端。</li>
</ol>
<p>总之，Alpaca-LoRA方法可以通过对GPU性能进行建模，优化模型训练过程，从而提高模型的微调效率和性能。该方法使用了PEFT库进行GPU性能建模，可以在消费级GPU上进行微调，适用于训练规模较小的模型。同时，该方法还使用了Huggingface库中的Trainer类进行模型微调和评估，简化了模型微调过程，使得模型训练和评估更加高效和方便。</p>
<h4 id="知识点4：-Alpaca所用的self-instruct的影响力：解决一大批模型的数据扩展问题"><a href="#知识点4：-Alpaca所用的self-instruct的影响力：解决一大批模型的数据扩展问题" class="headerlink" title="知识点4： Alpaca所用的self-instruct的影响力：解决一大批模型的数据扩展问题"></a>知识点4： Alpaca所用的self-instruct的影响力：解决一大批模型的数据扩展问题</h4><p>Alpaca模型使用的self-instruct方式对于解决一大批模型的数据扩展问题具有重要的影响力。传统的自然语言处理模型的训练需要大量的标注数据，但是标注数据的收集非常困难和昂贵，尤其是对于一些特定领域或语种的任务，标注数据更加稀缺。因此，如何扩展数据集，提高模型性能，一直是自然语言处理领域的研究热点之一。</p>
<p>Alpaca模型采用了self-instruct方式来扩展数据集，即通过使用模型自身生成的文本来增加训练数据。这种方式有两个主要的优势：</p>
<ol>
<li>数据扩展：使用模型自身生成的文本可以大大扩展数据集，提高模型性能。</li>
<li>数据多样性：生成的文本可以涵盖更多的语言风格和主题，从而提高数据集的多样性。</li>
</ol>
<p>通过使用self-instruct方式，Alpaca模型成功地提高了模型性能，并在多个自然语言处理任务上实现了领先的性能。同时，这种方式对于解决自然语言处理领域的数据稀缺问题具有重要的影响力，可以为其他自然语言处理模型的训练提供参考和启示。</p>
</li>
<li><h4 id="第5课-Vicuna-shareGPT-、BELLE-self-instruct-、Chinese-LLaMA-x2F-Chinese-Alpaca"><a href="#第5课-Vicuna-shareGPT-、BELLE-self-instruct-、Chinese-LLaMA-x2F-Chinese-Alpaca" class="headerlink" title="第5课 Vicuna(shareGPT)、BELLE(self-instruct)、Chinese-LLaMA&#x2F;Chinese-Alpaca"></a>第5课 Vicuna(shareGPT)、BELLE(self-instruct)、Chinese-LLaMA&#x2F;Chinese-Alpaca</h4><h4 id="知识点1：-UC-Berkeley的Vicuna-13B：通过ShareGPT-com的7万条对话数据微调LLaMA"><a href="#知识点1：-UC-Berkeley的Vicuna-13B：通过ShareGPT-com的7万条对话数据微调LLaMA" class="headerlink" title="知识点1： UC Berkeley的Vicuna-13B：通过ShareGPT.com的7万条对话数据微调LLaMA"></a>知识点1： UC Berkeley的Vicuna-13B：通过ShareGPT.com的7万条对话数据微调LLaMA</h4><p>Vicuna-13B是由UC Berkeley开发的一种基于LLaMA模型的自然语言处理模型。该模型使用了ShareGPT.com上的7万条对话数据进行微调，从而提高了模型的性能和准确率。</p>
<p>ShareGPT.com是一个在线平台，提供了大量的对话数据集，可以用于自然语言处理模型的微调。UC Berkeley的研究团队使用了ShareGPT.com上的7万条对话数据集，通过微调LLaMA模型，提高了模型在多项自然语言处理任务上的性能。</p>
<p>具体而言，Vicuna-13B模型的微调步骤如下：</p>
<ol>
<li>数据准备：从ShareGPT.com上下载7万条对话数据，并将其处理为Huggingface库支持的格式。</li>
<li>LLaMA模型定义：使用Huggingface库中的LLaMA模型类定义模型。</li>
<li>训练参数定义：定义训练参数，例如批处理大小、学习率、梯度累积步数等。</li>
<li>微调模型：使用Trainer类进行模型微调，提高模型在多项自然语言处理任务上的性能。</li>
<li>模型评估：使用Trainer类进行模型评估，并选择最佳的微调模型。</li>
<li>模型保存：使用Trainer类将最佳的微调模型保存到本地或云端。</li>
</ol>
<p>总之，Vicuna-13B模型使用了ShareGPT.com上的7万条对话数据进行微调，优化了模型性能和准确率。该方法可以为其他自然语言处理模型的微调提供参考和启示，并为自然语言处理领域的研究和应用提供了新的思路和方法。</p>
<h4 id="知识点2：-BELLE：结合中文语料通过Self-Instruct方式微调BLOOM-7B或LLaMA"><a href="#知识点2：-BELLE：结合中文语料通过Self-Instruct方式微调BLOOM-7B或LLaMA" class="headerlink" title="知识点2： BELLE：结合中文语料通过Self Instruct方式微调BLOOM-7B或LLaMA"></a>知识点2： BELLE：结合中文语料通过Self Instruct方式微调BLOOM-7B或LLaMA</h4><p>BELLE是一种基于Self Instruct方式微调BLOOM-7B或LLaMA模型的自然语言处理模型，该模型结合了中文语料，可以用于中文文本的处理任务。</p>
<p>具体而言，BELLE模型的微调步骤如下：</p>
<ol>
<li>数据准备：准备用于微调的中文语料，并将其处理为Huggingface库支持的格式。</li>
<li>模型选择：选择BLOOM-7B或LLaMA作为基础模型。</li>
<li>训练参数定义：定义训练参数，例如批处理大小、学习率、梯度累积步数等。</li>
<li>Self Instruct微调：使用Self Instruct方式进行微调，即使用模型自身生成的文本来增加训练数据。</li>
<li>模型评估：使用Trainer类进行模型评估，并选择最佳的微调模型。</li>
<li>模型保存：使用Trainer类将最佳的微调模型保存到本地或云端。</li>
</ol>
<p>BELLE模型使用了Self Instruct方式进行微调，可以扩展数据集，提高模型性能。同时，该模型结合了中文语料，可以用于中文文本的处理任务，如中文分词、命名实体识别、情感分析等。这些任务在中文自然语言处理中非常重要，BELLE模型可以为这些任务提供高效和准确的解决方案。</p>
<h3 id="知识点3：-Chinese-LLaMA-x2F-Chinese-Alpaca：通过中文数据预训练-x2F-指令微调-涉及词表扩充"><a href="#知识点3：-Chinese-LLaMA-x2F-Chinese-Alpaca：通过中文数据预训练-x2F-指令微调-涉及词表扩充" class="headerlink" title="知识点3： Chinese-LLaMA&#x2F;Chinese-Alpaca：通过中文数据预训练&#x2F;指令微调(涉及词表扩充)"></a>知识点3： Chinese-LLaMA&#x2F;Chinese-Alpaca：通过中文数据预训练&#x2F;指令微调(涉及词表扩充)</h3><p>Chinese-LLaMA和Chinese-Alpaca是基于LLaMA和Alpaca模型的中文自然语言处理模型，分别通过中文数据预训练和指令微调进行优化。</p>
<p>Chinese-LLaMA是基于LLaMA模型的中文自然语言处理模型，通过在大规模中文语料上进行预训练，提高模型的性能和泛化能力。在预训练过程中，模型可以自动学习中文语言的特征和规律，从而提高模型在中文自然语言处理任务中的表现。</p>
<p>Chinese-Alpaca则是通过指令微调方式对模型进行优化，通过对模型的指令微调，提高模型在特定的中文自然语言处理任务中的性能。在微调过程中，还会对模型的词表进行扩充，以适应更加复杂和多样的中文文本。</p>
<p>总之，Chinese-LLaMA和Chinese-Alpaca是基于LLaMA和Alpaca模型的中文自然语言处理模型，分别通过中文数据预训练和指令微调进行优化。这些模型可以为中文自然语言处理任务提供高效和准确的解决方案，具有重要的研究和应用价值。</p>
<h4 id="知识点4：-姜子牙Ziya-LLaMA-13B-v1的模型结构与微调部署"><a href="#知识点4：-姜子牙Ziya-LLaMA-13B-v1的模型结构与微调部署" class="headerlink" title="知识点4： 姜子牙Ziya-LLaMA-13B-v1的模型结构与微调部署"></a>知识点4： 姜子牙Ziya-LLaMA-13B-v1的模型结构与微调部署</h4><p>姜子牙Ziya-LLaMA-13B-v1是一种基于LLaMA模型的自然语言处理模型，由中国科学院自动化研究所的研究团队研发。该模型使用了大规模的中文语料进行预训练，并使用了指令微调来优化模型在特定任务上的表现。</p>
<p>姜子牙Ziya-LLaMA-13B-v1模型的结构与微调部署包括以下几个方面：</p>
<ol>
<li>模型结构：该模型基于LLaMA模型，使用了13亿个参数进行训练，具有较强的语言建模能力和泛化能力。该模型还使用了Transformer架构，通过多层自注意力机制来处理输入序列，从而提高模型的性能和效率。</li>
<li>预训练：该模型使用了大规模的中文语料进行预训练，包括维基百科、新闻、社交网络等多种类型的语料。预训练过程中，模型可以自动学习中文语言的特征和规律，从而提高模型的泛化能力和表现。</li>
<li>指令微调：该模型使用指令微调来优化模型在特定任务上的表现，例如中文分词、命名实体识别、情感分析等。在微调过程中，还会对模型的词表进行扩充，以适应更加复杂和多样的中文文本。</li>
<li>部署：该模型可以在GPU和CPU上进行高效的推理和部署，支持在线和离线应用场景。该模型还可以与其他自然语言处理工具和框架进行集成，以实现更加高效和全面的自然语言处理应用。</li>
</ol>
<p>总之，姜子牙Ziya-LLaMA-13B-v1是一种基于LLaMA模型的自然语言处理模型，具有较强的语言建模能力和泛化能力。该模型使用了大规模的中文语料进行预训练，并使用了指令微调来优化模型在特定任务上的表现。该模型可以在GPU和CPU上进行高效的推理和部署，具有重要的研究和应用价值。</p>
</li>
</ul>
<h3 id="第三部分-以ChatGLM2-6B-x2F-MOSS-x2F-baichuan为例如何训练LLM及调参部署"><a href="#第三部分-以ChatGLM2-6B-x2F-MOSS-x2F-baichuan为例如何训练LLM及调参部署" class="headerlink" title="第三部分 以ChatGLM2-6B&#x2F;MOSS&#x2F;baichuan为例如何训练LLM及调参部署"></a>第三部分 以ChatGLM2-6B&#x2F;MOSS&#x2F;baichuan为例如何训练LLM及调参部署</h3><ul>
<li><h4 id="第6课-ChatGLM2-6B-x2F-MOSS-x2F-baichuan的框架对比"><a href="#第6课-ChatGLM2-6B-x2F-MOSS-x2F-baichuan的框架对比" class="headerlink" title="第6课 ChatGLM2-6B&#x2F;MOSS&#x2F;baichuan的框架对比"></a>第6课 ChatGLM2-6B&#x2F;MOSS&#x2F;baichuan的框架对比</h4><h4 id="知识点1：-GLM的预训练和微调"><a href="#知识点1：-GLM的预训练和微调" class="headerlink" title="知识点1： GLM的预训练和微调"></a>知识点1： GLM的预训练和微调</h4><p>GLM（Generative Language Modeling）是一种基于生成式语言模型的自然语言处理方法，其预训练和微调步骤如下：</p>
<ol>
<li>预训练：GLM使用大规模的未标注数据进行预训练。在预训练阶段，GLM通过学习语言的概率分布来捕捉语言的语法和语义规律，从而提高模型的泛化能力。预训练的目标是通过最大化训练数据中的联合概率分布来训练模型，通常使用类似于语言模型的损失函数，例如交叉熵损失函数。</li>
<li>微调：在预训练后，GLM可以通过微调来适应特定的任务和领域。微调的目标是在特定任务和领域中优化模型的性能，通常使用类似于分类或回归的损失函数，例如交叉熵损失函数或均方误差损失函数。微调过程中，可以使用监督学习的方法，通过标注数据进行模型训练；也可以使用无监督学习的方法，通过自监督学习或对抗学习等方式进行模型训练。</li>
</ol>
<p>总之，GLM的预训练和微调是自然语言处理中常用的方法，可以提高模型的泛化能力和性能。预训练通过学习语言的概率分布来提高模型的泛化能力，微调则通过在特定任务和领域中优化模型的性能来提高模型的表现。这些方法在自然语言处理的各个领域中都有广泛的应用</p>
<h4 id="知识点2：-ChatGLM2-6B的升级：最长上下文32K且推理速度相比一代提升42"><a href="#知识点2：-ChatGLM2-6B的升级：最长上下文32K且推理速度相比一代提升42" class="headerlink" title="知识点2： ChatGLM2-6B的升级：最长上下文32K且推理速度相比一代提升42%"></a>知识点2： ChatGLM2-6B的升级：最长上下文32K且推理速度相比一代提升42%</h4><p>ChatGLM2-6B 在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了如下新特性：</p>
<ul>
<li>• <strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，全面升级了基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li>
<li>• <strong>更长的上下文</strong>：基于 FlashAttention 技术，将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。</li>
<li>• <strong>更高效的推理</strong>：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。</li>
<li>• <strong>更开放的协议</strong>：ChatGLM2-6B 权重对学术研究完全开放，在获得官方的书面许可后，亦<strong>允许商业使用</strong>。</li>
</ul>
<h4 id="知识点3：-MOSS大模型的训练框架与微调部署"><a href="#知识点3：-MOSS大模型的训练框架与微调部署" class="headerlink" title="知识点3： MOSS大模型的训练框架与微调部署"></a>知识点3： MOSS大模型的训练框架与微调部署</h4><p>MOSS 是一个支持中英双语和多种插件的开源对话语言模型，moss-moon 系列模型具有 160 亿参数，<strong>在 FP16 精度下可在单张 A100 &#x2F; A800 或两张 3090 显卡运行，在 INT4&#x2F;8 精度下可在单张 3090 显卡运行</strong>。MOSS 基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/OpenLMLab/MOSS/blob/main/README.md">https://github.com/OpenLMLab/MOSS/blob/main/README.md</a></p>
<h3 id="知识点4：-baichuan-7B-x2F-13B的模型结构-类似LLaMA-与微调部署"><a href="#知识点4：-baichuan-7B-x2F-13B的模型结构-类似LLaMA-与微调部署" class="headerlink" title="知识点4： baichuan-7B&#x2F;13B的模型结构(类似LLaMA)与微调部署"></a>知识点4： baichuan-7B&#x2F;13B的模型结构(类似LLaMA)与微调部署</h3><p>Baichuan-13B 是由百川智能继 <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/baichuan-7B">Baichuan-7B</a> 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (<a target="_blank" rel="noopener" href="https://huggingface.co/baichuan-inc/Baichuan-13B-Base">Baichuan-13B-Base</a>) 和对齐 (<a target="_blank" rel="noopener" href="https://huggingface.co/baichuan-inc/Baichuan-13B-Chat">Baichuan-13B-Chat</a>) 两个版本。Baichuan-13B 有如下几个特点：</p>
<p>1.<strong>更大尺寸、更多数据：</strong>Baichuan-13B 在 Baichuan-7B 的基础上进一步扩大参数量到 130 亿，并且在高质量的语料上训练了 1.4 万亿 tokens，超过 LLaMA-13B 40%，是当前开源 13B 尺寸下训练数据量最多的模型。支持中英双语，使用 ALiBi 位置编码，上下文窗口长度为 4096。</p>
<p>2.<strong>同时开源预训练和对齐模型：</strong>预训练模型是适用开发者的『 基座 』，而广大普通用户对有对话功能的对齐模型具有更强的需求。因此本次开源我们同时发布了对齐模型（Baichuan-13B-Chat），具有很强的对话能力，开箱即用，几行代码即可简单的部署。</p>
<p>3.<strong>更高效的推理：</strong>为了支持更广大用户的使用，我们本次同时开源了 int8 和 int4 的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低了部署的机器资源门槛，可以部署在如 Nvidia 3090 这样的消费级显卡上。</p>
<p>4.<strong>开源免费可商用：</strong>Baichuan-13B 不仅对学术研究完全开放，开发者也仅需邮件申请并获得官方商用许可后，即可以免费商用。</p>
<p>开源地址： <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/Baichuan-13B">https://github.com/baichuan-inc/Baichuan-13B</a> </p>
<p>﻿</p>
</li>
<li><h4 id="第7课-基于ChatGLM2-6B的微调与实践应用"><a href="#第7课-基于ChatGLM2-6B的微调与实践应用" class="headerlink" title="第7课 基于ChatGLM2-6B的微调与实践应用"></a>第7课 基于ChatGLM2-6B的微调与实践应用</h4><h4 id="知识点1：-微调ChatGLM-6B：针对各种数据集通过LoRA或P-Tuning-v2"><a href="#知识点1：-微调ChatGLM-6B：针对各种数据集通过LoRA或P-Tuning-v2" class="headerlink" title="知识点1： 微调ChatGLM-6B：针对各种数据集通过LoRA或P-Tuning v2"></a>知识点1： 微调ChatGLM-6B：针对各种数据集通过LoRA或P-Tuning v2</h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Acecai01/article/details/131463608">https://blog.csdn.net/Acecai01/article/details/131463608</a></p>
<h4 id="知识点2：-基于ChatGLM-6B的本地知识库的应用实现-含完整代码"><a href="#知识点2：-基于ChatGLM-6B的本地知识库的应用实现-含完整代码" class="headerlink" title="知识点2： 基于ChatGLM-6B的本地知识库的应用实现(含完整代码)"></a>知识点2： 基于ChatGLM-6B的本地知识库的应用实现(含完整代码)</h4><p><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/643977aa446c45f4592a1e59">https://www.heywhale.com/mw/project/643977aa446c45f4592a1e59</a></p>
</li>
<li><h4 id="第8课-大语言模型预训练、微调、部署指南"><a href="#第8课-大语言模型预训练、微调、部署指南" class="headerlink" title="第8课 大语言模型预训练、微调、部署指南"></a>第8课 大语言模型预训练、微调、部署指南</h4><h4 id="知识点1：-Megatron-DeepSpeed到底是什么原理"><a href="#知识点1：-Megatron-DeepSpeed到底是什么原理" class="headerlink" title="知识点1： Megatron-DeepSpeed到底是什么原理"></a>知识点1： Megatron-DeepSpeed到底是什么原理</h4><p>Megatron和DeepSpeed是针对Transformer模型的两种不同的优化策略。</p>
<p>Megatron的基本思想是:</p>
<ol>
<li>模型并行(model parallel):将模型的不同部分(如attention模块和FFN模块)分配给不同的GPU。从而在更多GPU上训练更大的模型。</li>
<li>数据并行(data parallel):将同一批训练数据分发给多个GPU,并行计算其梯度,最后汇总。有效利用多GPU资源。</li>
<li>花式混合并行:结合模型并行和数据并行,以及不同并行粒度,实现高效训练。</li>
</ol>
<p>而DeepSpeed的核心思想是:</p>
<ol>
<li>零位优化(zero stage optimization):针对模型的不同部分进行不同的优化,包括量化、剪裁、稀疏 etc。</li>
<li>混合精度训练: 使用FP16等低精度的数据格式训练,降低GPU计算开销。</li>
<li>模型压缩:采用知识蒸馏、剪裁等手段,有效压缩模型体积。</li>
<li>高效数据读取:利用复用缓存、批归一化等手段改进数据读取。</li>
</ol>
<p>简而言之:</p>
<ul>
<li>Megatron侧重于利用多GPU资源训练大模型。</li>
<li>DeepSpeed则侧重于对单个模型实施各种优化手段,降低训练成本。</li>
</ul>
<p>两者互补且互有所长,目前通常会采用Megatron + DeepSpeed的混合策略,共同促进 Transformer 模型高效训练。</p>
<h4 id="知识点2：-ZeRO、数据并行、模型并行、张量并行"><a href="#知识点2：-ZeRO、数据并行、模型并行、张量并行" class="headerlink" title="知识点2： ZeRO、数据并行、模型并行、张量并行"></a>知识点2： ZeRO、数据并行、模型并行、张量并行</h4><p>ZeRO、数据并行、模型并行、张量并行都是分布式训练中常用的技术，用于加速大规模模型的训练。</p>
<ol>
<li>ZeRO：ZeRO（Zero Redundancy Optimizer）是一种优化分布式训练的技术，可以减少通信开销和内存占用，提高训练速度和规模。ZeRO采用了模型并行和数据并行的方式，将模型分割成多个部分，并将不同部分分配到不同的GPU上进行计算。同时，ZeRO还通过减少参数冗余和优化内存使用等技术，来提高训练效率和性能。</li>
<li>数据并行：数据并行是一种分布式训练的技术，将数据分成多个部分，分配到不同的GPU上进行计算，以提高训练速度和规模。数据并行通常用于处理大规模数据集，可以将数据集分成多个小批次，每个GPU处理其中的一部分数据，然后将梯度合并到一个梯度中，更新模型参数。</li>
<li>模型并行：模型并行是一种分布式训练的技术，将模型分成多个部分，分配到不同的GPU上进行计算，以提高训练速度和规模。模型并行通常用于处理大规模模型，可以将模型分成多个子模型，每个GPU处理其中的一部分模型，然后将梯度合并到一个梯度中，更新模型参数。</li>
<li>张量并行：张量并行是一种分布式训练的技术，将张量分成多个部分，分配到不同的GPU上进行计算，以提高训练速度和规模。张量并行通常用于处理大规模张量，可以将张量分成多个子张量，每个GPU处理其中的一部分张量，然后将梯度合并到一个梯度中，更新模型参数。</li>
</ol>
<p>总之，ZeRO、数据并行、模型并行、张量并行都是分布式训练中常用的技术，用于加速大规模模型的训练。这些技术的应用可以提高训练效率和性能，从而加速模型的训练和推理。</p>
<h4 id="知识点3：-如何更好的调参：Optimizer设计-Adam等-、epochs、learning-rate、dropout"><a href="#知识点3：-如何更好的调参：Optimizer设计-Adam等-、epochs、learning-rate、dropout" class="headerlink" title="知识点3： 如何更好的调参：Optimizer设计(Adam等)、epochs、learning rate、dropout"></a>知识点3： 如何更好的调参：Optimizer设计(Adam等)、epochs、learning rate、dropout</h4><p>调参是模型优化的重要步骤，可以通过优化参数来提高模型的性能。在调参过程中，需要考虑优化器设计、epochs、learning rate和dropout等参数。下面是一些有关这些参数的调优技巧：</p>
<ol>
<li>优化器设计：常用的优化器包括Adam、SGD等，不同的优化器在不同的场景下性能表现可能会有所不同。一般来说，Adam优化器在大多数情况下表现良好，但在某些场景下，SGD等优化器的表现可能会更好。因此，在调参时需要根据具体场景选择合适的优化器。</li>
<li>epochs：epochs是指模型训练的轮数。在调参时，需要通过实验找到一个合适的epochs值。通常情况下，epochs值越大，模型的性能可能会更好，但是训练时间和计算资源也会增加。因此，需要在模型性能和计算效率之间进行权衡，找到一个合适的epochs值。</li>
<li>learning rate：learning rate是指模型训练中的学习率，是一个非常重要的参数。在调参时，需要通过实验找到一个合适的learning rate值。通常情况下，learning rate越小，模型的收敛时间越长，但是模型的性能可能会更好；learning rate越大，模型的收敛时间越短，但是模型的性能可能会受到影响。因此，需要在模型性能和收敛速度之间进行权衡，找到一个合适的learning rate值。</li>
<li>dropout：dropout是一种正则化技术，用于防止过拟合。在调参时，需要通过实验找到一个合适的dropout值。通常情况下，dropout值越大，模型的正则化效果越好，但是可能会影响模型的性能；dropout值越小，模型的性能可能会更好，但是可能会导致过拟合。因此，需要在模型性能和正则化效果之间进行权衡，找到一个合适的dropout值。</li>
</ol>
<p>总之，调参是模型优化的重要步骤，需要根据具体场景选择合适的优化器、epochs、learning rate和dropout等参数。在调参过程中，需要进行实验和对比，找到一个合适的参数组合，以提高模型的性能和泛化能力。</p>
<h4 id="知识点4：-正则化-x2F-稳定性，与数据增强"><a href="#知识点4：-正则化-x2F-稳定性，与数据增强" class="headerlink" title="知识点4： 正则化&#x2F;稳定性，与数据增强"></a>知识点4： 正则化&#x2F;稳定性，与数据增强</h4><p>正则化和稳定性是一些用于提高模型泛化能力的技术，而数据增强则是一种数据预处理技术，可以扩充训练数据集，提高模型的鲁棒性和泛化能力。</p>
<ol>
<li>正则化：正则化是一种用于降低模型过拟合的技术，可以通过在损失函数中添加正则项，限制模型参数的大小和复杂度。常用的正则化方法包括L1正则化、L2正则化等。正则化可以提高模型的泛化能力，避免模型在训练数据上过拟合，但是可能会导致模型的训练时间增加。</li>
<li>稳定性：稳定性是一种用于提高模型鲁棒性和泛化能力的技术，可以使用各种方法来增强模型的稳定性，例如添加噪声、使用批标准化等。稳定性可以使模型对噪声和异常数据更加鲁棒，并且在测试集上表现更好。</li>
<li>数据增强：数据增强是一种数据预处理技术，可以通过对原始数据进行变换和扩充，来增加训练数据的数量，提高模型的鲁棒性和泛化能力。数据增强可以采用各种方法，例如旋转、平移、剪切、缩放等。数据增强可以帮助模型更好地学习数据的不变性，提高模型的泛化能力和鲁棒性。</li>
</ol>
<p>总之，正则化和稳定性是提高模型泛化能力的重要技术，可以减少过拟合、提高鲁棒性；数据增强是一种数据预处理技术，可以扩充训练数据集，提高模型的鲁棒性和泛化能力。在实际应用中，可以根据具体场景选择合适的技术，以提高模型的性能和泛化能力。</p>
<h4 id="知识点5：-如何更好的部署：模型的压缩、蒸馏、量化、剪枝"><a href="#知识点5：-如何更好的部署：模型的压缩、蒸馏、量化、剪枝" class="headerlink" title="知识点5： 如何更好的部署：模型的压缩、蒸馏、量化、剪枝"></a>知识点5： 如何更好的部署：模型的压缩、蒸馏、量化、剪枝</h4><ul>
<li><p>模型的部署是将训练好的模型应用于实际场景的重要步骤。为了在实际场景中获得更好的性能和效率，需要对模型进行一系列优化，例如模型的压缩、蒸馏、量化、剪枝等。</p>
<ol>
<li>模型压缩：模型压缩是一种用于减少模型大小和计算量的技术，可以通过去除冗余参数、降低模型精度、剪枝等方法实现。常用的模型压缩方法包括网络剪枝、低秩分解、权重共享等。模型压缩可以减少模型的大小和计算量，从而提高模型的效率和性能。</li>
<li>模型蒸馏：模型蒸馏是一种用于将大模型转换为小模型的技术，可以通过训练小模型来拟合大模型的行为。模型蒸馏可以通过减少模型中的冗余参数和复杂度，以及提高模型的泛化能力，来实现模型的压缩和优化。</li>
<li>模型量化：模型量化是一种用于减少模型计算量和内存占用的技术，可以将模型参数从高精度浮点数转换为低精度整数表示。常用的模型量化方法包括定点量化、动态量化等。模型量化可以显著减少模型的计算量和内存占用，从而提高模型的效率和性能。</li>
<li>模型剪枝：模型剪枝是一种用于减少模型大小和计算量的技术，可以通过去除冗余参数、减少模型深度、删除不重要的连接等方法实现。常用的模型剪枝方法包括结构剪枝、权重剪枝等。模型剪枝可以减少模型的大小和计算量，从而提高模型的效率和性能。</li>
</ol>
<p>总之，模型的部署是将训练好的模型应用于实际场景的重要步骤，需要对模型进行一系列优化，例如模型的压缩、蒸馏、量化、剪枝等。在实际应用中，可以根据具体场景选择合适的优化方法，以提高模型的效率和性能。</p>
</li>
</ul>
</li>
</ul>
<h3 id="第四部分-结合垂域数据或自己的数据定制自己的ChatGPT"><a href="#第四部分-结合垂域数据或自己的数据定制自己的ChatGPT" class="headerlink" title="第四部分 结合垂域数据或自己的数据定制自己的ChatGPT"></a>第四部分 结合垂域数据或自己的数据定制自己的ChatGPT</h3><ul>
<li><h4 id="第9课-以医疗-x2F-金融-x2F-法律数据为例：垂域数据与自己数据的处理"><a href="#第9课-以医疗-x2F-金融-x2F-法律数据为例：垂域数据与自己数据的处理" class="headerlink" title="第9课 以医疗&#x2F;金融&#x2F;法律数据为例：垂域数据与自己数据的处理"></a>第9课 以医疗&#x2F;金融&#x2F;法律数据为例：垂域数据与自己数据的处理</h4><h4 id="知识点1：-数据的收集清洗、格式规范：数据处理好了-成功一半"><a href="#知识点1：-数据的收集清洗、格式规范：数据处理好了-成功一半" class="headerlink" title="知识点1： 数据的收集清洗、格式规范：数据处理好了 成功一半"></a>知识点1： 数据的收集清洗、格式规范：数据处理好了 成功一半</h4><p>数据的收集、清洗和格式规范是机器学习和深度学习等任务的重要前置步骤，对于模型的训练和性能有着至关重要的影响。下面简要介绍一下数据处理的常见方法和技巧：</p>
<ol>
<li>数据收集：数据收集是指收集和获取用于训练模型的数据。在数据收集之前，需要明确数据的来源和目的，并且了解数据的特征和属性。常用的数据收集方法包括数据爬取、数据采集、人工标注等。</li>
<li>数据清洗：数据清洗是指对原始数据进行去噪、去重、填充缺失值、异常值处理等操作，以保证数据的质量和准确性。数据清洗可以帮助排除数据中的无效信息和异常数据，提高模型的准确性和鲁棒性。</li>
<li>数据格式规范：数据格式规范是指对数据的格式、结构、类型等进行规范化和标准化，以保证数据的一致性和统一性。常用的数据格式规范方法包括数据类型转换、标准化、归一化等。数据格式规范可以提高模型的训练效率和性能。</li>
</ol>
<p>总之，数据的收集、清洗和格式规范是机器学习和深度学习等任务的重要前置步骤，对于模型的训练和性能有着至关重要的影响。在实际应用中，需要根据具体场景采取合适的数据处理方法和技巧，以提高模型的准确性和鲁棒性。</p>
<h4 id="知识点2：-分词-tokenizer-与编码-如BPE、WordPiece等）"><a href="#知识点2：-分词-tokenizer-与编码-如BPE、WordPiece等）" class="headerlink" title="知识点2： 分词(tokenizer)与编码(如BPE、WordPiece等）"></a>知识点2： 分词(tokenizer)与编码(如BPE、WordPiece等）</h4><p>在自然语言处理任务中，分词和编码是两个重要的步骤。</p>
<ol>
<li>分词（tokenizer）：分词是将一段文本分割成一个个单独的词语的过程。在自然语言处理中，通常将一个词定义为具有独立语义或语法功能的最小单位。分词的目的是将文本转换为计算机可以理解和处理的形式。常见的分词方法有基于规则的分词方法和基于统计模型的分词方法。</li>
<li>编码：编码是将文本转换为数字序列的过程。在自然语言处理中，通常使用向量表示来表示文本。常用的编码方法有BPE（Byte Pair Encoding）、WordPiece等。这些编码方法利用统计模型和机器学习算法，将词语或字母组合转换为数字序列。编码的目的是将文本转换为计算机可以处理的数字表示形式，以便进行后续的处理和分析。</li>
</ol>
<p>总之，分词和编码是自然语言处理中的两个重要步骤，分别用于将文本转换为计算机可以理解和处理的形式。在实际应用中，可以根据具体任务和数据集的特点选择合适的分词和编码方法，以提高模型的性能和效果。</p>
<h4 id="知识点3：-数据增强技术、数据集划分与加载"><a href="#知识点3：-数据增强技术、数据集划分与加载" class="headerlink" title="知识点3： 数据增强技术、数据集划分与加载"></a>知识点3： 数据增强技术、数据集划分与加载</h4><p>数据增强技术、数据集划分和加载是机器学习中常用的数据预处理步骤，用于提高模型的泛化性能和减少过拟合。</p>
<p>数据增强技术指的是通过对原始数据进行一系列的变换和扩充来生成更多的训练数据，从而提高模型的鲁棒性和泛化性能。常用的数据增强技术包括图像翻转、旋转、缩放、裁剪、加噪声等。对于文本数据，常用的数据增强技术包括替换、删除、插入、交换等。数据增强技术可以通过增加数据量、增加数据多样性、减少过拟合等方式提高模型的性能。</p>
<p>数据集划分是将原始数据集划分为训练集、验证集和测试集的过程。训练集用于训练模型，验证集用于调整模型的超参数和评估模型的性能，测试集用于最终评估模型的性能。数据集划分的目的是防止模型在训练过程中过拟合，同时也可以用于评估模型的泛化性能。通常，训练集占原始数据集的大多数，验证集和测试集占比较少的部分。在实际应用中，可以采用交叉验证等技术来更好地利用数据集。</p>
<p>数据加载是将数据集加载到内存中以便进行训练和评估。数据加载的方式包括批量读取、并行读取等。在深度学习中，通常采用小批量随机梯度下降的方式来训练模型，因此需要将数据集划分为若干个小批量，并且要保证每个小批量的数据样本是均匀分布的。数据加载的效率对于训练模型的速度和性能有重要影响，因此需要采用高效的数据加载技术。常用的数据加载库包括PyTorch、TensorFlow等。</p>
<h4 id="知识点4：-如何提升数据利用率及数据细粒度上的理解"><a href="#知识点4：-如何提升数据利用率及数据细粒度上的理解" class="headerlink" title="知识点4： 如何提升数据利用率及数据细粒度上的理解"></a>知识点4： 如何提升数据利用率及数据细粒度上的理解</h4><p>提升数据利用率和数据细粒度上的理解是机器学习和深度学习中数据处理的重要目标之一。下面是一些常用的方法：</p>
<ol>
<li>数据增强：数据增强是通过对原始数据进行变换和扩充，来增加训练数据的数量，提高模型的鲁棒性和泛化能力。数据增强可以帮助模型更好地学习数据的不变性，提高数据利用率和模型的泛化能力。</li>
<li>数据清洗和预处理：数据清洗和预处理是将原始数据进行去噪、去重、填充缺失值、异常值处理等操作，以保证数据的质量和准确性。数据清洗和预处理可以提高数据的质量和利用率，减少噪声和干扰，提高模型的准确性和鲁棒性。</li>
<li>数据标注和注释：数据标注和注释是对数据进行分类、标记、注释、描述等操作，以便更好地理解和利用数据。数据标注和注释可以提高数据的细粒度理解和利用率，帮助模型更好地了解数据的含义和关系。</li>
<li>多任务学习：多任务学习是一种同时训练多个任务的方法，可以提高数据的利用率和模型的泛化能力。多任务学习可以利用不同任务之间的相关性和共享信息，提高数据的利用率和模型的效果。</li>
</ol>
<p>总之，提升数据利用率和数据细粒度上的理解是机器学习和深度学习中数据处理的重要目标之一。在实际应用中，可以根据具体任务和数据集的特点选择合适的方法和技巧，以提高数据利用率和模型的效果。</p>
</li>
<li><h4 id="第10课-模型的优化、评估与部署上线"><a href="#第10课-模型的优化、评估与部署上线" class="headerlink" title="第10课 模型的优化、评估与部署上线"></a>第10课 模型的优化、评估与部署上线</h4><h4 id="知识点1：-如何更好的解决灾难性遗忘：再训练、学习率调整、权重衰减等"><a href="#知识点1：-如何更好的解决灾难性遗忘：再训练、学习率调整、权重衰减等" class="headerlink" title="知识点1： 如何更好的解决灾难性遗忘：再训练、学习率调整、权重衰减等"></a>知识点1： 如何更好的解决灾难性遗忘：再训练、学习率调整、权重衰减等</h4><p>灾难性遗忘是指在模型训练过程中，当新的数据被添加进来，原来的模型会忘记之前学习到的知识，导致模型的性能下降。为了解决灾难性遗忘问题，可以采用以下方法：</p>
<ol>
<li>再训练：在原来的模型基础上，继续使用新的数据进行训练。这种方法可以保留原来的知识，同时学习新的知识。但是，再训练可能会导致过拟合，需要注意调整超参数，以避免出现过拟合问题。</li>
<li>学习率调整：在新数据的训练过程中，可以降低学习率，以减缓模型对新数据的更新速度。这种方法可以让模型更加稳定，避免新数据对原有知识的影响过大。但是，学习率调整可能会导致收敛速度较慢，需要根据具体情况进行调整。</li>
<li>权重衰减：在模型训练过程中，可以使用权重衰减（Weight Decay）等正则化方法，对模型的权重进行约束。这种方法可以减少模型的过拟合，避免新数据对原有知识的影响过大。但是，权重衰减可能会导致模型的性能下降，需要根据具体情况进行调整。</li>
</ol>
<p>总之，灾难性遗忘是机器学习中常见的问题，可以通过再训练、学习率调整、权重衰减等方法来解决。在实际应用中，需要根据具体情况选择合适的方法和调整超参数，以提高模型的性能和泛化能力。</p>
<h3 id="知识点2：-如何更好的在线监测、在线学习"><a href="#知识点2：-如何更好的在线监测、在线学习" class="headerlink" title="知识点2： 如何更好的在线监测、在线学习"></a>知识点2： 如何更好的在线监测、在线学习</h3><p>在线监测和在线学习是机器学习中常见的技术，可以提高模型的效果和实用性。以下是一些常用的方法：</p>
<ol>
<li>在线监测：在线监测是指对模型的实时性能进行监控和评估。可以使用一些指标（如准确率、召回率、F1值等）来评估模型的性能，并及时发现和处理异常情况。在线监测可以帮助我们及时发现模型的问题，及时进行调整和优化，提高模型的性能和实用性。</li>
<li>在线学习：在线学习是指在模型部署后，不断从实时数据中学习和更新模型。可以使用增量学习算法（如在线SVM、增量式随机森林等）进行在线学习。在线学习可以使模型不断适应变化的数据，保持模型的实时性和准确性。</li>
<li>滑动窗口：滑动窗口是一种常用的数据处理方法，可以对数据进行实时处理。可以使用滑动窗口来实现在线学习和在线监测，对实时数据进行处理和分析。滑动窗口可以使模型及时获取最新的数据，保持模型的实时性和准确性。</li>
</ol>
<p>总之，在线监测和在线学习是机器学习中常见的技术，可以提高模型的效果和实用性。在实际应用中，可以根据具体需求和场景选择合适的方法和算法，以提高模型的性能和实用性。</p>
<h4 id="知识点3：-如何评估一个LLM的效果，都有哪些评估指标"><a href="#知识点3：-如何评估一个LLM的效果，都有哪些评估指标" class="headerlink" title="知识点3： 如何评估一个LLM的效果，都有哪些评估指标"></a>知识点3： 如何评估一个LLM的效果，都有哪些评估指标</h4><p>如何评估一个LLM的效果，都有哪些评估指标：</p>
<ol>
<li>困惑度（Perplexity）：困惑度是一种常用的评估指标，用于衡量语言模型在预测下一个词时的不确定性。困惑度越低，说明模型的预测能力越好。</li>
<li>准确率（Accuracy）：准确率是指模型在预测下一个词时的正确率。准确率越高，说明模型的预测能力越好。</li>
<li>召回率（Recall）：召回率是指模型在预测下一个词时能够正确预测出的比例。召回率越高，说明模型的预测能力越好。</li>
<li>F1值（F1-score）：F1值是准确率和召回率的调和平均数，用于综合评估模型的预测能力。</li>
<li>BLEU 分数（BLEU Score）：BLEU 分数是一种常用的机器翻译评估指标，用于衡量翻译结果与参考答案之间的相似度。</li>
<li>ROUGE 分数（ROUGE Score）：ROUGE 分数是一种常用的文本摘要评估指标，用于衡量模型生成的摘要与参考摘要之间的相似度。</li>
<li>生成样本的多样性（Diversity）：生成样本的多样性是指模型生成的文本样本的多样性程度。如果模型生成的文本样本过于单调，会影响用户的体验。</li>
<li>生成速度（Generation Speed）：生成速度是指模型生成文本的速度。如果模型的生成速度过慢，会影响用户的体验。</li>
</ol>
<p>以上指标并不是全部，评估指标的选择应该根据具体的应用场景和需求来确定。</p>
<h4 id="知识点4：-知识蒸馏与模型剪枝、量化与轻量化模型"><a href="#知识点4：-知识蒸馏与模型剪枝、量化与轻量化模型" class="headerlink" title="知识点4： 知识蒸馏与模型剪枝、量化与轻量化模型"></a>知识点4： 知识蒸馏与模型剪枝、量化与轻量化模型</h4><p>知识蒸馏、模型剪枝、量化和轻量化模型是一些常用的模型压缩技术，可以减少模型的大小和计算量，提高模型的效率和实用性。以下是这些技术的简要介绍：</p>
<ol>
<li>知识蒸馏（Knowledge Distillation）：知识蒸馏是一种将大型模型的知识传递到小型模型中的技术。通常，使用一个大型模型作为“教师模型”，将其预测结果作为“软标签”传递给一个小型模型作为“学生模型”，以便学生模型更快地收敛并获得更好的性能。</li>
<li>模型剪枝（Model Pruning）：模型剪枝是一种通过删除模型中不必要的参数和连接来减少模型大小和计算量的技术。通常，使用一些启发式算法（如L1正则化、结构化剪枝、通道剪枝等）来寻找可以删除的参数和连接，并将其从模型中删除。</li>
<li>量化（Quantization）：量化是一种将模型参数从高精度浮点数转换为低精度整数或定点数的技术。通常，使用一些量化算法（如线性量化、对称量化、非对称量化等）来将模型参数量化，并将其存储为整数或定点数，以减少模型的大小和计算量。</li>
<li>轻量化模型（Lightweight Model）：轻量化模型是一种专门设计用于移动设备或嵌入式设备上的小型模型。通常，使用一些轻量化技术（如深度可分离卷积、MobileNet、ShuffleNet等）来设计高效的模型结构，并减少模型的大小和计算量。</li>
</ol>
<p>总之，知识蒸馏、模型剪枝、量化和轻量化模型是一些常用的模型压缩技术，可以减少模型的大小和计算量，提高模型的效率和实用性。在实际应用中，可以根据具体场景和需求选择合适的技术和方法，以提高模型的性能和效率。</p>
<h4 id="知识点5：-以OpenMP为例，如何更好的做集群部署"><a href="#知识点5：-以OpenMP为例，如何更好的做集群部署" class="headerlink" title="知识点5： 以OpenMP为例，如何更好的做集群部署"></a>知识点5： 以OpenMP为例，如何更好的做集群部署</h4><p>OpenMP 是一种多线程编程模型，常用于共享内存并行计算。在集群部署环境下，可以通过以下几种方法来更好地使用 OpenMP：</p>
<ol>
<li>了解集群环境：在进行 OpenMP 集群部署之前，需要了解集群的硬件配置、操作系统、网络拓扑结构等信息。这有助于更好地优化 OpenMP 程序的性能。</li>
<li>选择适当的编译器：在进行 OpenMP 集群部署时，需要选择适当的编译器。常用的编译器包括 GCC、Intel C++ 和 Clang 等。不同的编译器对 OpenMP 的支持程度不同。选择合适的编译器可以提高程序的性能。</li>
<li>优化 OpenMP 程序：在进行 OpenMP 集群部署时，需要对 OpenMP 程序进行优化。常用的优化方法包括数据局部性优化、循环展开、数据复制等。这些优化方法可以提高程序的性能。</li>
<li>部署程序：在进行 OpenMP 集群部署时，需要将程序部署到集群上。通常的做法是将程序复制到每个节点上，并通过 Shell 脚本等方式启动程序。程序的启动方式和参数需要根据具体情况进行设置。</li>
<li>使用任务调度系统：在集群部署环境下，可以使用任务调度系统来管理 OpenMP 程序的运行。常用的任务调度系统包括 Slurm、PBS 和 LSF 等。使用任务调度系统可以更好地管理集群资源，提高程序的执行效率。</li>
</ol>
<p>以上是 OpenMP 在集群部署环境下的一些常用方法和技巧。需要根据具体情况进行选择和调整。</p>
</li>
</ul>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-【转载】LangChain+ChatGLM2-6B搭建知识库" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <div class="article-entry" itemprop="articleBody">
      <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">title: 【转载】LangChain+ChatGLM2-6B搭建知识库</span><br><span class="line">date: 2023-06-26 06:50:32</span><br><span class="line">tags: 大模型, ChatGLM2-6B</span><br></pre></td></tr></table></figure>

<p>本文来自博客园，原文链接：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html">https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html</a></p>
<h2 id="ChatGLM2-6B-介绍"><a href="#ChatGLM2-6B-介绍" class="headerlink" title="ChatGLM2-6B 介绍"></a>ChatGLM2-6B 介绍</h2><p>ChatGLM2-6B 在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了如下新特性：</p>
<ul>
<li>• <strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，全面升级了基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li>
</ul>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632435-1491933983.png" class="" title="图片">

<ul>
<li>• <strong>更长的上下文</strong>：基于 FlashAttention 技术，将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。</li>
<li>• <strong>更高效的推理</strong>：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。</li>
</ul>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632905-1098197173.png" class="" title="图片">

<ul>
<li>• <strong>更开放的协议</strong>：ChatGLM2-6B 权重对学术研究完全开放，在获得官方的书面许可后，亦<strong>允许商业使用</strong>。</li>
</ul>
<p>相比于初代模型，ChatGLM2-6B 多个维度的能力都取得了提升，以下是一些官方对比示例。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632839-1115027413.png" class="" title="图片">

<p>总的来说，看起来效果还不错，下面跟着树先生一起来试试水~</p>
<p>本文我将分 3 步带着大家一起实操一遍，并与之前 ChatGLM-6B 进行对比。</p>
<ul>
<li>• ChatGLM2-6B 部署</li>
<li>• ChatGLM2-6B 微调</li>
<li>• LangChain + ChatGLM2-6B 构建个人专属知识库</li>
</ul>
<h2 id="ChatGLM2-6B-部署"><a href="#ChatGLM2-6B-部署" class="headerlink" title="ChatGLM2-6B 部署"></a>ChatGLM2-6B 部署</h2><p>这里我们还是白嫖阿里云的机器学习 PAI 平台，使用 A10 显卡，这部分内容之前文章中有介绍。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4NDg5OTg1Mg==&mid=2247484248&idx=1&sn=711d8ea75fc825ae7a1ade82b6eb5f2f&scene=21#wechat_redirect">免费部署一个开源大模型 MOSS</a></p>
<p>环境准备好了以后，就可以开始准备部署工作了。</p>
<h3 id="下载源码"><a href="#下载源码" class="headerlink" title="下载源码"></a>下载源码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/THUDM/ChatGLM2-6B</span><br></pre></td></tr></table></figure>

<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ChatGLM2-6B</span><br><span class="line"><span class="comment"># 其中 transformers 库版本推荐为 4.30.2，torch 推荐使用 2.0 及以上的版本，以获得最佳的推理性能</span></span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<h3 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里我将下载的模型文件放到了本地的 chatglm-6b 目录下</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm2-6b <span class="variable">$PWD</span>/chatglm2-6b</span><br></pre></td></tr></table></figure>

<h3 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为前面改了模型默认下载地址，所以这里需要改下路径参数</span></span><br><span class="line"><span class="comment"># 修改 web_demo.py 文件</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想要本地访问，需要修改此处</span></span><br><span class="line">demo.queue().launch(share=<span class="literal">True</span>, inbrowser=<span class="literal">True</span>, server_name=<span class="string">&#x27;0.0.0.0&#x27;</span>, server_port=<span class="number">7860</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Web-模式启动"><a href="#Web-模式启动" class="headerlink" title="Web 模式启动"></a>Web 模式启动</h3><p>官方推荐用 Streamlit 启动会更流程一些，但受限于 PAI 平台没有分配弹性公网，所以还是用老的 gradio 启动吧。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python web_demo.py</span><br></pre></td></tr></table></figure>

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632951-287181832.png" class="" title="图片">img

<h3 id="ChatGLM2-6B-对比-ChatGLM-6B"><a href="#ChatGLM2-6B-对比-ChatGLM-6B" class="headerlink" title="ChatGLM2-6B 对比 ChatGLM-6B"></a>ChatGLM2-6B 对比 ChatGLM-6B</h3><p>先让 ChatGPT 作为考官，出几道题。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632967-1521522527.png" class="" title="图片">

<p>ChatGLM-6B 回答：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632495-642956216.png" class="" title="图片">

<p>ChatGLM2-6B 回答：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633082-1769147658.png" class="" title="图片">

<p>明显可以看出，ChatGLM2-6B 相比于上一代模型响应速度更快，问题回答精确度更高，且拥有更长的（32K）上下文！</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632541-2120443628.png" class="" title="图片">

<h2 id="基于-P-Tuning-微调-ChatGLM2-6B"><a href="#基于-P-Tuning-微调-ChatGLM2-6B" class="headerlink" title="基于 P-Tuning 微调 ChatGLM2-6B"></a>基于 P-Tuning 微调 ChatGLM2-6B</h2><p>ChatGLM2-6B 环境已经有了，接下来开始模型微调，这里我们使用官方的 P-Tuning v2 对 ChatGLM2-6B 模型进行参数微调，P-Tuning v2 将需要微调的参数量减少到原来的 0.1%，再通过模型量化、Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行。</p>
<h3 id="安装依赖-1"><a href="#安装依赖-1" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行微调需要 4.27.1 版本的 transformers</span></span><br><span class="line">pip <span class="keyword">install </span>transformers==<span class="number">4</span>.<span class="number">27</span>.<span class="number">1</span></span><br><span class="line">pip <span class="keyword">install </span>rouge_chinese nltk <span class="keyword">jieba </span>datasets</span><br></pre></td></tr></table></figure>

<h3 id="禁用-W-amp-B"><a href="#禁用-W-amp-B" class="headerlink" title="禁用 W&amp;B"></a>禁用 W&amp;B</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁用 W&amp;B，如果不禁用可能会中断微调训练，以防万一，还是禁了吧</span></span><br><span class="line"><span class="built_in">export</span> WANDB_DISABLED=<span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h3><p>这里为了简化，我只准备了5条测试数据，分别保存为 train.json 和 dev.json，放到 ptuning 目录下，实际使用的时候肯定需要大量的训练数据。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;你好，你是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;你是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;树先生是谁&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;介绍下树先生&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br><span class="line">&#123;&quot;<span class="attribute">content</span>&quot;: <span class="string">&quot;树先生&quot;</span>, <span class="string">&quot;summary&quot;</span>: <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<h3 id="参数调整-1"><a href="#参数调整-1" class="headerlink" title="参数调整"></a>参数调整</h3><p>修改 <code>train.sh</code> 和 <code>evaluate.sh</code> 中的 <code>train_file</code>、<code>validation_file</code>和<code>test_file</code>为你自己的 JSON 格式数据集路径，并将 <code>prompt_column</code> 和 <code>response_column</code> 改为 JSON 文件中输入文本和输出文本对应的 KEY。可能还需要增大 <code>max_source_length</code> 和 <code>max_target_length</code> 来匹配你自己的数据集中的最大输入输出长度。并将模型路径 <code>THUDM/chatglm2-6b</code> 改为你本地的模型路径。</p>
<h4 id="1、train-sh-文件修改"><a href="#1、train-sh-文件修改" class="headerlink" title="1、train.sh 文件修改"></a>1、train.sh 文件修改</h4><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=<span class="number">32</span></span><br><span class="line">LR=<span class="number">2e-2</span></span><br><span class="line">NUM_GPUS=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">torchrun <span class="comment">--standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \</span></span><br><span class="line">    <span class="comment">--do_train \</span></span><br><span class="line">    <span class="comment">--train_file train.json \</span></span><br><span class="line">    <span class="comment">--validation_file dev.json \</span></span><br><span class="line">    <span class="comment">--preprocessing_num_workers 10 \</span></span><br><span class="line">    <span class="comment">--prompt_column content \</span></span><br><span class="line">    <span class="comment">--response_column summary \</span></span><br><span class="line">    <span class="comment">--overwrite_cache \</span></span><br><span class="line">    <span class="comment">--model_name_or_path /mnt/workspace/chatglm2-6b \</span></span><br><span class="line">    <span class="comment">--output_dir output/adgen-chatglm2-6b-pt-$PRE_SEQ_LEN-$LR \</span></span><br><span class="line">    <span class="comment">--overwrite_output_dir \</span></span><br><span class="line">    <span class="comment">--max_source_length 128 \</span></span><br><span class="line">    <span class="comment">--max_target_length 128 \</span></span><br><span class="line">    <span class="comment">--per_device_train_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--per_device_eval_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--gradient_accumulation_steps 16 \</span></span><br><span class="line">    <span class="comment">--predict_with_generate \</span></span><br><span class="line">    <span class="comment">--max_steps 3000 \</span></span><br><span class="line">    <span class="comment">--logging_steps 10 \</span></span><br><span class="line">    <span class="comment">--save_steps 1000 \</span></span><br><span class="line">    <span class="comment">--learning_rate $LR \</span></span><br><span class="line">    <span class="comment">--pre_seq_len $PRE_SEQ_LEN \</span></span><br><span class="line">    <span class="comment">--quantization_bit 4</span></span><br></pre></td></tr></table></figure>

<p><code>train.sh</code> 中的 <code>PRE_SEQ_LEN</code> 和 <code>LR</code> 分别是 soft prompt 长度和训练的学习率，可以进行调节以取得最佳的效果。P-Tuning-v2 方法会冻结全部的模型参数，可通过调整 <code>quantization_bit</code> 来改变原始模型的量化等级，不加此选项则为 FP16 精度加载。</p>
<h4 id="2、evaluate-sh-文件修改"><a href="#2、evaluate-sh-文件修改" class="headerlink" title="2、evaluate.sh 文件修改"></a>2、evaluate.sh 文件修改</h4><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=<span class="number">32</span></span><br><span class="line">CHECKPOINT=adgen-chatglm2<span class="number">-6</span>b-pt<span class="number">-32</span><span class="number">-2e-2</span></span><br><span class="line">STEP=<span class="number">3000</span></span><br><span class="line">NUM_GPUS=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">torchrun <span class="comment">--standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \</span></span><br><span class="line">    <span class="comment">--do_predict \</span></span><br><span class="line">    <span class="comment">--validation_file dev.json \</span></span><br><span class="line">    <span class="comment">--test_file dev.json \</span></span><br><span class="line">    <span class="comment">--overwrite_cache \</span></span><br><span class="line">    <span class="comment">--prompt_column content \</span></span><br><span class="line">    <span class="comment">--response_column summary \</span></span><br><span class="line">    <span class="comment">--model_name_or_path /mnt/workspace/chatglm2-6b \</span></span><br><span class="line">    <span class="comment">--ptuning_checkpoint ./output/$CHECKPOINT/checkpoint-$STEP \</span></span><br><span class="line">    <span class="comment">--output_dir ./output/$CHECKPOINT \</span></span><br><span class="line">    <span class="comment">--overwrite_output_dir \</span></span><br><span class="line">    <span class="comment">--max_source_length 128 \</span></span><br><span class="line">    <span class="comment">--max_target_length 128 \</span></span><br><span class="line">    <span class="comment">--per_device_eval_batch_size 1 \</span></span><br><span class="line">    <span class="comment">--predict_with_generate \</span></span><br><span class="line">    <span class="comment">--pre_seq_len $PRE_SEQ_LEN \</span></span><br><span class="line">    <span class="comment">--quantization_bit 4</span></span><br></pre></td></tr></table></figure>

<p><code>CHECKPOINT</code> 实际就是 <code>train.sh</code> 中的 <code>output_dir</code>。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>train.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<p>5 条数据大概训练了 50 分钟左右。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633066-1570481016.png" class="" title="图片">

<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>evaluate.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633061-1802964012.png" class="" title="图片">

<p>执行完成后，会生成评测文件，评测指标为中文 Rouge score 和 BLEU-4。生成的结果保存在 .&#x2F;output&#x2F;adgen-chatglm2-6b-pt-32-2e-2&#x2F;generated_predictions.txt。我们准备了 5 条推理数据，所以相应的在文件中会有 5 条评测数据，labels 是 dev.json 中的预测输出，predict 是 ChatGLM2-6B 生成的结果，对比预测输出和生成结果，评测模型训练的好坏。如果不满意调整训练的参数再次进行训练。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，我是树先生的助手小6。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;labels&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;predict&quot;</span><span class="punctuation">:</span> <span class="string">&quot;树先生是一个程序员，热衷于用技术探索商业价值，持续努力为粉丝带来价值输出，运营公众号《程序员树先生》。&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="部署微调后的模型"><a href="#部署微调后的模型" class="headerlink" title="部署微调后的模型"></a>部署微调后的模型</h3><p>这里我们先修改 web_demo.sh 的内容以符合实际情况，将 <code>pre_seq_len</code> 改成你训练时的实际值，将 <code>THUDM/chatglm2-6b</code> 改成本地的模型路径。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PRE_SEQ_LEN=32</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 web_demo.py \</span><br><span class="line">    --model_name_or_path /mnt/workspace/chatglm2-6b \</span><br><span class="line">    --ptuning_checkpoint output/adgen-chatglm2-6b-pt-32-2e-2/checkpoint-3000 \</span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span></span><br></pre></td></tr></table></figure>

<p>然后再执行。</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bash </span>web_demo.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<h3 id="结果对比"><a href="#结果对比" class="headerlink" title="结果对比"></a>结果对比</h3><h4 id="原始模型"><a href="#原始模型" class="headerlink" title="原始模型"></a>原始模型</h4><img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633070-939675307.png" class="" title="图片">

<h4 id="微调后模型"><a href="#微调后模型" class="headerlink" title="微调后模型"></a>微调后模型</h4><img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633099-1579471952.png" class="" title="图片">

<h2 id="LangChain-ChatGLM2-6B-构建知识库"><a href="#LangChain-ChatGLM2-6B-构建知识库" class="headerlink" title="LangChain + ChatGLM2-6B 构建知识库"></a>LangChain + ChatGLM2-6B 构建知识库</h2><h3 id="LangChain-知识库技术原理"><a href="#LangChain-知识库技术原理" class="headerlink" title="LangChain 知识库技术原理"></a>LangChain 知识库技术原理</h3><p>目前市面上绝大部分知识库都是 LangChain + LLM + embedding 这一套，实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的<code>top k</code>个 -&gt; 匹配出的文本作为上下文和问题一起添加到 prompt 中 -&gt; 提交给 LLM 生成回答。</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632909-1342948348.png" class="" title="图片">

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632518-1835023463.png" class="" title="图片">

<p>从上面就能看出，其核心技术就是向量 embedding，将用户知识库内容经过 embedding 存入向量知识库，然后用户每一次提问也会经过 embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 promt 提交给 LLM 回答，很好理解吧。一个典型的 prompt 模板如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">已知信息：</span></span><br><span class="line"><span class="string">&#123;context&#125; </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 </span></span><br><span class="line"><span class="string">问题是：&#123;question&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>更多关于向量 embedding 的内容可以参考我之前写的一篇文章。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4NDg5OTg1Mg==&mid=2247484333&idx=1&sn=213a245558ba7b52e5736682a2ec45a9&chksm=cfb06acef8c7e3d8b08099a29d93891d455f4d7c6cc4dd72ea391b23a183bb09bcdb880a2423&scene=21#wechat_redirect">ChatGPT 引爆向量数据库赛道</a></p>
<h3 id="项目部署"><a href="#项目部署" class="headerlink" title="项目部署"></a>项目部署</h3><p><strong>下载源码</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/imClumsyPanda/langchain-ChatGLM.git</span><br></pre></td></tr></table></figure>

<p><strong>安装依赖</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> langchain-ChatGLM</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<p><strong>下载模型</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 git lfs</span></span><br><span class="line">git lfs install</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 LLM 模型</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/THUDM/chatglm2-6b <span class="variable">$PWD</span>/chatglm2-6b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 Embedding 模型</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/GanymedeNil/text2vec-large-chinese <span class="variable">$PWD</span>/text2vec</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型需要更新时，可打开模型所在文件夹后拉取最新模型文件/代码</span></span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>

<p><strong>参数调整</strong></p>
<p>模型下载完成后，请在 <code>configs/model_config.py</code> 文件中，对<code>embedding_model_dict</code>和<code>llm_model_dict</code>参数进行修改。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">embedding_model_dict = &#123;</span><br><span class="line">    <span class="string">&quot;ernie-tiny&quot;</span>: <span class="string">&quot;nghuyong/ernie-3.0-nano-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ernie-base&quot;</span>: <span class="string">&quot;nghuyong/ernie-3.0-base-zh&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec-base&quot;</span>: <span class="string">&quot;shibing624/text2vec-base-chinese&quot;</span>,</span><br><span class="line">    <span class="string">&quot;text2vec&quot;</span>: <span class="string">&quot;/mnt/workspace/text2vec&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-small&quot;</span>: <span class="string">&quot;moka-ai/m3e-small&quot;</span>,</span><br><span class="line">    <span class="string">&quot;m3e-base&quot;</span>: <span class="string">&quot;moka-ai/m3e-base&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">llm_model_dict = &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">&quot;chatglm2-6b&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;chatglm2-6b&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pretrained_model_name&quot;</span>: <span class="string">&quot;/mnt/workspace/chatglm2-6b&quot;</span>,</span><br><span class="line">        <span class="string">&quot;local_model_path&quot;</span>: None,</span><br><span class="line">        <span class="string">&quot;provides&quot;</span>: <span class="string">&quot;ChatGLM&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># LLM 名称改成 chatglm2-6b</span></span><br><span class="line">LLM_MODEL = <span class="string">&quot;chatglm2-6b&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="项目启动"><a href="#项目启动" class="headerlink" title="项目启动"></a>项目启动</h3><p><strong>Web 模式启动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python webui.py</span><br></pre></td></tr></table></figure>

<p>如果报了这个错：</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632966-402914823.png" class="" title="图片">

<p>升级下 protobuf 即可。</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="comment">--upgrade protobuf==3.19.6</span></span><br></pre></td></tr></table></figure>

<p>启动成功！</p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632524-1702966774.png" class="" title="图片">

<p><strong>模型配置</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632543-184630604.png" class="" title="图片">

<p><strong>上传知识库</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632952-239703160.png" class="" title="图片">

<p><strong>基于</strong> <strong>ChatGLM2-6B 的知识库问答</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632941-638928516.png" class="" title="图片">

<h3 id="定制-UI"><a href="#定制-UI" class="headerlink" title="定制 UI"></a><strong>定制</strong> <strong>UI</strong></h3><p>由于 LangChain 项目更新了接口，树先生之前开发的定制 UI 也同步更新进行了适配。</p>
<p><strong>选择知识库</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632525-1874686678.png" class="" title="图片">

<p><strong>基于知识库问答</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632893-1731487740.png" class="" title="图片">

<p><strong>显示答案来源</strong></p>
<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220632499-811642262.png" class="" title="图片">

<img src="/2023/07/16/%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91LangChain+ChatGLM2-6B%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3132769-20230715220633092-2050305602.png" class="" title="图片">

<p>好了，这一篇还挺长的，不过很多内容之前文章中都有提到，相当于是一篇 LangChain + LLM + embedding 构建知识库的<strong>总结篇</strong>了，大家收藏好这一篇就行了~</p>
<p>本文来自博客园，原文链接：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html">https://www.cnblogs.com/botai/p/LangChain_ChatGLM2-6B.html</a></p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-如何利用ChatGPT赚钱？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/07/04/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8ChatGPT%E8%B5%9A%E9%92%B1%EF%BC%9F/">如何利用ChatGPT赚钱？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-07-04T15:26:48.000Z" itemprop="datePublished">
  2023-07-04
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>您是否正在寻找无需付出太多努力即可产生被动收入的方法？由于人工智能和聊天机器人的进步，现在可以使用这些技术赚钱。在这篇博文中，我们将探讨非技术人员通过 ChatGPT 产生被动收入的一些最有效的方法。</p>
<p>ChatGPT 被誉为世界上最智能的生成人工智能，它正在改变在线赚钱的游戏规则。有了这个革命性的免费工具，您无需任何技能，无需资金，每天即可赚取 1000 美元或更多。</p>
<p>这是一个令人兴奋的人工智能新时代，现在是参与并利用这个机会的最佳时机。人们使用 ChatGPT 来观看 Youtube、写博客、自由职业以及许多其他赚钱方式。因此，让我们深入了解如何使用 ChatGPT 赚钱并产生各种被动收入。</p>
<p>可是等等！我们有一些您不想错过的东西：了解更多信息的绝佳机会。加入我们，踏上一段令人难以置信的知识和成长之旅。我们呼吁所有数据科学和人工智能爱好者参加2023 年 DataHack 峰会，该峰会将于 8 月 2 日至 5 日在班加罗尔的 NIMHANS 会议中心举行。通过实践学习、行业见解和交流机会体验激动人心的活动。不要错过这个令人难以置信的数据驱动思想的聚会！</p>
<p>目录<br>如何通过 ChatGPT 赚钱<br>使用 ChatGPT 获取一些商业创意<br>使用 ChatGPT 进行自由职业<br>使用 ChatGPT 写博客<br>内容创作<br>关键词研究和SEO优化<br>观众参与度<br>使用 ChatGPT 进行电子邮件联盟营销<br>将 ChatGPT 用于 Youtube 频道<br>使用 ChatGPT 编写电子书并自行出版<br>结论<br>如何通过 ChatGPT 赚钱<br>您可以通过多种方式使用 ChatGPT 赚钱。即使您对人工智能一无所知，请继续阅读，发现 6 种赚钱方法。</p>
<p>从 ChatGPT 获取商业创意<br>自由职业者<br>写博客<br>电子邮件营销<br>使用 ChatGPT 创建视频<br>撰写电子书并自行出版<br>让我们深入研究一下</p>
<p>使用 ChatGPT 获取一些商业创意<br>ChatGPT 是一款人工智能驱动的工具，可以帮助您产生被动收入。然而，为了最大限度地发挥其潜力，有效地利用它至关重要。</p>
<p>成为全栈数据科学家<br>转变为专家并对数据科学世界产生重大影响。<br>如果您不熟悉它的工作原理，没问题！只需询问适合您独特喜好的定制副业想法即可。ChatGPT 将通过了解您的兴趣、才能和障碍来发挥其魔力，然后生成符合您期望的定制业务概念。准备好开始不费吹灰之力就能赚到钱了！”</p>
<p>让我们向 ChatGPT 询问一位拥有数字营销和销售经验的数据分析师的一些商业想法。</p>
<p>使用 chatgpt 为自己创造业务<br>收到这些量身定制的想法后，您可以与 ChatGPT 进一步讨论，以概念化创业计划、考虑重要因素等。或者，您可以首先声明“为……生成一个新的商业想法”，并允许 ChatGPT 提供一些出色的建议。</p>
<p>使用chatgpt生成商业计划<br>通过充分利用 ChatGPT，您可以释放大量创收机会。</p>
<p>使用 ChatGPT 进行自由职业<br>使用 ChatGPT 将您的自由职业游戏提升到一个新的水平！这种最先进的人工智能工具使专业人士能够赚取额外收入并制作出令客户惊叹的高质量内容。公司甚至为那些利用这项技术创造出经过精心研究的精美作品的人提供奖励。</p>
<p>您可以使用 ChatGPT 提供的一些自由职业服务包括：</p>
<p>使用 ChatGPT 编写博客或网站内容<br>使用 CHATGPT 翻译任何语言<br>ChatGPT 的电子邮件写作服务<br>使用 ChatGPT 撰写标题和号召性用语<br>使用 ChatGPT 为 Youtube 视频编写脚本<br>为帖子或营销编写社交媒体内容<br>使用 ChatGPT 写一个短篇故事<br>使用 ChatGPT 进行主题标签研究<br>使用 ChatGPT 写博客<br>您可以通过多种方式利用 ChatGPT 的优势：</p>
<p>内容创作<br>这个人工智能驱动的工具可以帮助博主生成想法、大纲，甚至完成博客文章的草稿。当您感到陷入困境时，它非常适合。借助 ChatGPT，您可以节省数小时的研究时间，并在几秒钟内访问相关信息、统计数据和事实。准备好使用 ChatGPT 将您的博客游戏提升到新的水平。</p>
<p>使用 chatGPT 获取内容创意和大纲<br>编辑和校对<br>ChatGPT 被证明是博客作者的宝贵资产，它可以提供编辑、校对方面的帮助，并通过拼写、语法和理解方面的建议和更正来增强博客文章的整体可读性，从而帮助减少错误并确保清晰度。</p>
<p>关键词研究和SEO优化<br>搜索引擎优化 (SEO) 是博客的一个重要方面，ChatGPT 可以帮助您针对搜索引擎优化博客文章。它可以建议您的博客文章中包含的关键字，提供有关如何构建内容以获得最大可见度的指导，并提供有关如何提高整体在线形象的提示。</p>
<p>使用 ChatGPT 进行关键字研究和 SEO<br>单击此处了解有关 ChatGPT 如何帮助 Seo 优化的更多信息</p>
<p>观众参与度<br>ChatGPT 可以通过提供对话开始、回答常见问题和解决读者的疑虑来帮助博主与受众互动。这使得博主能够与受众进行更深层次的联系并建立忠实的追随者。</p>
<p>通过利用 ChatGPT 的功能，博主可以持续创建高质量的内容，吸引目标受众，提高整体在线形象并产生被动收入。</p>
<p>使用 ChatGPT 进行电子邮件联盟营销<br>使用 ChatGPT 赚钱的最简单方法之一是通过电子邮件联属营销。该聊天机器人拥有出色的写作技巧，可以起草令人信服的电子邮件，激励用户点击链接并进行购买或订阅服务。</p>
<p>要启动电子邮件联属营销活动，请选择一个联属计划，例如 Amazon、Shopify、ConvertKit 等。选择联属计划后，请创建一个电子邮件列表，针对对您所推广的产品或服务感兴趣的潜在客户。要构建此电子邮件列表，您可以使用铅磁铁或鼓励使用其他方法进行电子邮件注册。</p>
<p>在 ChatGPT 的帮助下，您可以制作引人入胜的电子邮件，不仅让用户了解产品的优点，还可以激励他们点击您的链接并进行购买。</p>
<p>使用 ChatGPT 进行电子邮件营销<br>使用 ChatGPT 进行联盟营销<br>通过电子邮件联属网络营销，您可以为通过推荐链接进行的每笔销售赚取佣金 - 所有这些都只需您付出最少的努力。那为什么还要等呢？今天就让 ChatGPT 帮助您启动可盈利的电子邮件活动！</p>
<p>将 ChatGPT 用于 Youtube 频道<br>借助 ChatGPT，您可以就您选择的类别中的视频想法进行集体讨论 - 如果您感到困惑，ChatGPT 甚至可以为您编写脚本。</p>
<p>让我们向 ChatGPT 询问一些有关时尚视频博主的 Youtube 视频创意。</p>
<p>使用 ChatGPT 获取 Youtube 视频创意。<br>现在，为您的 YouTube 视频生成脚本。</p>
<p>使用 ChatGPT 为您的 Youtube 频道或视频编写脚本<br>一旦您有了脚本，就可以使用Pictory.ai或invideo.io轻松将其转换为具有专业外观的视频。这些人工智能驱动的平台可以快速将您的文本转换为可在 YouTube 上发布的带旁白的视频。谁知道呢？凭借一些营销头脑，您可以开始兼职赚钱！</p>
<p>那为什么还要等呢？今天就让 ChatGPT 帮助您发现新的、令人兴奋的内容创意，并将您的 YouTube 频道提升到一个新的水平！</p>
<p>使用 ChatGPT 编写电子书并自行出版<br>您是否对写作充满热情，但很难想出新鲜和创新的想法？嗯，这就是 ChatGPT 的用武之地！据路透社最近报道，由于 ChatGPT 的推出，人工智能编写的电子书在亚马逊上出现了令人难以置信的增长。</p>
<p>从充满冒险故事的儿童读物到激励读者实现梦想的励志讲座，甚至是将读者带入新世界的惊心动魄的科幻小说 - 在 ChatGPT 的指导下，您可以实现无限的成就。</p>
<p>最好的部分是什么？您不需要成为文学天才才能在电子书市场上掀起波澜。借助 Kindle Direct Publishing，将您的图书发布到热切的读者手中从未如此简单。此外，使用 ChatGPT 编写电子书意味着您将节省时间和精力，让您能够更加专注于营销和推广您的工作。</p>
<p>因此，无论您是一位想要尝试新事物的经验丰富的作家，还是一位一直想出版自己的书的新手，通过 ChatGPT 自行出版都是赚取额外现金的合法且令人兴奋的方式。</p>
<p>结论<br>如果您正在寻找一种无需付出太多努力就能赚钱的方法，那么 ChatGPT 就是您的最佳选择。这种人工智能驱动的工具正在改变在线赚钱的游戏规则。借助 ChatGPT，您只需很少的技能且无需资金即可产生被动收入。</p>
<p>在这篇博文中，我们探讨了非技术人员使用 ChatGPT 赚钱的一些最有效方法。从获得商业创意到自由职业、博客、电子邮件营销、制作视频和编写电子书，使用这个革命性的工具有无限的赚钱可能性。</p>
<p>那为什么还要等呢？立即开始使用 ChatGPT，发现新的、令人兴奋的方式来产生被动收入。凭借其能力，可能性是无限的！</p>
<p>要了解有关生成模型开发和实践经验的更多信息，请加入我们在2023 年 DataHack 峰会上举办的“使用生成模型进行自然语言处理”研讨会。参加 2023 年 DataHack 峰会将改变您的游戏规则。这些研讨会旨在提供巨大的价值，为您提供实用技能和现实知识。凭借实践经验，您将有信心正面应对数据挑战。不要错过这个宝贵的机会，可以增强您的专业知识、与行业领导者建立联系并释放新的职业机会。立即注册参加 2023 年 DataHack 峰会！</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-什么是Dreambooth？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/">什么是Dreambooth？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-07-04T13:26:35.000Z" itemprop="datePublished">
  2023-07-04
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p><a target="_blank" rel="noopener" href="https://dreambooth.github.io/">Dreambooth</a>由 Google 研究团队于 2022 年发布，是一种通过向模型注入自定义主题来微调扩散模型（如Stable Diffusion）的技术。</p>
<p>为什么叫Dreambooth？据谷歌研究团队称，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">It’s like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you.</span><br></pre></td></tr></table></figure>

<p>DreamBooth 算法对 Imagen 模型进行了微调，从而实现了将现实物体在图像中真实还原的功能，通过少量实体物品图像的 fine-turning，使得原有的 SD 模型能对图像实体记忆保真，识别文本中该实体在原图像中的主体特征甚至主题风格，是一种新的文本到图像“个性化”（可适应用户特定的图像生成需求）扩散模型。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/stable-diffusion-quick-kit-series-model-fine-tuning-with-dreambooth-optimization-practices-on-sagemaker2.png" class="" title="img">

<p>听起来很棒！但它的效果如何？下面是研究文章中的一个例子。仅使用特定狗（我们称之为<strong>Devora</strong>）的 3 张图像作为输入，dreamboothed 模型就可以在不同的环境中生成 Devora 的图像。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/image-13.png" class="" title="dreambooth 研究文章中的 dreambooth 示例">

<p>只需 3 个训练图像，Dreambooth 即可将自定义主题无缝注入扩散模型。</p>
<h3 id="Dreambooth-如何运作？"><a href="#Dreambooth-如何运作？" class="headerlink" title="Dreambooth 如何运作？"></a>Dreambooth 如何运作？</h3><p>Dreambooth 是对整个神经网络所有层权重进行调整，会将输入的图像训练进 Stable Diffusion 模型，它的本质是先复制了源模型，在源模型的基础上做了微调（fine tunning）并独立形成了一个新模型，在它的基本上可以做任何事情。缺点是，训练它需要大量 VRAM, 目前经过调优后可以在 16GB 显存下完成训练。</p>
<p>您可能会问，为什么不能简单地使用这些图像通过额外的步骤来训练模型？问题是，这样做会因<em>过度拟合</em>（因为数据集非常小）导致失败。</p>
<p>Dreambooth 通过以下方式解决了这些问题：</p>
<ol>
<li>对新主题使用一个<strong>罕见的单词（请注意，我为狗使用了一个罕见的名字****Devora</strong>），这样它一开始在模型中就没有太多意义。</li>
<li><strong>类的预先保留</strong>：为了保留<strong>类</strong>（上例中的狗）的含义，模型以注入主体（Devora）的方式进行微调，同时生成类（狗）的图像。保存下来。</li>
</ol>
<p>还有另一种类似的技术称为<a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"><a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"> textual inversion</a>. </a>。不同之处在于，Dreambooth 对整个模型进行了微调，而<a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"> textual inversion</a>.则注入了一个新词，而不是重复使用生僻词，并且仅对模型的文本嵌入部分进行了微调。</p>
<h3 id="训练-Dreambooth-需要什么？"><a href="#训练-Dreambooth-需要什么？" class="headerlink" title="训练 Dreambooth 需要什么？"></a>训练 Dreambooth 需要什么？</h3><p>你需要三样东西</p>
<ol>
<li>一些自定义图像</li>
<li>唯一标识符</li>
<li>一个类名class</li>
</ol>
<p>在上面的例子中。唯一标识符是<strong>Devora</strong>，class名称是<strong>狗</strong>。</p>
<p>然后你需要构建你的<strong>实例提示</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of [unique identifier] [class name]</span><br></pre></td></tr></table></figure>

<p>还有<strong>class提示</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of [class name]</span><br></pre></td></tr></table></figure>

<p>在上面的例子中，<strong>实例提示符</strong>是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of Devora dog</span><br></pre></td></tr></table></figure>

<p>由于 Devora 是一只狗，所以<strong>class提示</strong>是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of a dog</span><br></pre></td></tr></table></figure>

<h3 id="获取训练图像"><a href="#获取训练图像" class="headerlink" title="获取训练图像"></a>获取训练图像</h3><p>为您的自定义主题拍摄 3-10 张照片。照片应该从不同的角度拍摄。</p>
<p>拍摄对象还应该处于多种背景中，以便模型可以将拍摄对象与背景区分开来。</p>
<p>我将在教程中使用这个玩具。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/0E5AFC83-B759-4FE9-8E16-A60774E1DEDF_1_105_c.jpeg" class="" title="img">

<h3 id="调整图像大小"><a href="#调整图像大小" class="headerlink" title="调整图像大小"></a>调整图像大小</h3><p>为了在训练中使用图像，您首先需要将它们的大小调整为 512×512 像素，以便使用 v1 模型进行训练。</p>
<p><a target="_blank" rel="noopener" href="https://www.birme.net/?target_width=512&target_height=512">BIRME</a>是一个调整图像大小的便捷网站。</p>
<ol>
<li>将您的图像拖放到 BIRME 页面。</li>
<li>调整每张图像的画布，使其充分显示主题。</li>
<li>确保宽度和高度均为 512 像素。</li>
<li>按<strong>“</strong>SAVE FILES** ”**将调整大小的图像保存到您的计算机。</li>
</ol>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/image-14.png" class="" title="img">

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>目前业界对 DreamBooth 做 fine tuning 主要为两种方式，一是在 Stable Diffusion WebUI 可视话界面进行模型的选择，训练图片的上载及本地化的训练；二是在第三方 IDE 平台如 colab notebook 上以脚本交互式开发的方式进行训练。</p>
<p>第一种方式只能在部署 Stable Diffusion WebUI 应用的单一服务器或主机上训练，无法与企业及客户的后台平台及业务集成；而第二种方式侧重于算法工程师个人在开发测试阶段进行模型实验探索，无法实现生产化工程化的部署。此外，以上两种方式训练 dreambooth，还需要关注高性能算力机资源的成本（尤其对模型效果要求较高的场景，需要多达 50 张以上的 class images，显存容易 OOM），基础模型和 fine tuning 后模型的存储和管理，训练超参的管理，统一的日志监控，训练加速，依赖 lib 编译打包等具体实施落地层面的一系列困难和挑战。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time_start = time.time()</span><br><span class="line"><span class="comment">#@title DreamBooth</span></span><br><span class="line">HUGGINGFACE_TOKEN = <span class="string">&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Name/Path of the initial model. (Find model name [here](https://huggingface.co/models))</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">BRANCH = <span class="string">&quot;fp16&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Enter instance prompt and class prompt.\</span></span><br><span class="line"><span class="comment">#@markdown Example 1: photo of zwx person, photo of a person\</span></span><br><span class="line"><span class="comment">#@markdown Example 2: photo of zwx toy, photo of a toy</span></span><br><span class="line">instance_prompt = <span class="string">&quot;photo of zwx toy&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">class_prompt =  <span class="string">&quot;photo of a toy&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">training_steps = <span class="number">800</span> <span class="comment">#@param &#123;type:&quot;integer&quot;&#125;</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span> <span class="comment">#@param &#123;type:&quot;number&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown  Convert to fp16? (takes half the space (2GB)).</span></span><br><span class="line">fp16 = <span class="literal">True</span> <span class="comment">#@param &#123;type: &quot;boolean&quot;&#125;</span></span><br><span class="line"><span class="comment">#@markdown  Compile xformers (Try only if you see xformers error. Will take 1 more hour).</span></span><br><span class="line">complie_xformers = <span class="literal">False</span> <span class="comment">#@param &#123;type: &quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line">save_to_gdrive = <span class="literal">True</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line"><span class="keyword">if</span> save_to_gdrive:</span><br><span class="line">  drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Clear log after run?</span></span><br><span class="line">CLEAR_LOG = <span class="literal">False</span> <span class="comment">#@param &#123;type:&quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OUTPUT_DIR = <span class="string">&quot;stable_diffusion_weights/output&quot;</span> </span><br><span class="line">OUTPUT_DIR = <span class="string">&quot;/content/&quot;</span> + OUTPUT_DIR</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check type of GPU and VRAM available.</span></span><br><span class="line">!nvidia-smi --query-gpu=name,memory.total,memory.free --<span class="built_in">format</span>=csv,noheader</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] Weights will be saved at <span class="subst">&#123;OUTPUT_DIR&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">!mkdir -p $OUTPUT_DIR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.</span></span><br><span class="line"></span><br><span class="line">concepts_list = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;instance_prompt&quot;</span>:      instance_prompt,</span><br><span class="line">        <span class="string">&quot;class_prompt&quot;</span>:         class_prompt,</span><br><span class="line">        <span class="string">&quot;instance_data_dir&quot;</span>:    <span class="string">&quot;/content/data/instance&quot;</span>,</span><br><span class="line">        <span class="string">&quot;class_data_dir&quot;</span>:       <span class="string">&quot;/content/data/class&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># `class_data_dir` contains regularization images</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> concepts_list:</span><br><span class="line">    os.makedirs(c[<span class="string">&quot;instance_data_dir&quot;</span>], exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;concepts_list.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(concepts_list, f, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> concepts_list:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Uploading instance images for `<span class="subst">&#123;c[<span class="string">&#x27;instance_prompt&#x27;</span>]&#125;</span>`&quot;</span>)</span><br><span class="line">    uploaded = files.upload()</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> uploaded.keys():</span><br><span class="line">        dst_path = os.path.join(c[<span class="string">&#x27;instance_data_dir&#x27;</span>], filename)</span><br><span class="line">        shutil.move(filename, dst_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clear</span>():</span><br><span class="line">    <span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output; <span class="keyword">return</span> clear_output()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># huggingface token</span></span><br><span class="line">!mkdir -p ~/.huggingface</span><br><span class="line">!echo -n <span class="string">&quot;&#123;HUGGINGFACE_TOKEN&#125;&quot;</span> &gt; ~/.huggingface/token</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># install repos</span></span><br><span class="line">!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py</span><br><span class="line">!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py</span><br><span class="line"><span class="comment">#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span></span><br><span class="line">%pip install -qq git+https://github.com/ShivamShrirao/diffusers</span><br><span class="line">%pip install -q -U --pre triton</span><br><span class="line">%pip install -q accelerate==<span class="number">0.19</span><span class="number">.0</span> transformers ftfy bitsandbytes==<span class="number">0.35</span><span class="number">.0</span> gradio natsort safetensors xformers</span><br><span class="line"><span class="comment"># install xformer wheel</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Install xformers&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> complie_xformers:</span><br><span class="line">  %pip install git+https://github.com/facebookresearch/xformers@4c06c79<span class="comment">#egg=xformers</span></span><br><span class="line"><span class="comment">#else:</span></span><br><span class="line"><span class="comment">#  %pip install  --no-deps -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"><span class="comment">#%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/4c06c79_various6/xformers-0.0.15.dev0_4c06c79.d20221201-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"><span class="comment">#%pip install -q https://github.com/ShivamShrirao/xformers-wheels/releases/download/4c06c79/xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############## Edit this section to customize parameters</span></span><br><span class="line">!python3 train_dreambooth.py \</span><br><span class="line">  --pretrained_model_name_or_path=$MODEL_NAME \</span><br><span class="line">  --pretrained_vae_name_or_path=<span class="string">&quot;stabilityai/sd-vae-ft-mse&quot;</span> \</span><br><span class="line">  --output_dir=$OUTPUT_DIR \</span><br><span class="line">  --revision=$BRANCH \</span><br><span class="line">  --with_prior_preservation --prior_loss_weight=<span class="number">1.0</span> \</span><br><span class="line">  --seed=<span class="number">1337</span> \</span><br><span class="line">  --resolution=<span class="number">512</span> \</span><br><span class="line">  --train_batch_size=<span class="number">1</span> \</span><br><span class="line">  --train_text_encoder \</span><br><span class="line">  --mixed_precision=<span class="string">&quot;fp16&quot;</span> \</span><br><span class="line">  --use_8bit_adam \</span><br><span class="line">  --gradient_accumulation_steps=<span class="number">1</span> \</span><br><span class="line">  --learning_rate=$learning_rate \</span><br><span class="line">  --lr_scheduler=<span class="string">&quot;constant&quot;</span> \</span><br><span class="line">  --lr_warmup_steps=<span class="number">0</span> \</span><br><span class="line">  --num_class_images=<span class="number">50</span> \</span><br><span class="line">  --sample_batch_size=<span class="number">4</span> \</span><br><span class="line">  --max_train_steps=$training_steps \</span><br><span class="line">  --save_interval=<span class="number">10000</span> \</span><br><span class="line">  --save_sample_prompt=<span class="string">&quot;$instance_prompt&quot;</span> \</span><br><span class="line">  --concepts_list=<span class="string">&quot;concepts_list.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.</span></span><br><span class="line"><span class="comment"># `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory).</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> natsort <span class="keyword">import</span> natsorted</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">weightdirs = natsorted(glob(OUTPUT_DIR + os.sep + <span class="string">&quot;*&quot;</span>))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(weightdirs) == <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">raise</span> KeyboardInterrupt(<span class="string">&quot;No training weights directory found&quot;</span>)</span><br><span class="line">WEIGHTS_DIR = weightdirs[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ckpt_path = WEIGHTS_DIR + <span class="string">&quot;/model.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line">half_arg = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> fp16:</span><br><span class="line">    half_arg = <span class="string">&quot;--half&quot;</span></span><br><span class="line">!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] Converted ckpt saved at <span class="subst">&#123;ckpt_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CLEAR_LOG:</span><br><span class="line">  clear()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] WEIGHTS_DIR=<span class="subst">&#123;WEIGHTS_DIR&#125;</span>&quot;</span>)</span><br><span class="line">minutes = (time.time()-time_start)/<span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dreambooth completed successfully. It took %1.1f minutes.&quot;</span>%minutes)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"></span><br><span class="line">weights_folder = OUTPUT_DIR</span><br><span class="line">folders = <span class="built_in">sorted</span>([f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(weights_folder) <span class="keyword">if</span> f != <span class="string">&quot;0&quot;</span>], key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x))</span><br><span class="line"></span><br><span class="line">row = <span class="built_in">len</span>(folders)</span><br><span class="line">col = <span class="built_in">len</span>(os.listdir(os.path.join(weights_folder, folders[<span class="number">0</span>], <span class="string">&quot;samples&quot;</span>)))</span><br><span class="line">scale = <span class="number">4</span></span><br><span class="line">fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw=&#123;<span class="string">&#x27;hspace&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;wspace&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, folder <span class="keyword">in</span> <span class="built_in">enumerate</span>(folders):</span><br><span class="line">    folder_path = os.path.join(weights_folder, folder)</span><br><span class="line">    image_folder = os.path.join(folder_path, <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    images = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(image_folder)]</span><br><span class="line">    <span class="keyword">for</span> j, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="keyword">if</span> row == <span class="number">1</span>:</span><br><span class="line">            currAxes = axes[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            currAxes = axes[i, j]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            currAxes.set_title(<span class="string">f&quot;Image <span class="subst">&#123;j&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            currAxes.text(-<span class="number">0.1</span>, <span class="number">0.5</span>, folder, rotation=<span class="number">0</span>, va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, transform=currAxes.transAxes)</span><br><span class="line">        image_path = os.path.join(image_folder, image)</span><br><span class="line">        img = mpimg.imread(image_path)</span><br><span class="line">        currAxes.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">        currAxes.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">&#x27;grid.png&#x27;</span>, dpi=<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> save_to_gdrive:</span><br><span class="line">  <span class="keyword">import</span> os.path</span><br><span class="line">  gPath = <span class="string">&quot;/content/drive/MyDrive/Dreambooth_model&quot;</span></span><br><span class="line">  !mkdir -p $gPath</span><br><span class="line">  filename = <span class="string">&#x27;model.ckpt&#x27;</span></span><br><span class="line">  i = <span class="number">1</span></span><br><span class="line">  ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">  <span class="keyword">while</span> os.path.isfile(ckpt_gpath):</span><br><span class="line">    filename = <span class="string">&#x27;model%d.ckpt&#x27;</span>%i</span><br><span class="line">    ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">  !cp $ckpt_path $ckpt_gpath</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Model saved to %s&#x27;</span>%ckpt_gpath)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>完成训练大约需要30分钟。完成后模型可以放入AUTOMATIC1111 GUI ，就可以用新模型生成的一些示例图像。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-GPU虚拟化" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/27/GPU%E8%99%9A%E6%8B%9F%E5%8C%96/">GPU虚拟化</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-27T11:53:18.000Z" itemprop="datePublished">
  2023-06-27
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>GPU 资源池化技术从初期的简单虚拟化，到资源池化，经历了四个技术演进阶段。</p>
<p><strong>简单虚拟化</strong></p>
<p>将物理 GPU 按照 2 的 N 次方，切分成多个固定大小的 vGPU（VirtualGPU，虚拟 GPU），每个 vGPU 的算力和显存相等。实践证明，不同的 AI 模型对于算力、显存资源的需求是不同的。所以，这样的切分方式，并不能满足 AI 模型多样化的需求。</p>
<p><strong>任意虚拟化</strong></p>
<p>将物理 GPU 按照算力和显存两个维度，自定义切分，获得满足 AI 应用个性化需求的 vGPU。</p>
<p><strong>远程调用</strong></p>
<p>AI 应用与物理 GPU 服务器分离部署，允许通过高性能网络远程调用 GPU资源。这样可以实现 AI 应用与物理 GPU 资源剥离，AI 应用可以部署在私有云的任</p>
<p>意位置，只需要网络可达，即可调用 GPU 资源。</p>
<p><strong>资源池化</strong></p>
<p>形成 GPU 资源池后，需要统一的管理面来实现管理、监控、资源调度和资源回收等功能。同时，也需要提供北向 API，与数据中心级的资源调度平台对</p>
<p>接，让用户在单一界面，就可以调度包括 vGPU 在内的数据中心内的各类资源。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-如何使用-ChatGPT-和-LangChain-框架构建自己的QA应用" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/">如何使用 ChatGPT 和 LangChain 框架构建自己的QA应用</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:50:32.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>大型语言模型 (LLM) 正在成为一种变革性技术，使开发人员能够构建他们以前无法构建的应用程序。但是单独使用这些 LLM 往往不足以创建一个真正强大的应用程序——当您可以将它们与其他计算或知识来源相结合时，真正的力量就来了。</p>
<p>LLM 本质上是非常通用的，这意味着虽然它们可以有效地执行许多通用的任务，比如回答如何制作一道美味的红烧肉。但它们通常不能直接为特定领域的问题或任务提供具体答案。</p>
<p><strong>这里就提出了一个问题：如何基于ChatGPT为自己的业务赋能。</strong></p>
<p>虽然官方提供了微调服务，但是由于缺乏最佳实践作为参考，加上费用不小，对于很多没有专门的算法人员的企业来说，显然微调不是一个好选择。</p>
<p>幸运的是，LangChain出现了，它是包含一个称为数据增强生成的功能，它允许您提供一些上下文数据来增强 LLM 的知识；</p>
<p>今天就给大家介绍下如何基于自己文本资料，构建基于文档的问答Demo。</p>
<p>代码的结构可以分为3个主要部分：</p>
<ol>
<li><p>1.加载自己的txt文件(里面是自己业务领域的东西)</p>
</li>
<li><p>2.创建embedding和向量化</p>
</li>
<li><p>3.查询txt</p>
</li>
</ol>
<p>现在，让我们深入了解这些步骤中的每一个！</p>
<p>运行前提条件：</p>
<p>需要有个OpenAI api_key用于程序跟ChatGPT做身份验证，这个需要去OpenAI 官方注册，如果你不会搞，可以留言说明。</p>
<p>python安装依赖：我用的python 3.11.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain llama_index openai chromadb</span><br></pre></td></tr></table></figure>

<p>然后我们需要在终端中设置环境变量，设置open api。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export OPENAI_API_KEY=&quot;...&quot;</span><br></pre></td></tr></table></figure>

<p>或者，您可以从 Jupyter notebook（或 Python 脚本）中执行此操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import osos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;...&quot;</span><br></pre></td></tr></table></figure>

<p>首先准备一个content.txt文档，文件里存入一些数据，内容摘录：</p>
<img src="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/640.png" class="" title="图片">

<p>现在创建一个程序来完成问答：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="comment"># 加载放了QA的txt文件</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;content.txt&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"><span class="comment"># 把大段文字切成小块</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">200</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(documents)</span><br><span class="line"><span class="comment"># 创建embeddings</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">docsearch = Chroma.from_documents(texts, embeddings)</span><br><span class="line"><span class="comment"># 通过这个OpenAIEmbedding api给每个小文档计算embedding，存到doc_search</span></span><br><span class="line"><span class="comment"># 根据查询输入找到相似度最高的块作为上下文</span></span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="string">&quot;stuff&quot;</span>, retriever=docsearch.as_retriever())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">q</span>):</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f&quot;Query: <span class="subst">&#123;q&#125;</span>&quot;</span>)</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f&quot;Answer: <span class="subst">&#123;qa.run(q)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">     <span class="built_in">print</span>(query(<span class="string">&quot;如何开会&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>我们看下结果, 返回结果就是我原始文档中数据，很强大有没有!而且代码也很简单。</p>
<img src="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/640-1687733479465-1.png" class="" title="图片">

<p>借助这个小案例，其实也就是抛砖引玉，这里关键就是引入了LangChain这个框架，把ChatGPT和LangChain结合那就是如虎填翼。有了这样一个基础，你想想只要能文本化的东西都能创建这样一个知识提取的应用，结合特定业务那就有非常多的应用空间。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-LoRA是什么？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/">LoRA是什么？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:48:58.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p><strong>1. LoRA介绍</strong></p>
<p>LoRA（ Low-rank Adaptation）是微软研究员引入的一项新技术，主要用于处理大模型微调的问题，本文主要介绍Lora如何微调Stable Diffusion。除此之外微调Stable Diffusion还有DreamBooth（DR）和Textual Inversion（TI）等训练技术。</p>
<p>LoRA有什么厉害之处？实际上LoRA在文件大小和训练能力之间提供了一个很好的平衡。DR虽然功能强大，但是生成的模型文件很大（2-7G）。TI生成的文件虽小（100k），但是训练效果不怎么好。</p>
<p>LoRA介于两者之间，模型（2-200M）文件大小可控，训练能力不错。玩过Stable Diffusion的人都知道，要试验各种模型，前提是你要有足够的磁盘空间，一般一个模型都是好几G的。下载那么大的模型，你的网络带宽也得非常OK（至少每秒几M），不然真的没耐心。所以这也是LoRA比较流行的原因之一吧。与TI相同，LoRA模型不能单独使用，必须与训练的基础模型配合一起使用。</p>
<p><strong>2. LoRA是如何工作的？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先我们来看看Stable Diffusion的架构，主要由三部分组成：文本编码器，可将您的提示词转换为latent向量。一种扩散模型，它反复对 64x64 latent patch进行&quot;去噪&quot;。解码器，将最终的 64x64 latent patch转换为更高分辨率的 512x512 图像。它的一个处理过程就是下图所示，上面每一个组成部分都是一个模型，然后通过PIPLINE把几个模型串联起来，就可以达到生成图片的效果。而且每个部分实际上都是可以独立部署的。</span><br></pre></td></tr></table></figure>



<p>最近总是听到LoRA，LoRA，名字怪好听，就是难理解，什么低秩适应完全搞不懂，今天就来扒扒，看看它是怎么个低法。</p>
<p>原来LoRA对Stable Diffusion模型最关键的部分进行了微小修改：cross-attention layers，它是模型中图像和提示相交的部分-交叉注意层（U-Net 噪声预测器的 QKV 部分）。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640.png" class="" title="图片">



<p>研究人员发现，通过聚焦大模型的 Transformer 注意力块，使用 LoRA 进行的微调质量与全模型微调相当，同时速度更快且需要更少的计算。</p>
<p>Simo Ryu (<code>@cloneofsimo</code>) 是第一个提出适用于 Stable Diffusion 的 LoRA实现的人。如果想查看相关示例和许多其他有趣的讨论和见解。请一定要看看他们的GitHub 项目。</p>
<p>cross-attention 注意力交叉层的权重排列在矩阵中。矩阵是一堆按行和列排列的数字<br>就像在Excle里面一样。LoRA通过将其权重添加到这些矩阵来微调模型。</p>
<p>假设模型是包含1000行和2000列的矩阵。那模型需要存2,000,000 个数字 (1,000 x 2,000)。</p>
<p>LoRA将矩阵分解为：1,000×2 矩阵和 2×2,000 矩阵。那模型只需要存 6,000 个数字 (1,000 x 2 + 2 x 2,000)，少了333倍，这就是LoRA文件小得多的原因。</p>
<p>我们看下作者在github中的介绍：</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-1.png" class="" title="图片">



<p>意思就是说：完全微调过程慢，在质量和训练速度很难找到平衡，虽然也有像TI这样的方法，但是效果不理想。LoRA的出现解决了社区遇到的问题：就是模型太大，用户想要基于社区各种模型进行微调，因为模型太大而无法使用，LoRA尝试微调模型的残差，而不是整个模型：也就训练delta W代替W。而delta W进一步分解成A矩阵和B的转置矩阵相乘。然后微调A和B替代W，A和B比原始的W要小多。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-2.jpeg" class="" title="图片">


<p>LoRA 将一个大矩阵分解为两个小的低阶矩阵，在这个例子中，矩阵的秩为2。它比原始维度低很多，所以被称之为低秩矩阵。</p>
<p>使用 LoRA 对插图数据集进行微调：W&#x3D;W0 + aΔW， a是合并比率。ΔW说就是上面说的矩阵A矩阵和B矩阵的转置乘积，gif是将 alpha从0缩放到1。将alpha设置为0与使用原始模型相同，将alpha设置为1与使用完全微调的模型相同，通过整个公式我们也就清楚了LoRA为什么要配合基础模型一起用，因为W0来自于基础模型（几个G），ΔW来自于自己训练的LoRA模型（小，百M），用a来控制两个模型的融合程度。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-3.gif" class="" title="图片">




<p><strong>3. 在哪里可以找到LoRA模型？</strong>  </p>
<p>这里推荐3个：<br>第一个是civitai.com,江湖人称之为C站，模型很丰富，因为有些模型不合规，国内现在直接访问不了，需要魔法。<br>第二个是Hugging Face,模型不是很多，但是还有很多不错的模型可挑选。<br>第三个是炼丹阁：这是一家国内公司连夜去civitai搬运的，模型经过了筛选，具有一定的合规性，试过也可以。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-1-LoRA介绍" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/1-LoRA%E4%BB%8B%E7%BB%8D/">1. LoRA介绍</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:48:21.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      
    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-sd-webui-只能由一个用户同时使用吗？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/20/sd-webui-%E5%8F%AA%E8%83%BD%E7%94%B1%E4%B8%80%E4%B8%AA%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%90%97%EF%BC%9F/">sd-webui 只能由一个用户同时使用吗？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-19T23:36:30.000Z" itemprop="datePublished">
  2023-06-20
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>有人在AUTOMATIC1111问：<br>我在带有 NVIDIA Tesla T4 的 GPU 服务器上部署了一个 sd webui，并配置了一个远程服务，这意味着任何地方的任何人都可以通过 GPU 服务器的公共 IP 地址访问 sd webui url。</p>
<p>例如，Li 从纽约访问 webui url，而 James 从芝加哥访问 webui url</p>
<p>但是我发现如果李正在生成图像，同时，詹姆斯点击生成按钮，他必须等待李完成她的生成工作（webui 显示“等待，排队”）。真是令人费解，李在生成图像时GPU服务器几乎空闲，为什么詹姆斯不能立即开始生成？如果我希望 2 或 200 人可以同时从不同的地方开始生成图像，我该怎么办？</p>
<p>Q：如何配置为“启动多个 WebUI 实例，我是新手？</p>
<p>A: 如果启动多个WebUI，它们会默认挂载在不同的端口上，可以并行工作。 由于 Python 的性质，单实例并行是不可能的。</p>
<p>总结：在不同的 web 端口上启动 web 服务，并部署负载平衡以响应多用户生成的请求。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
  </div>


  


        </div>
      
      
        <div class="d-flex justify-content-center pt-5">
          <a href="/archives/" title="→ 查看更多" class="btn btn-lg bg-white shadow-hover">→ 查看更多</a>
        </div>
      
    </div>
  </section>


    </section>
    <footer class="footer pt-5 mt-5">
  <div class="container">
    <div class="py-3">
      <div class="row justify-content-between">
        <div class="col-6">
          <img class="filter-gray mb-3 lazyload" height="40" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
          <p class="mb-4">这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。</p>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="javascript:;">
                  <img 0="微信" src="/images/icons/contact_wechat.svg">
                </a>
              </li>
            
              <li class="list-inline-item">
                <a href="mailto:a@abc.com">
                  <img 0="邮箱" src="/images/icons/contact_email.svg">
                </a>
              </li>
            
          </ul>
        </div>
        <div class="col-4">
          <h5>友情链接</h5>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="https://acorn.imaging.xin/" title="Acorn" target="_blank" rel="noopener">Acorn</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://github.com/" title="GitHub" target="_blank" rel="noopener">GitHub</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://duoyu.wang/" title="To Base64" target="_blank" rel="noopener">To Base64</a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
    <hr class="hr" style="opacity: .25;">
    <div class="pt-3 pb-5">
      <ul class="list-inline mb-0 text-center">
        <li class="list-inline-item">&copy; 2023 AI架构 | AI系统基础架构设计与优化</li>
        
        <li class="list-inline-item">Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
        <li class="list-inline-item">Designer <a href="https://acorn.imaging.xin/" target="_blank">罗平</a></li>
      </ul>
    </div>
  </div>
</footer>
  </main>
  <div id="mobile-nav-dimmer"></div>
<div id="mobile-nav">
	<div id="mobile-nav-inner">
		<ul class="mobile-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		
	</div>
</div>

  <script src="/libs/feather/feather.min.js"></script>
<script src="/libs/lazysizes/lazysizes.min.js"></script>

	<script src="/libs/tocbot/tocbot.min.js"></script>
	<script>
    tocbot.init({
      // Where to render the table of contents.
      tocSelector: '.js-toc',
      // Where to grab the headings to build the table of contents.
      contentSelector: '.js-toc-content',
      // Which headings to grab inside of the contentSelector element.
      headingSelector: 'h2, h3',
      // For headings inside relative or absolute positioned containers within content.
      hasInnerContainers: true,
    });
	</script>





<script src="/js/mobile-nav.js"></script>


<script src="/js/script.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178892506-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178892506-1');
</script>
</body>
</html>