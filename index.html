<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="AI架构师, ai infra, AI系统设计, AI系统优化, 机器学习, 深度学习, 大数据处理, 高性能AI架构, 可扩展AI系统, ChatGPT, Stable Diffusion, AI 绘画, 大模型">
  
  
    <meta name="description" content="这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
  <title> AI架构 | AI系统基础架构设计与优化</title>
  
    <link rel="apple-touch-icon" sizes="57x57" href="/images/webclip/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/images/webclip/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/images/webclip/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/images/webclip/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/images/webclip/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/images/webclip/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/images/webclip/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/webclip/apple-touch-icon-180x180.png">
    <link rel="apple-touch-icon" sizes="167x167" href="/images/webclip/apple-touch-icon-167x167.png">
  
  
    <link rel="shortcut icon" href="/images/favicon.ico">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <main class="main">
    
	<header id="header" class="header header-absolute">

	<div class="container">
		<nav class="navbar d-flex align-items-center">
			<a class="brand" href="/">
				<img class="logo lazyload" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
			</a>
			<ul class="main-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		</nav>
		<a id="mobile-nav-toggle">
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
			<span class="mobile-nav-toggle-bar"></span>
		</a>
	</div>
</header>

    <section>
      <!-- Index -->

<div class="hero">
	<figure class="hero-figure">
		<img class="hero-figure-img" src="/images/banner/banner.jpg" alt="AI架构 | AI系统基础架构设计与优化">
		<figcaption>
			<div class="container">
				<div class="figure-inset">
					<h2 class="h1">深入AI架构的奥秘</h2>
					<p>创造卓越的智能解决方案</p>
				</div>
			</div>
		</figcaption>
		<div class="learn-more">
			<a class="anchor" href="#landingpage">
				<img id="landingpage" src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='24' height='24' fill='white' viewBox='0 0 16 16'><path d='M8 3a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 3zm4 8a4 4 0 0 1-8 0V5a4 4 0 1 1 8 0v6zM8 0a5 5 0 0 0-5 5v6a5 5 0 0 0 10 0V5a5 5 0 0 0-5-5z'/></svg>" alt="">
			</a>
		</div>
	</figure>
</div>


<section class="section py-5 bg-light">
  <div class="container">
    <!-- 
  Data Files: source/_data/culture.yml
-->
<div class="row">
  
</div>
  </div>
</section>


  <section class="section py-5 " id="">
    <div class="container">
      <div class="section-heading text-center mb-5">
        <h3>文章</h3>
        <p class="text-gray">汇聚热点话题 打造创新思路</p>
      </div>

      
        <div class="section-body">
          
  <div class="row flex-wrap">
    
      <div class="col-3">
        <article id="post-什么是Dreambooth？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/">什么是Dreambooth？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-07-04T13:26:35.000Z" itemprop="datePublished">
  2023-07-04
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p><a target="_blank" rel="noopener" href="https://dreambooth.github.io/">Dreambooth</a>由 Google 研究团队于 2022 年发布，是一种通过向模型注入自定义主题来微调扩散模型（如Stable Diffusion）的技术。</p>
<p>为什么叫Dreambooth？据谷歌研究团队称，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">It’s like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you.</span><br></pre></td></tr></table></figure>

<p>DreamBooth 算法对 Imagen 模型进行了微调，从而实现了将现实物体在图像中真实还原的功能，通过少量实体物品图像的 fine-turning，使得原有的 SD 模型能对图像实体记忆保真，识别文本中该实体在原图像中的主体特征甚至主题风格，是一种新的文本到图像“个性化”（可适应用户特定的图像生成需求）扩散模型。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/stable-diffusion-quick-kit-series-model-fine-tuning-with-dreambooth-optimization-practices-on-sagemaker2.png" class="" title="img">

<p>听起来很棒！但它的效果如何？下面是研究文章中的一个例子。仅使用特定狗（我们称之为<strong>Devora</strong>）的 3 张图像作为输入，dreamboothed 模型就可以在不同的环境中生成 Devora 的图像。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/image-13.png" class="" title="dreambooth 研究文章中的 dreambooth 示例">

<p>只需 3 个训练图像，Dreambooth 即可将自定义主题无缝注入扩散模型。</p>
<h3 id="Dreambooth-如何运作？"><a href="#Dreambooth-如何运作？" class="headerlink" title="Dreambooth 如何运作？"></a>Dreambooth 如何运作？</h3><p>Dreambooth 是对整个神经网络所有层权重进行调整，会将输入的图像训练进 Stable Diffusion 模型，它的本质是先复制了源模型，在源模型的基础上做了微调（fine tunning）并独立形成了一个新模型，在它的基本上可以做任何事情。缺点是，训练它需要大量 VRAM, 目前经过调优后可以在 16GB 显存下完成训练。</p>
<p>您可能会问，为什么不能简单地使用这些图像通过额外的步骤来训练模型？问题是，这样做会因<em>过度拟合</em>（因为数据集非常小）导致失败。</p>
<p>Dreambooth 通过以下方式解决了这些问题：</p>
<ol>
<li>对新主题使用一个<strong>罕见的单词（请注意，我为狗使用了一个罕见的名字****Devora</strong>），这样它一开始在模型中就没有太多意义。</li>
<li><strong>类的预先保留</strong>：为了保留<strong>类</strong>（上例中的狗）的含义，模型以注入主体（Devora）的方式进行微调，同时生成类（狗）的图像。保存下来。</li>
</ol>
<p>还有另一种类似的技术称为<a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"><a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"> textual inversion</a>. </a>。不同之处在于，Dreambooth 对整个模型进行了微调，而<a target="_blank" rel="noopener" href="https://textual-inversion.github.io/"> textual inversion</a>.则注入了一个新词，而不是重复使用生僻词，并且仅对模型的文本嵌入部分进行了微调。</p>
<h3 id="训练-Dreambooth-需要什么？"><a href="#训练-Dreambooth-需要什么？" class="headerlink" title="训练 Dreambooth 需要什么？"></a>训练 Dreambooth 需要什么？</h3><p>你需要三样东西</p>
<ol>
<li>一些自定义图像</li>
<li>唯一标识符</li>
<li>一个类名class</li>
</ol>
<p>在上面的例子中。唯一标识符是<strong>Devora</strong>，class名称是<strong>狗</strong>。</p>
<p>然后你需要构建你的<strong>实例提示</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of [unique identifier] [class name]</span><br></pre></td></tr></table></figure>

<p>还有<strong>class提示</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of [class name]</span><br></pre></td></tr></table></figure>

<p>在上面的例子中，<strong>实例提示符</strong>是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of Devora dog</span><br></pre></td></tr></table></figure>

<p>由于 Devora 是一只狗，所以<strong>class提示</strong>是</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a photo of a dog</span><br></pre></td></tr></table></figure>

<h3 id="获取训练图像"><a href="#获取训练图像" class="headerlink" title="获取训练图像"></a>获取训练图像</h3><p>为您的自定义主题拍摄 3-10 张照片。照片应该从不同的角度拍摄。</p>
<p>拍摄对象还应该处于多种背景中，以便模型可以将拍摄对象与背景区分开来。</p>
<p>我将在教程中使用这个玩具。</p>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/0E5AFC83-B759-4FE9-8E16-A60774E1DEDF_1_105_c.jpeg" class="" title="img">

<h3 id="调整图像大小"><a href="#调整图像大小" class="headerlink" title="调整图像大小"></a>调整图像大小</h3><p>为了在训练中使用图像，您首先需要将它们的大小调整为 512×512 像素，以便使用 v1 模型进行训练。</p>
<p><a target="_blank" rel="noopener" href="https://www.birme.net/?target_width=512&target_height=512">BIRME</a>是一个调整图像大小的便捷网站。</p>
<ol>
<li>将您的图像拖放到 BIRME 页面。</li>
<li>调整每张图像的画布，使其充分显示主题。</li>
<li>确保宽度和高度均为 512 像素。</li>
<li>按<strong>“</strong>SAVE FILES** ”**将调整大小的图像保存到您的计算机。</li>
</ol>
<img src="/2023/07/04/%E4%BB%80%E4%B9%88%E6%98%AFDreambooth%EF%BC%9F/image-14.png" class="" title="img">

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>目前业界对 DreamBooth 做 fine tuning 主要为两种方式，一是在 Stable Diffusion WebUI 可视话界面进行模型的选择，训练图片的上载及本地化的训练；二是在第三方 IDE 平台如 colab notebook 上以脚本交互式开发的方式进行训练。</p>
<p>第一种方式只能在部署 Stable Diffusion WebUI 应用的单一服务器或主机上训练，无法与企业及客户的后台平台及业务集成；而第二种方式侧重于算法工程师个人在开发测试阶段进行模型实验探索，无法实现生产化工程化的部署。此外，以上两种方式训练 dreambooth，还需要关注高性能算力机资源的成本（尤其对模型效果要求较高的场景，需要多达 50 张以上的 class images，显存容易 OOM），基础模型和 fine tuning 后模型的存储和管理，训练超参的管理，统一的日志监控，训练加速，依赖 lib 编译打包等具体实施落地层面的一系列困难和挑战。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time_start = time.time()</span><br><span class="line"><span class="comment">#@title DreamBooth</span></span><br><span class="line">HUGGINGFACE_TOKEN = <span class="string">&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Name/Path of the initial model. (Find model name [here](https://huggingface.co/models))</span></span><br><span class="line">MODEL_NAME = <span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">BRANCH = <span class="string">&quot;fp16&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Enter instance prompt and class prompt.\</span></span><br><span class="line"><span class="comment">#@markdown Example 1: photo of zwx person, photo of a person\</span></span><br><span class="line"><span class="comment">#@markdown Example 2: photo of zwx toy, photo of a toy</span></span><br><span class="line">instance_prompt = <span class="string">&quot;photo of zwx toy&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">class_prompt =  <span class="string">&quot;photo of a toy&quot;</span> <span class="comment">#@param &#123;type:&quot;string&quot;&#125;</span></span><br><span class="line">training_steps = <span class="number">800</span> <span class="comment">#@param &#123;type:&quot;integer&quot;&#125;</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span> <span class="comment">#@param &#123;type:&quot;number&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown  Convert to fp16? (takes half the space (2GB)).</span></span><br><span class="line">fp16 = <span class="literal">True</span> <span class="comment">#@param &#123;type: &quot;boolean&quot;&#125;</span></span><br><span class="line"><span class="comment">#@markdown  Compile xformers (Try only if you see xformers error. Will take 1 more hour).</span></span><br><span class="line">complie_xformers = <span class="literal">False</span> <span class="comment">#@param &#123;type: &quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line">save_to_gdrive = <span class="literal">True</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line"><span class="keyword">if</span> save_to_gdrive:</span><br><span class="line">  drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#@markdown Clear log after run?</span></span><br><span class="line">CLEAR_LOG = <span class="literal">False</span> <span class="comment">#@param &#123;type:&quot;boolean&quot;&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OUTPUT_DIR = <span class="string">&quot;stable_diffusion_weights/output&quot;</span> </span><br><span class="line">OUTPUT_DIR = <span class="string">&quot;/content/&quot;</span> + OUTPUT_DIR</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check type of GPU and VRAM available.</span></span><br><span class="line">!nvidia-smi --query-gpu=name,memory.total,memory.free --<span class="built_in">format</span>=csv,noheader</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] Weights will be saved at <span class="subst">&#123;OUTPUT_DIR&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">!mkdir -p $OUTPUT_DIR</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.</span></span><br><span class="line"></span><br><span class="line">concepts_list = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;instance_prompt&quot;</span>:      instance_prompt,</span><br><span class="line">        <span class="string">&quot;class_prompt&quot;</span>:         class_prompt,</span><br><span class="line">        <span class="string">&quot;instance_data_dir&quot;</span>:    <span class="string">&quot;/content/data/instance&quot;</span>,</span><br><span class="line">        <span class="string">&quot;class_data_dir&quot;</span>:       <span class="string">&quot;/content/data/class&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># `class_data_dir` contains regularization images</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> concepts_list:</span><br><span class="line">    os.makedirs(c[<span class="string">&quot;instance_data_dir&quot;</span>], exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;concepts_list.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(concepts_list, f, indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> concepts_list:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Uploading instance images for `<span class="subst">&#123;c[<span class="string">&#x27;instance_prompt&#x27;</span>]&#125;</span>`&quot;</span>)</span><br><span class="line">    uploaded = files.upload()</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> uploaded.keys():</span><br><span class="line">        dst_path = os.path.join(c[<span class="string">&#x27;instance_data_dir&#x27;</span>], filename)</span><br><span class="line">        shutil.move(filename, dst_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clear</span>():</span><br><span class="line">    <span class="keyword">from</span> IPython.display <span class="keyword">import</span> clear_output; <span class="keyword">return</span> clear_output()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># huggingface token</span></span><br><span class="line">!mkdir -p ~/.huggingface</span><br><span class="line">!echo -n <span class="string">&quot;&#123;HUGGINGFACE_TOKEN&#125;&quot;</span> &gt; ~/.huggingface/token</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># install repos</span></span><br><span class="line">!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py</span><br><span class="line">!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py</span><br><span class="line"><span class="comment">#%pip install torch==2.0.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span></span><br><span class="line">%pip install -qq git+https://github.com/ShivamShrirao/diffusers</span><br><span class="line">%pip install -q -U --pre triton</span><br><span class="line">%pip install -q accelerate==<span class="number">0.19</span><span class="number">.0</span> transformers ftfy bitsandbytes==<span class="number">0.35</span><span class="number">.0</span> gradio natsort safetensors xformers</span><br><span class="line"><span class="comment"># install xformer wheel</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Install xformers&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> complie_xformers:</span><br><span class="line">  %pip install git+https://github.com/facebookresearch/xformers@4c06c79<span class="comment">#egg=xformers</span></span><br><span class="line"><span class="comment">#else:</span></span><br><span class="line"><span class="comment">#  %pip install  --no-deps -q https://github.com/brian6091/xformers-wheels/releases/download/0.0.15.dev0%2B4c06c79/xformers-0.0.15.dev0+4c06c79.d20221205-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"><span class="comment">#%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/4c06c79_various6/xformers-0.0.15.dev0_4c06c79.d20221201-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"><span class="comment">#%pip install -q https://github.com/ShivamShrirao/xformers-wheels/releases/download/4c06c79/xformers-0.0.15.dev0+4c06c79.d20221201-cp38-cp38-linux_x86_64.whl</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############## Edit this section to customize parameters</span></span><br><span class="line">!python3 train_dreambooth.py \</span><br><span class="line">  --pretrained_model_name_or_path=$MODEL_NAME \</span><br><span class="line">  --pretrained_vae_name_or_path=<span class="string">&quot;stabilityai/sd-vae-ft-mse&quot;</span> \</span><br><span class="line">  --output_dir=$OUTPUT_DIR \</span><br><span class="line">  --revision=$BRANCH \</span><br><span class="line">  --with_prior_preservation --prior_loss_weight=<span class="number">1.0</span> \</span><br><span class="line">  --seed=<span class="number">1337</span> \</span><br><span class="line">  --resolution=<span class="number">512</span> \</span><br><span class="line">  --train_batch_size=<span class="number">1</span> \</span><br><span class="line">  --train_text_encoder \</span><br><span class="line">  --mixed_precision=<span class="string">&quot;fp16&quot;</span> \</span><br><span class="line">  --use_8bit_adam \</span><br><span class="line">  --gradient_accumulation_steps=<span class="number">1</span> \</span><br><span class="line">  --learning_rate=$learning_rate \</span><br><span class="line">  --lr_scheduler=<span class="string">&quot;constant&quot;</span> \</span><br><span class="line">  --lr_warmup_steps=<span class="number">0</span> \</span><br><span class="line">  --num_class_images=<span class="number">50</span> \</span><br><span class="line">  --sample_batch_size=<span class="number">4</span> \</span><br><span class="line">  --max_train_steps=$training_steps \</span><br><span class="line">  --save_interval=<span class="number">10000</span> \</span><br><span class="line">  --save_sample_prompt=<span class="string">&quot;$instance_prompt&quot;</span> \</span><br><span class="line">  --concepts_list=<span class="string">&quot;concepts_list.json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.</span></span><br><span class="line"><span class="comment"># `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory).</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> natsort <span class="keyword">import</span> natsorted</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">weightdirs = natsorted(glob(OUTPUT_DIR + os.sep + <span class="string">&quot;*&quot;</span>))</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(weightdirs) == <span class="number">0</span>:</span><br><span class="line">  <span class="keyword">raise</span> KeyboardInterrupt(<span class="string">&quot;No training weights directory found&quot;</span>)</span><br><span class="line">WEIGHTS_DIR = weightdirs[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ckpt_path = WEIGHTS_DIR + <span class="string">&quot;/model.ckpt&quot;</span></span><br><span class="line"></span><br><span class="line">half_arg = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">if</span> fp16:</span><br><span class="line">    half_arg = <span class="string">&quot;--half&quot;</span></span><br><span class="line">!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] Converted ckpt saved at <span class="subst">&#123;ckpt_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CLEAR_LOG:</span><br><span class="line">  clear()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;[*] WEIGHTS_DIR=<span class="subst">&#123;WEIGHTS_DIR&#125;</span>&quot;</span>)</span><br><span class="line">minutes = (time.time()-time_start)/<span class="number">60</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Dreambooth completed successfully. It took %1.1f minutes.&quot;</span>%minutes)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"></span><br><span class="line">weights_folder = OUTPUT_DIR</span><br><span class="line">folders = <span class="built_in">sorted</span>([f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(weights_folder) <span class="keyword">if</span> f != <span class="string">&quot;0&quot;</span>], key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x))</span><br><span class="line"></span><br><span class="line">row = <span class="built_in">len</span>(folders)</span><br><span class="line">col = <span class="built_in">len</span>(os.listdir(os.path.join(weights_folder, folders[<span class="number">0</span>], <span class="string">&quot;samples&quot;</span>)))</span><br><span class="line">scale = <span class="number">4</span></span><br><span class="line">fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw=&#123;<span class="string">&#x27;hspace&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;wspace&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, folder <span class="keyword">in</span> <span class="built_in">enumerate</span>(folders):</span><br><span class="line">    folder_path = os.path.join(weights_folder, folder)</span><br><span class="line">    image_folder = os.path.join(folder_path, <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    images = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(image_folder)]</span><br><span class="line">    <span class="keyword">for</span> j, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="keyword">if</span> row == <span class="number">1</span>:</span><br><span class="line">            currAxes = axes[j]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            currAxes = axes[i, j]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            currAxes.set_title(<span class="string">f&quot;Image <span class="subst">&#123;j&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">            currAxes.text(-<span class="number">0.1</span>, <span class="number">0.5</span>, folder, rotation=<span class="number">0</span>, va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, transform=currAxes.transAxes)</span><br><span class="line">        image_path = os.path.join(image_folder, image)</span><br><span class="line">        img = mpimg.imread(image_path)</span><br><span class="line">        currAxes.imshow(img, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">        currAxes.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">&#x27;grid.png&#x27;</span>, dpi=<span class="number">72</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> save_to_gdrive:</span><br><span class="line">  <span class="keyword">import</span> os.path</span><br><span class="line">  gPath = <span class="string">&quot;/content/drive/MyDrive/Dreambooth_model&quot;</span></span><br><span class="line">  !mkdir -p $gPath</span><br><span class="line">  filename = <span class="string">&#x27;model.ckpt&#x27;</span></span><br><span class="line">  i = <span class="number">1</span></span><br><span class="line">  ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">  <span class="keyword">while</span> os.path.isfile(ckpt_gpath):</span><br><span class="line">    filename = <span class="string">&#x27;model%d.ckpt&#x27;</span>%i</span><br><span class="line">    ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">  ckpt_gpath = gPath + <span class="string">&#x27;/&#x27;</span> + filename</span><br><span class="line">  !cp $ckpt_path $ckpt_gpath</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;Model saved to %s&#x27;</span>%ckpt_gpath)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>完成训练大约需要30分钟。完成后模型可以放入AUTOMATIC1111 GUI ，就可以用新模型生成的一些示例图像。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-GPU虚拟化" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/27/GPU%E8%99%9A%E6%8B%9F%E5%8C%96/">GPU虚拟化</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-27T11:53:18.000Z" itemprop="datePublished">
  2023-06-27
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>GPU 资源池化技术从初期的简单虚拟化，到资源池化，经历了四个技术演进阶段。</p>
<p><strong>简单虚拟化</strong></p>
<p>将物理 GPU 按照 2 的 N 次方，切分成多个固定大小的 vGPU（VirtualGPU，虚拟 GPU），每个 vGPU 的算力和显存相等。实践证明，不同的 AI 模型对于算力、显存资源的需求是不同的。所以，这样的切分方式，并不能满足 AI 模型多样化的需求。</p>
<p><strong>任意虚拟化</strong></p>
<p>将物理 GPU 按照算力和显存两个维度，自定义切分，获得满足 AI 应用个性化需求的 vGPU。</p>
<p><strong>远程调用</strong></p>
<p>AI 应用与物理 GPU 服务器分离部署，允许通过高性能网络远程调用 GPU资源。这样可以实现 AI 应用与物理 GPU 资源剥离，AI 应用可以部署在私有云的任</p>
<p>意位置，只需要网络可达，即可调用 GPU 资源。</p>
<p><strong>资源池化</strong></p>
<p>形成 GPU 资源池后，需要统一的管理面来实现管理、监控、资源调度和资源回收等功能。同时，也需要提供北向 API，与数据中心级的资源调度平台对</p>
<p>接，让用户在单一界面，就可以调度包括 vGPU 在内的数据中心内的各类资源。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-如何使用-ChatGPT-和-LangChain-框架构建自己的QA应用" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/">如何使用 ChatGPT 和 LangChain 框架构建自己的QA应用</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:50:32.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>大型语言模型 (LLM) 正在成为一种变革性技术，使开发人员能够构建他们以前无法构建的应用程序。但是单独使用这些 LLM 往往不足以创建一个真正强大的应用程序——当您可以将它们与其他计算或知识来源相结合时，真正的力量就来了。</p>
<p>LLM 本质上是非常通用的，这意味着虽然它们可以有效地执行许多通用的任务，比如回答如何制作一道美味的红烧肉。但它们通常不能直接为特定领域的问题或任务提供具体答案。</p>
<p><strong>这里就提出了一个问题：如何基于ChatGPT为自己的业务赋能。</strong></p>
<p>虽然官方提供了微调服务，但是由于缺乏最佳实践作为参考，加上费用不小，对于很多没有专门的算法人员的企业来说，显然微调不是一个好选择。</p>
<p>幸运的是，LangChain出现了，它是包含一个称为数据增强生成的功能，它允许您提供一些上下文数据来增强 LLM 的知识；</p>
<p>今天就给大家介绍下如何基于自己文本资料，构建基于文档的问答Demo。</p>
<p>代码的结构可以分为3个主要部分：</p>
<ol>
<li><p>1.加载自己的txt文件(里面是自己业务领域的东西)</p>
</li>
<li><p>2.创建embedding和向量化</p>
</li>
<li><p>3.查询txt</p>
</li>
</ol>
<p>现在，让我们深入了解这些步骤中的每一个！</p>
<p>运行前提条件：</p>
<p>需要有个OpenAI api_key用于程序跟ChatGPT做身份验证，这个需要去OpenAI 官方注册，如果你不会搞，可以留言说明。</p>
<p>python安装依赖：我用的python 3.11.1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain llama_index openai chromadb</span><br></pre></td></tr></table></figure>

<p>然后我们需要在终端中设置环境变量，设置open api。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export OPENAI_API_KEY=&quot;...&quot;</span><br></pre></td></tr></table></figure>

<p>或者，您可以从 Jupyter notebook（或 Python 脚本）中执行此操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import osos.environ[&quot;OPENAI_API_KEY&quot;] = &quot;...&quot;</span><br></pre></td></tr></table></figure>

<p>首先准备一个content.txt文档，文件里存入一些数据，内容摘录：</p>
<img src="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/640.png" class="" title="图片">

<p>现在创建一个程序来完成问答：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="comment"># 加载放了QA的txt文件</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;content.txt&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"><span class="comment"># 把大段文字切成小块</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">200</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">texts = text_splitter.split_documents(documents)</span><br><span class="line"><span class="comment"># 创建embeddings</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">docsearch = Chroma.from_documents(texts, embeddings)</span><br><span class="line"><span class="comment"># 通过这个OpenAIEmbedding api给每个小文档计算embedding，存到doc_search</span></span><br><span class="line"><span class="comment"># 根据查询输入找到相似度最高的块作为上下文</span></span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=<span class="string">&quot;stuff&quot;</span>, retriever=docsearch.as_retriever())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">q</span>):</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f&quot;Query: <span class="subst">&#123;q&#125;</span>&quot;</span>)</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f&quot;Answer: <span class="subst">&#123;qa.run(q)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">     <span class="built_in">print</span>(query(<span class="string">&quot;如何开会&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>我们看下结果, 返回结果就是我原始文档中数据，很强大有没有!而且代码也很简单。</p>
<img src="/2023/06/26/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-ChatGPT-%E5%92%8C-LangChain-%E6%A1%86%E6%9E%B6%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84QA%E5%BA%94%E7%94%A8/640-1687733479465-1.png" class="" title="图片">

<p>借助这个小案例，其实也就是抛砖引玉，这里关键就是引入了LangChain这个框架，把ChatGPT和LangChain结合那就是如虎填翼。有了这样一个基础，你想想只要能文本化的东西都能创建这样一个知识提取的应用，结合特定业务那就有非常多的应用空间。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-LoRA是什么？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/">LoRA是什么？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:48:58.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p><strong>1. LoRA介绍</strong></p>
<p>LoRA（ Low-rank Adaptation）是微软研究员引入的一项新技术，主要用于处理大模型微调的问题，本文主要介绍Lora如何微调Stable Diffusion。除此之外微调Stable Diffusion还有DreamBooth（DR）和Textual Inversion（TI）等训练技术。</p>
<p>LoRA有什么厉害之处？实际上LoRA在文件大小和训练能力之间提供了一个很好的平衡。DR虽然功能强大，但是生成的模型文件很大（2-7G）。TI生成的文件虽小（100k），但是训练效果不怎么好。</p>
<p>LoRA介于两者之间，模型（2-200M）文件大小可控，训练能力不错。玩过Stable Diffusion的人都知道，要试验各种模型，前提是你要有足够的磁盘空间，一般一个模型都是好几G的。下载那么大的模型，你的网络带宽也得非常OK（至少每秒几M），不然真的没耐心。所以这也是LoRA比较流行的原因之一吧。与TI相同，LoRA模型不能单独使用，必须与训练的基础模型配合一起使用。</p>
<p><strong>2. LoRA是如何工作的？</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">首先我们来看看Stable Diffusion的架构，主要由三部分组成：文本编码器，可将您的提示词转换为latent向量。一种扩散模型，它反复对 64x64 latent patch进行&quot;去噪&quot;。解码器，将最终的 64x64 latent patch转换为更高分辨率的 512x512 图像。它的一个处理过程就是下图所示，上面每一个组成部分都是一个模型，然后通过PIPLINE把几个模型串联起来，就可以达到生成图片的效果。而且每个部分实际上都是可以独立部署的。</span><br></pre></td></tr></table></figure>



<p>最近总是听到LoRA，LoRA，名字怪好听，就是难理解，什么低秩适应完全搞不懂，今天就来扒扒，看看它是怎么个低法。</p>
<p>原来LoRA对Stable Diffusion模型最关键的部分进行了微小修改：cross-attention layers，它是模型中图像和提示相交的部分-交叉注意层（U-Net 噪声预测器的 QKV 部分）。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640.png" class="" title="图片">



<p>研究人员发现，通过聚焦大模型的 Transformer 注意力块，使用 LoRA 进行的微调质量与全模型微调相当，同时速度更快且需要更少的计算。</p>
<p>Simo Ryu (<code>@cloneofsimo</code>) 是第一个提出适用于 Stable Diffusion 的 LoRA实现的人。如果想查看相关示例和许多其他有趣的讨论和见解。请一定要看看他们的GitHub 项目。</p>
<p>cross-attention 注意力交叉层的权重排列在矩阵中。矩阵是一堆按行和列排列的数字<br>就像在Excle里面一样。LoRA通过将其权重添加到这些矩阵来微调模型。</p>
<p>假设模型是包含1000行和2000列的矩阵。那模型需要存2,000,000 个数字 (1,000 x 2,000)。</p>
<p>LoRA将矩阵分解为：1,000×2 矩阵和 2×2,000 矩阵。那模型只需要存 6,000 个数字 (1,000 x 2 + 2 x 2,000)，少了333倍，这就是LoRA文件小得多的原因。</p>
<p>我们看下作者在github中的介绍：</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-1.png" class="" title="图片">



<p>意思就是说：完全微调过程慢，在质量和训练速度很难找到平衡，虽然也有像TI这样的方法，但是效果不理想。LoRA的出现解决了社区遇到的问题：就是模型太大，用户想要基于社区各种模型进行微调，因为模型太大而无法使用，LoRA尝试微调模型的残差，而不是整个模型：也就训练delta W代替W。而delta W进一步分解成A矩阵和B的转置矩阵相乘。然后微调A和B替代W，A和B比原始的W要小多。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-2.jpeg" class="" title="图片">


<p>LoRA 将一个大矩阵分解为两个小的低阶矩阵，在这个例子中，矩阵的秩为2。它比原始维度低很多，所以被称之为低秩矩阵。</p>
<p>使用 LoRA 对插图数据集进行微调：W&#x3D;W0 + aΔW， a是合并比率。ΔW说就是上面说的矩阵A矩阵和B矩阵的转置乘积，gif是将 alpha从0缩放到1。将alpha设置为0与使用原始模型相同，将alpha设置为1与使用完全微调的模型相同，通过整个公式我们也就清楚了LoRA为什么要配合基础模型一起用，因为W0来自于基础模型（几个G），ΔW来自于自己训练的LoRA模型（小，百M），用a来控制两个模型的融合程度。</p>
<img src="/2023/06/26/LoRA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/640-1687733382169-3.gif" class="" title="图片">




<p><strong>3. 在哪里可以找到LoRA模型？</strong>  </p>
<p>这里推荐3个：<br>第一个是civitai.com,江湖人称之为C站，模型很丰富，因为有些模型不合规，国内现在直接访问不了，需要魔法。<br>第二个是Hugging Face,模型不是很多，但是还有很多不错的模型可挑选。<br>第三个是炼丹阁：这是一家国内公司连夜去civitai搬运的，模型经过了筛选，具有一定的合规性，试过也可以。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-1-LoRA介绍" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/26/1-LoRA%E4%BB%8B%E7%BB%8D/">1. LoRA介绍</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-25T22:48:21.000Z" itemprop="datePublished">
  2023-06-26
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      
    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-sd-webui-只能由一个用户同时使用吗？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/20/sd-webui-%E5%8F%AA%E8%83%BD%E7%94%B1%E4%B8%80%E4%B8%AA%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8%E5%90%97%EF%BC%9F/">sd-webui 只能由一个用户同时使用吗？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-19T23:36:30.000Z" itemprop="datePublished">
  2023-06-20
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>有人在AUTOMATIC1111问：<br>我在带有 NVIDIA Tesla T4 的 GPU 服务器上部署了一个 sd webui，并配置了一个远程服务，这意味着任何地方的任何人都可以通过 GPU 服务器的公共 IP 地址访问 sd webui url。</p>
<p>例如，Li 从纽约访问 webui url，而 James 从芝加哥访问 webui url</p>
<p>但是我发现如果李正在生成图像，同时，詹姆斯点击生成按钮，他必须等待李完成她的生成工作（webui 显示“等待，排队”）。真是令人费解，李在生成图像时GPU服务器几乎空闲，为什么詹姆斯不能立即开始生成？如果我希望 2 或 200 人可以同时从不同的地方开始生成图像，我该怎么办？</p>
<p>Q：如何配置为“启动多个 WebUI 实例，我是新手？</p>
<p>A: 如果启动多个WebUI，它们会默认挂载在不同的端口上，可以并行工作。 由于 Python 的性质，单实例并行是不可能的。</p>
<p>总结：在不同的 web 端口上启动 web 服务，并部署负载平衡以响应多用户生成的请求。</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-controlnet更新" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/18/controlnet%E6%9B%B4%E6%96%B0/">controlnet更新</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T23:50:57.000Z" itemprop="datePublished">
  2023-06-18
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>在你没注意的时候ControlNet又更新了几次？ ControlNet更新总整理!! - YouTube<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=PTyWM15W7mg">https://www.youtube.com/watch?v=PTyWM15W7mg</a></p>
<p>Transcript:</p>
<p>（00：16） 大家好， 今天我们来更新一下信息 从上一篇tile模型更新后 ControlNet又用非常惊人的速度持续更新 今天我们来看看 到底又更新了些什么吧 在开始前说一下 我们拍这片的时候 Controlnet的版本是1.1.201版 那就开始吧 首先Controlnet最让人诟病的问题 就是模型跟预处理器非常的乱 现在的Controlnet预处理器 大概有40个以上 然后模型也有十几个 每一个选择都非常花时间 现在终于出现了非常方便的选择器 只需要在画面上 点选你要的内容 例如我选openpose 系统就会把openpose 能够对应的预处理器放上去 如果你有对应的模型 系统也会帮你把模型放好 非常的方便 </p>
<p>（01：19） 之后不用在预处理器之海迷航啦 接下来我们来说说 又增加了什么预处理器吧 第一个是Depth 他新增了一个leres++ 这个预处理器 对画面的细分度更高 可以针对每一个区域 做出更明确的分类 我们来看一下对照图 这是每一个预处理器 分别画出五张 从中间挑出三张比较好的结果 在我看来 细致度排名分别是这样子 什么意思呢 就是leres++ 对画面的还原度最高 不过主要还是看你的需求 再选择要使用的预处理器 接着我们看Tile Tile多了两个预处理器 过去有使用过tile应该会知道tile在放大动漫图像的时候会有颜色失真的问题 原本的颜色会跑掉很多还有一些报告指出某些部分会模糊通常是因为CFG不足导致 </p>
<p>（02：24） 所以目前tile针对这两个问题 做了对应的处理 经过调整 已经不需要提高CFG去控制Tile了 另外增加了两个预处理器 一个是tile_colorfix 他会针对颜色做补正 让颜色不会跑掉 另一个是tile_colorfix+sharp 这个是颜色补正加上锐利化 下方还有可以调整锐利度的滑块 这也是目前官方最推荐的预处理器 大家可以参考一下 目前在颜色修复上 我认为效果非常明显 但是要注意 colorfix会把原图的颜色锁死 是没有办法靠提词改变颜色的喔 再来是inpaint 在过去要使用inpaint模型的时候 通常我们会使用图生图的局部重绘 但是这样会有一个问题 我们得要上面涂一次屏蔽 下面ControlNet也图一次屏蔽 现在ControlNet不用这么麻烦了 </p>
<p>（03：27） 我们只需要在局部重绘的地方涂好屏蔽 下方ControlNet的部分 不需要放入任何东西 只需要选择局部重绘 ControlNet的启用打勾 直接按下生产就可以了 系统会自动把局部重绘的图像 送到ControlNet里面 就不用像过去的教学一样 上下都涂屏蔽了 而inpaint目前多了一个 inpaint_only预处理器 这个预处理器是用在 不使用webui的局部重绘功能时 如果你用inpaint_only预处理器 系统就不会重绘屏蔽以外的部分 最后是Reference 这是一个全新功能 他只有预处理器 不需要模型 Reference是一个概念简单 但是非常强大的功能 我们先来说一下原理 Reference的意思是参考 那他怎么参考的呢 其实就是将我们用来参考的图像 先送进SD的注意力层 等到我们的提词和参数进入的时候 做为参考 </p>
<p>（04：31） 得出相近的结果 目前Reference有三种 参考的方式有点不同 我们一样直接上对照图 Reference_only 读取的比较像是风格 Reference_adain 比较像是构图 最后是Reference_adain+attn 这是前面两个的结合 所以构图跟风格都会转移 这个功能有一个重点 他对参考图像的质量要求很高 如果你的图片是网页上面的照片 很容易产生质量不佳的状况 今天就介绍到这边（？ 等一下！！ 在视频快做完的时候 ControlNet又更新了4次 我们又有一个重大的更新 大家知道Adobe的Filefly 前阵子发布了 生成式填色的功能吧 不使用任何提词 就可以获得很棒的重绘结果 这个功能甚至可以轻松的向外绘制 前几天如果你问 SD能不能做一样的事 那答案是否定的 我们不使用任何提词 </p>
<p>（05：34） 就没有办法达成这个效果 而且结果并没有办法非常多样化 但就在前不久 ControlNet做到了 我们现在可以用SD 重现类似的成果 做法很简单 只要再文生图的地方使用inpaint 需要的只有高分辨率修复打勾 CFG官方建议小于5 接着打开ControlNet 模型选择局部重绘 下方的控制模式 改成ControlNet更重要 接着放上你想要填色的图片 这边要注意你的预处理器了 使用inpaint only 才不会改变屏蔽以外的图像 针对要填色的区域涂上屏蔽后 只要按下产生就可以了 如果你想要画面多做一些改变 只要加上少量的提 词 就能够获得很好的成果 那如果想要向外绘制呢 我们现在也可以用inpaint来达成了 只要把ControlNet的缩放模式 </p>
<p>（06：38） 改为调整大小并填充 接着调整你需要扩张的像素 按下一次产生 就会获得一张向外扩充的图片 接着再把这张图片 放入ControlNet中 把需要扩充的内容图上屏蔽 当然也可以再其他绘图软件中先做好 再放到SD里面 就不需要只是为了把图像扩张（口误） 而多执行一次 说不定你看到这部影片的时候 已经不需要这样做了 最后只要按下产生就完成了 今天真的就介绍到这边 各位看完这篇的时候 ControlNet可能又更新了好几次了 等过阵子 我们再来整理ControlNet又会了些什么吧 那就这样啦 下次见 掰</p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-Stable-Diffusion的工作原理" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/17/Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">Stable Diffusion的工作原理</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T08:07:29.000Z" itemprop="datePublished">
  2023-06-17
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 是一种文本到图像的潜在扩散模型，由<a target="_blank" rel="noopener" href="https://github.com/CompVis">CompVis</a>、<a target="_blank" rel="noopener" href="https://stability.ai/">Stability AI</a>和<a target="_blank" rel="noopener" href="https://laion.ai/">LAION</a>的研究人员和工程师创建。它使用来自<a target="_blank" rel="noopener" href="https://laion.ai/blog/laion-5b/">LAION-5B</a>数据库子集的 512x512 图像进行训练。 <em>LAION-5B</em>是目前最大的、可免费访问的多模态数据集。</p>
<p>看过Stable Diffusion可以产生的高质量图像后，让我们尝试更好地了解模型的功能。Stable Diffusion基于一种特殊类型的扩散模型，称为<strong>Latent Diffusion</strong>（<strong>潜在扩散</strong>）, 由本文提出：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>。</p>
<p>一般来说，扩散模型是机器学习系统，经过训练可以逐步对随机高斯噪声<em>进行去噪</em>，以获得感兴趣的样本，例如<em>图像</em>。要更详细地了解它们的工作原理，请查看<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">此 colab</a>。</p>
<p>扩散模型已显示可实现生成图像数据的最先进结果。但扩散模型的一个缺点是反向去噪过程很慢，因为它具有重复的、连续的性质。此外，这些模型会消耗大量内存，因为它们在像素空间中运行，在生成高分辨率图像时像素空间会变得很大。因此，训练这些模型并将它们用于推理具有挑战性。</p>
<p><em>潜在扩散可以通过在较低维度的潜在</em>空间上应用扩散过程而不是使用实际像素空间来降低内存和计算复杂性。这是标准扩散模型和潜在扩散模型之间的主要区别：<strong>在潜在扩散中，模型经过训练以生成图像的潜在（压缩）表示。</strong></p>
<p>潜在扩散有三个主要组成部分。</p>
<ol>
<li>A autoencoder (VAE).</li>
<li>A <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq">U-Net</a>.</li>
<li>A text-encoder, <em>e.g.</em> <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIP’s Text Encoder</a>.</li>
</ol>
<p><strong>1. The autoencoder (VAE)</strong></p>
<p>VAE模型有两部分，编码器和解码器。编码器用于将图像转换为低维潜在表示，作为<em>U-Net</em>模型的输入。相反，解码器将潜在表示转换回图像。</p>
<p>在潜在扩散<em>训练</em>期间，编码器用于获取图像的潜在表示（<em>潜在）用于前向扩散过程，它在每一步应用越来越多的噪声。</em>在<em>推理过程</em>中，使用 VAE 解码器将反向扩散过程生成的去噪潜伏转换回图像。我们在推理过程中，我们<strong>只需要 VAE 解码器</strong>。</p>
<p><strong>2. The U-Net</strong></p>
<p>U-Net 的编码器部分和解码器部分均由 ResNet 块组成。编码器将图像表示压缩为较低分辨率的图像表示，解码器将较低分辨率的图像表示解码回据称噪声较小的原始高分辨率图像表示。更具体地说，U-Net 输出预测可用于计算预测去噪图像表示的噪声残差。</p>
<p>为了防止 U-Net 在下采样时丢失重要信息，通常在编码器的下采样 ResNet 和解码器的上采样 ResNet 之间添加快捷连接。此外，稳定扩散 U-Net 能够通过交叉注意层在文本嵌入上调节其输出。通常在 ResNet 块之间将交叉注意层添加到 U-Net 的编码器和解码器部分。</p>
<p><strong>3.文本编码器</strong></p>
<p>文本编码器负责将输入提示（<em>例如</em>“An astronaut riding a horse”）转换为 U-Net 可以理解的嵌入空间。它通常是一个简单的<em>基于转换器的</em>编码器，将输入标记序列映射到潜在文本嵌入序列。</p>
<p>受<a target="_blank" rel="noopener" href="https://imagen.research.google/">Imagen</a>的启发，Stable Diffusion 在训练期间不<strong>训练</strong>文本编码器，而只是使用 CLIP 已经训练好的文本编码器<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a>。</p>
<p><strong>为什么潜在扩散快速有效？</strong></p>
<p>由于潜在扩散在低维空间上运行，因此与像素空间扩散模型相比，它大大降低了内存和计算要求。例如，Stable Diffusion 中使用的自动编码器的缩减系数为 8。这意味着形状图像<code>(3, 512, 512)</code>进入<code>(3, 64, 64)</code>潜在空间，这需要<code>8 × 8 = 64</code>更少的内存。</p>
<p>这就是为什么可以<code>512 × 512</code>如此快速地生成图像，即使在 16GB Colab GPU 上也是如此！</p>
<p><strong>推理过程中的稳定扩散</strong></p>
<img src="/2023/06/17/Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion.png" class="" title="SD管道">

<p>稳定扩散模型将latent seed和 text prompt作为输入。然后使用latent seed生成大小为64×64的随机潜在图像，而text prompt通过 CLIP 的文本编码器转换为大小77×768的text embeddings。</p>
<p>接下来，U-Net在以text embeddings为条件的同时迭代地对随机潜在图像表示<em>进行去噪。</em>U-Net 的输出是噪声残差，用于通过调度程序算法计算去噪的潜在图像表示。许多不同的调度程序算法可用于此计算，每个算法都有其优点和缺点。对于稳定扩散，我们建议使用以下之一：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py">PNDM scheduler</a> (used by default)</li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py">DDIM scheduler</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py">K-LMS scheduler</a></li>
</ul>
<p>关于调度程序算法功能如何超出本笔记本范围的理论，但简而言之，应该记住他们根据先前的噪声表示和预测的噪声残差计算预测的去噪图像表示。有关更多信息，建议查看<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.00364">Elucidating the Design Space of Diffusion-Based Generative Models</a></a></p>
<p>重复去噪<em>过程**。</em>50 次逐步检索更好的潜在图像表示。完成后，潜在图像表示由变分自动编码器的解码器部分解码。</p>
<h2 id="编写自己的推理管道"><a href="#编写自己的推理管道" class="headerlink" title="编写自己的推理管道"></a>编写自己的推理管道</h2><p>最后，我们展示如何将 Stable Diffusion 与不同的调度器一起使用，即<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/pull/185">本 PR中添加的</a><a target="_blank" rel="noopener" href="https://github.com/crowsonkb">Katherine Crowson 的</a>K-LMS 调度器。</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/main">预训练模型</a>包括设置完整扩散管道所需的所有组件。它们存储在以下文件夹中：</p>
<ul>
<li><code>text_encoder</code>: Stable Diffusion 使用 CLIP，但其他扩散模型可能使用其他编码器，例如<code>BERT</code>.</li>
<li><code>tokenizer</code>. 它必须与模型使用的相匹配<code>text_encoder</code>。</li>
<li><code>scheduler</code>：用于在训练期间逐步向图像添加噪声的调度算法。</li>
<li><code>unet</code>：用于生成输入的潜在表示的模型。</li>
<li><code>vae</code>：自动编码器模块，我们将使用它来将潜在表示解码为真实图像。</li>
</ul>
<p>我们可以通过引用保存组件的文件夹来加载组件，<code>from_pretrained</code> 中传递<code>subfolder</code></p>
<p>首先，您应该安装<code>diffusers==0.10.2</code>以运行以下代码片段：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install diffusers==0.10.2 transformers scipy  accelerate</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> CLIPTextModel, CLIPTokenizer</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoencoderKL, UNet2DConditionModel, PNDMScheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Load the autoencoder model which will be used to decode the latents into image space. </span></span><br><span class="line">vae = AutoencoderKL.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="string">&quot;vae&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Load the tokenizer and text encoder to tokenize and encode the text. </span></span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(<span class="string">&quot;openai/clip-vit-large-patch14&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. The UNet model for generating the latents.</span></span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(<span class="string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>,  subfolder=<span class="string">&quot;unet&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>现在我们不再加载预定义的调度程序，而是加载具有一些拟合参数的<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/blob/71ba8aec55b52a7ba5a1ff1db1265ffdd3c65ea2/src/diffusers/schedulers/scheduling_lms_discrete.py#L26">K-LMS 调度程序</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> LMSDiscreteScheduler</span><br><span class="line"></span><br><span class="line">scheduler = LMSDiscreteScheduler(beta_start=<span class="number">0.00085</span>, beta_end=<span class="number">0.012</span>, beta_schedule=<span class="string">&quot;scaled_linear&quot;</span>, num_train_timesteps=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p>接下来，让我们将模型移动到 GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch_device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">vae.to(torch_device)</span><br><span class="line">text_encoder.to(torch_device)</span><br><span class="line">unet.to(torch_device) </span><br></pre></td></tr></table></figure>

<p>我们现在定义我们将用于生成图像的参数。</p>
<p>请注意，<code>guidance_scale</code>它的定义类似于<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.11487.pdf">Imagen 论文</a><code>w</code>中等式 (2) 的指导权重。对应于不进行无分类器指导。在这里，我们将其设置为 7.5，就像之前所做的那样。<code>guidance_scale == 1</code></p>
<p>与前面的示例相比，我们设置<code>num_inference_steps</code>为 100 以获得更清晰的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">prompt = [<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>]</span><br><span class="line"></span><br><span class="line">height = <span class="number">512</span>                        <span class="comment"># default height of Stable Diffusion</span></span><br><span class="line">width = <span class="number">512</span>                         <span class="comment"># default width of Stable Diffusion</span></span><br><span class="line"></span><br><span class="line">num_inference_steps = <span class="number">100</span>           <span class="comment"># Number of denoising steps</span></span><br><span class="line"></span><br><span class="line">guidance_scale = <span class="number">7.5</span>                <span class="comment"># Scale for classifier-free guidance</span></span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">0</span>)    <span class="comment"># Seed generator to create the inital latent noise</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="built_in">len</span>(prompt)</span><br></pre></td></tr></table></figure>



<p>首先，我们得到<code>text_embeddings</code>传递的提示。这些嵌入将用于调整 UNet 模型并引导图像生成类似于输入提示的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">text_input = tokenizer(prompt, padding=<span class="string">&quot;max_length&quot;</span>, max_length=tokenizer.model_max_length, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p>我们还将获得用于无分类器指导的无条件文本嵌入，它们只是填充标记（空文本）的嵌入。它们需要与条件<code>text_embeddings</code>(<code>batch_size</code>和<code>seq_length</code>)具有相同的形状</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_length = text_input.input_ids.shape[-<span class="number">1</span>]</span><br><span class="line">uncond_input = tokenizer(</span><br><span class="line">    [<span class="string">&quot;&quot;</span>] * batch_size, padding=<span class="string">&quot;max_length&quot;</span>, max_length=max_length, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">)</span><br><span class="line">uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[<span class="number">0</span>]   </span><br></pre></td></tr></table></figure>



<p>对于无分类器指导，我们需要进行两次前向传递：一次使用条件输入 ( <code>text_embeddings</code>)，另一个使用无条件嵌入 ( <code>uncond_embeddings</code>)。在实践中，我们可以将两者连接成一个批次，以避免进行两次前向传递。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text_embeddings = torch.cat([uncond_embeddings, text_embeddings])</span><br></pre></td></tr></table></figure>



<p>接下来，我们生成初始随机噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">latents = torch.randn(</span><br><span class="line">    (batch_size, unet.in_channels, height // <span class="number">8</span>, width // <span class="number">8</span>),</span><br><span class="line">    generator=generator,</span><br><span class="line">)</span><br><span class="line">latents = latents.to(torch_device)</span><br></pre></td></tr></table></figure>



<p>如果我们在这个阶段检查 ，<code>latents</code>我们会看到它们的形状是<code>torch.Size([1, 4, 64, 64])</code>，比我们想要生成的图像小得多。<code>512 × 512</code>该模型稍后会将这种潜在表示（纯噪声）转换为图像。</p>
<p>接下来，我们用我们选择的<code>num_inference_steps</code>. 这将计算<code>sigmas</code>去噪过程中要使用的确切时间步长值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br></pre></td></tr></table></figure>



<p>K-LMS 调度程序需要将 乘以<code>latents</code>它的<code>sigma</code>值。让我们在这里这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">latents = latents * scheduler.init_noise_sigma</span><br></pre></td></tr></table></figure>



<p>我们准备编写去噪循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">scheduler.set_timesteps(num_inference_steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> tqdm(scheduler.timesteps):</span><br><span class="line">    <span class="comment"># expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.</span></span><br><span class="line">    latent_model_input = torch.cat([latents] * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict the noise residual</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform guidance</span></span><br><span class="line">    noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">    latents = scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>



<p>我们现在使用<code>vae</code>将生成的解码<code>latents</code>回图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scale and decode the image latents with vae</span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    image = vae.decode(latents).sample</span><br></pre></td></tr></table></figure>



<p>最后，让我们将图像转换为 PIL，以便我们可以显示或保存它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">image = (image / <span class="number">2</span> + <span class="number">0.5</span>).clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">image = image.detach().cpu().permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).numpy()</span><br><span class="line">images = (image * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line">pil_images = [Image.fromarray(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">pil_images[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_k_lms.png"><img src="/./Stable-Diffusion%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/stable_diffusion_k_lms.png" alt="PNG"></a></p>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-图像生成热门项目" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/17/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%83%AD%E9%97%A8%E9%A1%B9%E7%9B%AE/">AIGC图像生成热门项目</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-17T04:17:42.000Z" itemprop="datePublished">
  2023-06-17
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <h2 id="Popular-Tasks-amp-Pipelines"><a href="#Popular-Tasks-amp-Pipelines" class="headerlink" title="Popular Tasks &amp; Pipelines"></a>Popular Tasks &amp; Pipelines</h2><table>
<thead>
<tr>
<th>Task</th>
<th>Pipeline</th>
<th>🤗 Hub</th>
</tr>
</thead>
<tbody><tr>
<td>Unconditional Image Generation</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm">DDPM</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/google/ddpm-ema-church-256">google&#x2F;ddpm-ema-church-256</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img">Stable Diffusion Text-to-Image</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/unclip">unclip</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/kakaobrain/karlo-v1-alpha">kakaobrain&#x2F;karlo-v1-alpha</a></td>
</tr>
<tr>
<td>Text-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/if">if</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0">DeepFloyd&#x2F;IF-I-XL-v1.0</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet">Controlnet</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/lllyasviel/sd-controlnet-canny">lllyasviel&#x2F;sd-controlnet-canny</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/pix2pix">Instruct Pix2Pix</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/timbrooks/instruct-pix2pix">timbrooks&#x2F;instruct-pix2pix</a></td>
</tr>
<tr>
<td>Text-guided Image-to-Image</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img">Stable Diffusion Image-to-Image</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5</a></td>
</tr>
<tr>
<td>Text-guided Image Inpainting</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint">Stable Diffusion Inpaint</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-inpainting">runwayml&#x2F;stable-diffusion-inpainting</a></td>
</tr>
<tr>
<td>Image Variation</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation">Stable Diffusion Image Variation</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers">lambdalabs&#x2F;sd-image-variations-diffusers</a></td>
</tr>
<tr>
<td>Super Resolution</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale">Stable Diffusion Upscale</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler">stabilityai&#x2F;stable-diffusion-x4-upscaler</a></td>
</tr>
<tr>
<td>Super Resolution</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale">Stable Diffusion Latent Upscale</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler">stabilityai&#x2F;sd-x2-latent-upscaler</a></td>
</tr>
</tbody></table>

    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
  
  
</article>
      </div>
      
      <div class="col-3">
        <article id="post-Stable Diffusion：什么是 LoRA 模型以及如何使用它们？" class="article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
  
  <div class="article-inner">
    
    <header class="article-header">
      
      
  
    <h5 class="article-title" itemprop="name">
      <a href="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/">Stable Diffusion：什么是 LoRA 模型以及如何使用它们？</a>
    </h5>
  

      <div class="article-meta">
        <time class="text-gray" datetime="2023-06-15T14:00:00.000Z" itemprop="datePublished">
  2023-06-15
</time>
        
      </div>
      
    </header>
    
    <div class="article-entry" itemprop="articleBody">
      <p>Stable Diffusion 已经风靡全球，让任何人都可以<a target="_blank" rel="noopener" href="https://softwarekeep.com/help-center/stable-diffusion-for-free-beginner-guide">免费生成 AI 驱动的艺术作品</a>。但是，如果您曾想生成知名人物、概念或使用特定风格的图像，您可能会对结果感到失望。Stable Diffusion 强大的 AI 本身并不能很好地使角色和风格栩栩如生，这很常见。幸运的是，LoRA 模型可以帮助解决这个问题。</p>
<p>有这么多惊人的扩展和模型可用于增强稳定扩散，LoRA 模型如此受欢迎也就不足为奇了。但是 LoRA 模型到底做了什么？您如何使用它使您的艺术更加壮观？</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/howtouselorainsd-1686843893059-1.png" class="" title="稳定扩散：什么是 LoRA 模型以及如何使用它们？">

<p>在本初学者指南中，我们探讨了什么是 LoRA 模型、在哪里可以找到它们以及如何在 Automatic1111 的 Web GUI 中使用它们，以及一些 LoRA 模型演示。</p>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a><strong>目录</strong></h4><ol>
<li>什么是 LoRA？</li>
<li>LoRA 模型类型</li>
<li>在哪里可以找到和下载 LoRA 模型</li>
<li>LoRA 模型安装到 Automatic1111</li>
<li>如何在 Automatic1111 中使用 LoRA 模型</li>
</ol>
<h2 id="什么是-LoRA？"><a href="#什么是-LoRA？" class="headerlink" title="什么是 LoRA？"></a><strong>什么是 LoRA？</strong></h2><p>LoRA 代表低秩适应。它允许您使用低秩自适应技术来快速微调扩散模型。简单来说，LoRA 训练模型可以更轻松地针对不同的概念（例如角色或特定风格）训练 Stable Diffusion。然后，这些训练有素的模型可以导出并供他们这一代的其他人使用。</p>
<p>稳定扩散模型因其生成高质量图像和文本的能力而在机器学习领域越来越受欢迎。然而，这些模型的一个主要缺点是它们的文件很大，这使得用户很难在他们的个人计算机上维护一个集合。这就是 LoRA 作为一种训练技术发挥作用的地方，它可以在保持可管理的文件大小的同时微调稳定扩散模型。</p>
<p><a target="_blank" rel="noopener" href="https://softwarekeep.com/help-center/best-stable-diffusion-models-to-try">LoRA 模型是小型稳定扩散模型，它对标准检查点模型</a>应用较小的更改，导致文件大小减少 2-500 MB，比检查点文件小得多。LoRA 在文件大小和训练能力之间提供了一个很好的权衡，使它们成为对拥有大量模型的用户有吸引力的解决方案。</p>
<h2 id="LoRA-模型类型"><a href="#LoRA-模型类型" class="headerlink" title="LoRA 模型类型"></a><strong>LoRA 模型类型</strong></h2><p>我们可以将 LoRA 模型分为几种不同的类型：</p>
<h3 id="Character-LoRA"><a href="#Character-LoRA" class="headerlink" title="Character LoRA"></a><strong>Character LoRA</strong></h3><p>在特定角色（例如卡通或视频游戏角色）上训练的模型。Character LoRA 能够准确地重现角色的外观和感觉，以及与其相关的任何关键特征。这是最常见的 LoRA 类型，因为在没有这种训练数据的情况下生成字符通常很棘手且不一致。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_character-1686843893059-3.png" class="" title="字符LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/1274/dreamlike-diffusion-10">Dreamlike Diffusion 1.0</a></li>
<li><strong>使用 LoRA</strong> ： <a target="_blank" rel="noopener" href="https://civitai.com/models/55339/the-joker-or-photorealistic-joker-lora">The Joker | 真实感</a></li>
<li><strong>使用提示</strong>：小丑肖像，高质量，8k</li>
</ul>
<p>应用角色 LoRA 可以让您快速生成具有真实外观的角色，使它们非常适合 AI 插图、角色概念艺术，甚至参考表。根据模型的训练，角色可能适合一套衣服、一种特定的发型，甚至是某种面部表情。然而，某些角色 LoRA 可以将您选择的角色放入新的服装和设置中，从而增加他们的魅力。</p>
<p>Character LoRA 适用于各种媒体，包括流行和鲜为人知的标题。您会发现来自超级马里奥、漫威和神奇宝贝等热门系列的角色，以及众多日本动漫角色，甚至是漫画书中的英雄。</p>
<p>当然，角色LoRA也可以应用于原始角色，只要有足够的训练数据即可。虽然低训练数据的实验正在进行中，但最好使用至少 10-20 个不同的图像来创建角色 LoRA。这将为您的训练过程增添多样性，提高生成角色的质量。</p>
<h3 id="Style-LoRA"><a href="#Style-LoRA" class="headerlink" title="Style LoRA"></a><strong>Style LoRA</strong></h3><p>Style LoRA 与角色 LoRA 有许多相似之处，但它不是针对特定角色或对象进行训练，而是侧重于艺术风格。这种类型的模型通常由特定艺术家进行艺术训练，让您可以在自己的作品中使用他们的标志性风格。Style LoRA 可用于任何事情，从风格化参考图像到创建相同风格的原创艺术作品。</p>
<p>顾名思义，这些模型针对特定风格进行训练，例如动画表演、水彩画、线稿等的特定外观。使用这种类型的 LoRA 模型，您可以轻松地为您的 AI 作品赋予与众不同的独特风格！</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_style-1686843893060-5.png" class="" title="风格LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/23900?modelVersionId=29792">AnyLoRA - Checkpoint</a></li>
<li><strong>使用的 LoRA</strong> : <a target="_blank" rel="noopener" href="https://civitai.com/models/7094/arcane-style-lora">Arcane Style LoRA</a></li>
<li><strong>使用提示</strong>：神秘风格，1girl，粉红色头发，长发，一条辫子，白衬衫，外套，黄色眼睛，看着观众，城市街道</li>
</ul>
<p>我们使用根据 Netflix 节目 Arcane 的风格训练的 LoRA 模型生成了一幅新的 AI 艺术品。该模型能够在原剧中没有出现的角色上捕捉到剧中鲜艳的色彩和独特的角色设计。</p>
<p>样式 LoRA 的优点在于它们与常规的稳定扩散检查点一起工作，使您无需合并大型模型即可创建令人惊叹的独特作品。例如，使用现实主义检查点和绘画风格 LoRA 将生成看起来像绘画的逼真图像。</p>
<h3 id="Concept-LoRA"><a href="#Concept-LoRA" class="headerlink" title="Concept LoRA"></a><strong>Concept LoRA</strong></h3><p>Concept LoRA 是一种特殊的 LoRA，它是针对特定概念或想法进行训练的。这些模型通常旨在概念化一些特定的东西，而这些东西很难通过简单的即时工程实现。例如，这种类型的 LoRA 可以针对特定的情绪、动作或非常特定的项目进行训练。</p>
<p>当您尝试创建传达特定概念的原创艺术作品时，这种类型的模型特别有用。例如，如果您想生成玻璃雕塑的图像，您可以使用针对该确切想法训练的概念 LoRA。结果将是一件独特而有趣的艺术品，清楚地传达了您的目标概念。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_concept-1686843893060-7.png" class="" title="概念LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/1274/dreamlike-diffusion-10">Dreamlike Diffusion 1.0</a></li>
<li><strong>使用的 LoRA</strong> ：<a target="_blank" rel="noopener" href="https://civitai.com/models/11203/glass-sculptures">玻璃雕塑</a></li>
<li><strong>使用提示</strong>：芭蕾舞演员、舞蹈、半透明、玻璃雕塑、倒影</li>
</ul>
<p>Concept LoRA 使创建风格化和概念性强的艺术作品变得更加容易。它们也非常适合创建更小、更模糊的作品，而这些作品很难用其他模型生成。因此，它们通常可以让您的作品在独特性和艺术价值方面更具优势。</p>
<h3 id="Pose-LoRA"><a href="#Pose-LoRA" class="headerlink" title="Pose LoRA"></a><strong>Pose LoRA</strong></h3><p>将姿势 LoRA 应用到你这一代就像听起来一样 - 它会以某种方式摆出你的角色。这对于生成动态场景非常有用，您可以在其中生成特定的姿势和动作，而这些姿势和动作通常很难通过常规的提示工程来实现。</p>
<p>Pose LoRA 模型更关注所述角色的姿势，而不是其风格或特征。例如，如果您要将姿势 LoRA 模型应用于人形角色，它会为他们创建不同的姿势，例如跑步、跳跃或坐着，但不会改变他们的特征、服装或改变模型的风格你正在使用。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_pose-1686843893060-9.png" class="" title="姿势LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/36520/ghostmix">GhostMix</a></li>
<li><strong>使用 LoRA</strong> ： <a target="_blank" rel="noopener" href="https://civitai.com/models/51900/shinji-in-a-chair-or-meme">Shinji in a Chair | 模因</a></li>
<li><strong>使用提示</strong>：独奏，男性焦点，坐着，低着头，黑色短发，连帽夹克，牛仔裤，运动鞋</li>
</ul>
<p>Pose LoRA 是一种很好的方式，可以更好地控制你的世代，而无需安装和学习更高级的解决方案，如 ControlNet。这种类型的 LoRA 只需对原始提示进行一些简单的更改，就可以帮助您创建动态有趣的场景。</p>
<h3 id="Clothing-LoRa"><a href="#Clothing-LoRa" class="headerlink" title="Clothing LoRa"></a><strong>Clothing LoRa</strong></h3><p>另一个有用的模型是服装 LoRA。如您所料，这种类型的 LoRA 模型旨在改变一个人的衣服和配饰。有了它，您可以快速轻松地为任何角色赋予新衣服，无论是现代风格还是历史风格。</p>
<p>这些模型的优点在于它们适用于任何类型的角色。只需一个模型，您就可以应用各种不同的人的风格和设计！例如，如果您想创建一个角色穿着中国传统服饰的场景，只需将您选择的服装 LoRA 应用到您的这一代，瞧——即时的中国传统服装！</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_clothing-1686843893060-11.png" class="" title="服装LoRA">

<ul>
<li><strong>使用的模型</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/36520/ghostmix">GhostMix</a></li>
<li><strong>使用的LoRA</strong> ：<a target="_blank" rel="noopener" href="https://civitai.com/models/15365/hanfu">汉服</a></li>
<li><strong>使用提示</strong>：女孩，蓝色汉服，全身</li>
</ul>
<p>很多时候，即使您清楚地描述了角色的着装，Stable Diffusion 也可能无法最好地将您的想法变为现实。然而，在服装 LoRA 的帮助下，您可以微调角色的确切外观，并为您的作品带来额外的真实感。</p>
<h3 id="Object-LoRA"><a href="#Object-LoRA" class="headerlink" title="Object LoRA"></a><strong>Object LoRA</strong></h3><p>最后但同样重要的是，我们有对象 LoRA。这是一个广泛的 LoRA 模型类别，用于生成家具、植物甚至车辆等对象。当然，您可以使用这些模型创建的项目类型取决于您使用的特定模型和您提供的提示。</p>
<p>但是，该术语也适用于用于创建更抽象对象的 LoRA，例如游戏或网站的 UI 元素。这对于为您的项目创建更具凝聚力的外观非常有用。</p>
<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/lora_object-1686843893060-13.png" class="" title="Object LoRA">

<ul>
<li><strong>使用型号</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/57931/szechuan-special-sauce?modelVersionId=62378">川菜特制酱料</a></li>
<li><strong>LoRA 使用</strong>：<a target="_blank" rel="noopener" href="https://civitai.com/models/58247/product-design-dark-minimalism-eddiemauro-lora">产品设计（黑暗极简主义-eddiemauro）</a></li>
<li><strong>使用提示</strong>：未来派水壶，计算机渲染，极简主义，4k</li>
</ul>
<p>Object LoRA 不仅是艺术家的宝贵工具，也是游戏开发人员、网页设计师和其他需要高效创建资产的创意专业人士的宝贵工具。能够生成具有自定义设计的对象让您可以自由地试验和探索不同的视觉效果，直到找到最适合您的项目的视觉效果。</p>
<h2 id="在哪里可以找到和下载-LoRA-模型"><a href="#在哪里可以找到和下载-LoRA-模型" class="headerlink" title="在哪里可以找到和下载 LoRA 模型"></a><strong>在哪里可以找到和下载 LoRA 模型</strong></h2><p>LoRA 模型在各种开源存储库中可用，包括 <a target="_blank" rel="noopener" href="https://civitai.com/">Civitai</a>和<a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face</a>。这些模型可免费使用，只需几个简单的步骤即可轻松下载。这些模型最好的地方在于它们的大小——大多数 LoRA 模型不超过几兆字节，这使得它们非常轻巧且易于使用。</p>
<p>下载您要使用的模型后，您必须将它们安装到正确的文件夹中。根据您的设置，这可能会发生变化。在本文中，我们将探索 LoRA 模型与 Automatic1111 webUI 的结合使用，但您可以研究您的平台以获取有关使用 LoRA 模型的具体说明。</p>
<h2 id="如何将-LoRA-模型安装到-Automatic1111-中"><a href="#如何将-LoRA-模型安装到-Automatic1111-中" class="headerlink" title="如何将 LoRA 模型安装到 Automatic1111 中"></a><strong>如何将 LoRA 模型安装到 Automatic1111 中</strong></h2><p>在将模型放入 webUI 之前，无论您使用什么平台生成图像，您很可能需要安装 LoRA 扩展本身。以下是安装 Automatic1111 扩展的方法：</p>
<ol>
<li>首先，启动 Automatic1111 网络用户界面。</li>
<li>打开“<strong>扩展”选项卡，然后</strong>从可用选项中单击“<strong>从 URL 安装”。</strong></li>
<li>将以下链接粘贴到“<strong>扩展的 git 存储库的 URL</strong> ”输入字段中，然后按“<strong>安装</strong>”按钮：<a target="_blank" rel="noopener" href="https://github.com/kohya-ss/sd-webui-additional-networks.git">https://github.com/kohya-ss/sd-webui-additional-networks.git</a><img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/Photoshop_s6W3JR02z5-1686843893060-15.png" class="" title="安装 lora 扩展"></li>
<li>切换到“ <strong>Installed</strong> ”选项卡，然后单击“ <strong>Apply and restart UI</strong> ”按钮。现在，等待 Automatic1111 Web UI 重新启动。<img src="/2023/06/15/Stable%20Diffusion%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%20LoRA%20%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%AE%83%E4%BB%AC%EF%BC%9F/Photoshop_MYVL9qfa6j-1686843893060-17.png" class="" title="重新启动用户界面"></li>
<li>执行这些步骤后，您应该注意到“ <strong>models</strong> ”文件夹中有一些新的子文件夹。这些模型将存储您的 LoRA。但是，您需要配置此文件夹，以便 Automatic1111 Web UI 可以读取它。</li>
<li>打开“<strong>设置</strong>”选项卡，然后切换到“<strong>其他网络</strong>”选项卡。找到“<strong>用于扫描 LoRA 模型的额外路径</strong>”输入字段并粘贴到正确的文件夹中。<strong>您会在“ stable-diffusion-webui&#x2F;models&#x2F;Lora ”目录</strong>中找到它。</li>
<li>输入 LoRA 文件夹的正确完整路径后，单击“<strong>应用设置</strong>”。</li>
</ol>
<p>这负责安装 LoRA 扩展，但是，这还不足以开始生成图像。您还需要将实际的 LoRA 模型安装到正确的文件夹中。为此，请获取下载的 LoRA 文件并将其放在“ <strong>stable-diffusion-webui&#x2F;models&#x2F;Lora</strong> ”文件夹中。</p>
<h2 id="如何在-Automatic1111-中使用-LoRA-模型"><a href="#如何在-Automatic1111-中使用-LoRA-模型" class="headerlink" title="如何在 Automatic1111 中使用 LoRA 模型"></a><strong>如何在 Automatic1111 中使用 LoRA 模型</strong></h2><p>一旦安装了您要使用的 LoRA 模型，您就可以开始使用它创建图像。</p>
<ol>
<li>启动 Automatic1111 Web UI 并选择所需的检查点模型。一些 LoRA 需要使用特定的检查点；始终检查您的 LoRA 的描述和说明。</li>
<li>输入您的提示。确保包括 LoRA 的触发词（如果有的话）。创作者通常会把这个词放在描述中，或者你可以在Civitai 上的“<strong>触发词”参数中找到它。</strong></li>
<li>单击“*<em>生成”按钮下的“*</em>**附加网络**”图标，然后切换到“ <strong>Lora</strong> ”选项卡。在这里，单击要使用的 Lora 以将其插入到提示符中。</li>
<li><strong>如果需要，通过将默认值“ 1</strong> ”修改为更低或更高的数字来更改 LoRA 的权重。例如，一些 LoRA 在“ <strong>0.6</strong> ”或“ <strong>1.2</strong> ”这样的权重上效果更好，这取决于它是如何训练的，以及你正在寻找什么样的结果。</li>
<li>完成生成设置的配置，然后单击“<strong>生成</strong>”按钮。</li>
</ol>
<p>您应该注意到 LoRA 已应用于生成的图像，使您能够处理更具体和独特的概念。您为配置 LoRA 所花费的时间和精力是非常值得的；结果可能是惊人的！</p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a><strong>结论</strong></h4><p>LoRA 模型是为 AI 生成的图像添加更多细节和准确性的好方法。只需几个简单的步骤，您就可以开始将这些模型整合到您的 Automatic1111 工作流程中，让您的项目进入一个充满可能性的全新世界。我们建议密切关注最新版本，因为平台上总是会添加新的和有趣的模型。快乐创造！</p>

    </div>
    <footer class="article-footer">
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%95%86%E4%B8%9A%E5%8C%96%E5%9C%BA%E6%99%AF/" rel="tag">商业化场景</a></li></ul>

    </footer>
  </div>
  
  
</article>
      </div>
      
  </div>


  


        </div>
      
      
        <div class="d-flex justify-content-center pt-5">
          <a href="/archives/" title="→ 查看更多" class="btn btn-lg bg-white shadow-hover">→ 查看更多</a>
        </div>
      
    </div>
  </section>


    </section>
    <footer class="footer pt-5 mt-5">
  <div class="container">
    <div class="py-3">
      <div class="row justify-content-between">
        <div class="col-6">
          <img class="filter-gray mb-3 lazyload" height="40" data-src="/images/brand.svg" alt="AI架构 | AI系统基础架构设计与优化" role="img">
          <p class="mb-4">这个网站旨在为AI架构师和对AI系统设计和优化感兴趣的人提供有价值的信息和资源。我们提供关于AI架构设计原理、机器学习和深度学习算法、大规模数据处理技术等方面的深入文章、案例研究和最佳实践指南。通过这些内容，我们希望帮助读者了解如何构建高性能、可扩展的AI架构，并提供实用的建议和方法来优化AI系统的性能和可靠性。无论是初学者还是有经验的专业人士，我们致力于为您提供有益的见解和实用的资源，以推动AI架构领域的发展和进步。</p>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="javascript:;">
                  <img 0="微信" src="/images/icons/contact_wechat.svg">
                </a>
              </li>
            
              <li class="list-inline-item">
                <a href="mailto:a@abc.com">
                  <img 0="邮箱" src="/images/icons/contact_email.svg">
                </a>
              </li>
            
          </ul>
        </div>
        <div class="col-4">
          <h5>友情链接</h5>
          <ul class="list-inline">
            
              <li class="list-inline-item">
                <a href="https://acorn.imaging.xin/" title="Acorn" target="_blank" rel="noopener">Acorn</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://github.com/" title="GitHub" target="_blank" rel="noopener">GitHub</a>
              </li>
            
              <li class="list-inline-item">
                <a href="https://duoyu.wang/" title="To Base64" target="_blank" rel="noopener">To Base64</a>
              </li>
            
          </ul>
        </div>
      </div>
    </div>
    <hr class="hr" style="opacity: .25;">
    <div class="pt-3 pb-5">
      <ul class="list-inline mb-0 text-center">
        <li class="list-inline-item">&copy; 2023 AI架构 | AI系统基础架构设计与优化</li>
        
        <li class="list-inline-item">Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
        <li class="list-inline-item">Designer <a href="https://acorn.imaging.xin/" target="_blank">罗平</a></li>
      </ul>
    </div>
  </div>
</footer>
  </main>
  <div id="mobile-nav-dimmer"></div>
<div id="mobile-nav">
	<div id="mobile-nav-inner">
		<ul class="mobile-nav">
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/">首页</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/stories">案例</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/archives">文章</a>
  </li>
  
    
  <li class="nav-item ">
    <a class="nav-item-link" href="/about">关于</a>
  </li>
  
</ul>
		
	</div>
</div>

  <script src="/libs/feather/feather.min.js"></script>
<script src="/libs/lazysizes/lazysizes.min.js"></script>

	<script src="/libs/tocbot/tocbot.min.js"></script>
	<script>
    tocbot.init({
      // Where to render the table of contents.
      tocSelector: '.js-toc',
      // Where to grab the headings to build the table of contents.
      contentSelector: '.js-toc-content',
      // Which headings to grab inside of the contentSelector element.
      headingSelector: 'h2, h3',
      // For headings inside relative or absolute positioned containers within content.
      hasInnerContainers: true,
    });
	</script>





<script src="/js/mobile-nav.js"></script>


<script src="/js/script.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-178892506-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-178892506-1');
</script>
</body>
</html>